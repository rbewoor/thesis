{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImgCapTraining_Link1_BlogPapersapce_CocoVal2017_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1xBjxTXYGkFR",
        "x4C22xXusYtk",
        "Hrej2EBES6K5",
        "P7Lmh8eeQzPn",
        "4JCI5otNTKKb",
        "Q_AAKliCGXtc",
        "qs5nZay2Eh1T",
        "UiuCDq-xcqlH",
        "-NO6tGXnFzCh",
        "y8rjoRQ2f_d9",
        "vth7V8Ll0UjK",
        "zkbT2AkO4w7R"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPasdiU_9WNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWnU8cGjH7Rh",
        "colab_type": "text"
      },
      "source": [
        "## Misc Testing - SKIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSQuyKW7H4vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################################################################################\n",
        "## notes on the coco train and val IMAGES 2017 dataset - links here - https://cocodataset.org/#download\n",
        "## train 2017 has 118287 images, 18GB - http://images.cocodataset.org/zips/train2017.zip\n",
        "## val   2017 has 5000   images, 1GB  - http://images.cocodataset.org/zips/val2017.zip\n",
        "## test  2017 has 40670  images, 6GB  - http://images.cocodataset.org/zips/test2017.zip\n",
        "\n",
        "## notes on the coco train+val 2017 annotations\n",
        "## combined file of 241MB - http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "## is a huge json - see the data exploration notes below\n",
        "#############################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GinHjXnqAVHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and verify the folder contents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSqsH9sh93ra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2436b616-6655-4f01-87dc-5690a1c5dc72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwqiBjkI-MhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69aaea27-e136-453b-88cd-1c8231518098"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5-H66cO-Mj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8bcd03-9b58-4c19-a38d-5d15102e3a50"
      },
      "source": [
        "! ls ./gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4cP0Q2y-MnO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2352f944-975b-4ea4-b67a-df40c525b24b"
      },
      "source": [
        "! ls './gdrive/My Drive/ThesisStoryGen/Data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "audio\t\t\t\t\t    coco_train2017_wget_1\n",
            "coco_annotations_trainval2017\t\t    coco_val2017_wget_1\n",
            "coco_test_2017_40k_images_op4neo_1_1_5k     DatasetsNotes-20200517.xlsx\n",
            "coco_test_2017_40k_images_op4neo_1_15k_20k  flickr30k_images.tar.gz\n",
            "coco_test_2017_40k_images_op4neo_1_5k_10k   ImgCapTraining\n",
            "coco_test2017_wget_1\t\t\t    saved_keras_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaygYywI-QiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd4f7550-ad5d-4020-db36-f8993b618343"
      },
      "source": [
        "! ls './gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val2017  val2017.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2c8iIGwmAhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cc5ab10-3c26-42c6-d0f3-ed3cbd87edaa"
      },
      "source": [
        "! ls './gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/' | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r74kOmX9Hgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzp5ATJ-ksEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guGz-uP02P6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################\n",
        "## data exploration on the annotations dataset\n",
        "################################################\n",
        "\n",
        "## about val2017 ::::\n",
        "## max caption length = 247 for val2017    'A large square concrete wall, which shows peo...'\n",
        "## 25014 caption entries for the 5000 images\n",
        "\n",
        "## about train2017 ::::\n",
        "## 591753 caption entries for the 118287 images\n",
        "## max caption length = 250 for train2017  - multiple entries\n",
        "##      'Front view of person holding animal spotted pr...'\n",
        "##      'The image shows an entertainment cubby in the ...'\n",
        "##      'The vee shaped part of a white-walled room sho...'\n",
        "##      'The scene shows outdoors, furthest to closest,...'\n",
        "##      'Black and white of two women sitting on a marb...'\n",
        "##      'From furthest to nearest, the view shows, clou...'\n",
        "##      'From below view of the head, neck, and paws of...'\n",
        "##      'A room with long table with widescreen televis...'\n",
        "##      'Two men holding tennis rackets up in a hand on...'\n",
        "##      'Restaurant table top with logo in middle and a...'\n",
        "##      'A near-ground shot shows logo-covered wall and...'\n",
        "##      'An apartment with an open floor plan,  that al...'\n",
        "##      'A black and white shot shows a  bus interior w...'\n",
        "##      'An image from a copyrighted foodie site shows ...'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt0Hn8vQLZ_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT04Md9pLaC4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a00cccfb-64e6-4a89-a585-9033c66abb63"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2PLhw0bPuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
        "IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
        "IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL7rx5lbbPw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7d5744d-a3c3-4849-e312-156a66ac86b6"
      },
      "source": [
        "os.listdir('/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['captions_val2017.json', 'captions_train2017.json', 'sample_data_in_file.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEVkLsD5LaKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(IPDIRANNO+'captions_train2017.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
        "  #type(data['annotations']) # is a list\n",
        "  #type(data['images'])      # also is a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "456rGQbIwreN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfanno = pd.DataFrame(data=data['annotations'])\n",
        "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
        "# dfanno.dtypes =\n",
        "#   image_id     int64\n",
        "#   id           int64\n",
        "#   caption     object\n",
        "#   dtype: object\n",
        "\n",
        "dfimages = pd.DataFrame(data=data['images'])\n",
        "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
        "# dfimages.dtypes =\n",
        "#   license           int64\n",
        "#   file_name        object\n",
        "#   coco_url         object\n",
        "#   height            int64\n",
        "#   width             int64\n",
        "#   date_captured    object\n",
        "#   flickr_url       object\n",
        "#   id                int64\n",
        "#   dtype: object\n",
        "## of above, am dropping useless columns\n",
        "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
        "\n",
        "## columns remaining in the dfs are:\n",
        "# dfanno columns are      image_id , id , caption\n",
        "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
        "# dfimages columns are    file_name        , height ,  width , id\n",
        "#                         000000397133.jpg , 427    ,  640   , 397133\n",
        "\n",
        "## the captions are not ordered for each image and seem to randomly placed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfN3LKaGwrnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e72a3c59-81fd-4b4d-eae9-ff9771fc1a66"
      },
      "source": [
        "dfanno['caplen'] = dfanno.caption.str.len()\n",
        "dfanno.caplen.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vqPc5ojzN5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "77bc9910-2fb5-424b-daba-90c4f6bad4e3"
      },
      "source": [
        "longest_caption_len = dfanno.caption[dfanno.caplen == dfanno.caplen.max()].to_string(index=False)\n",
        "print(f\"{type(longest_caption_len)}\")\n",
        "print(f\"{longest_caption_len}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            " Front view of person holding animal spotted pr...\n",
            " The image shows an entertainment cubby in the ...\n",
            " The vee shaped part of a white-walled room sho...\n",
            " The scene shows outdoors, furthest to closest,...\n",
            " Black and white of two women sitting on a marb...\n",
            " From furthest to nearest, the view shows, clou...\n",
            " From below view of the head, neck, and paws of...\n",
            " A room with long table with widescreen televis...\n",
            " Two men holding tennis rackets up in a hand on...\n",
            " Restaurant table top with logo in middle and a...\n",
            " A near-ground shot shows logo-covered wall and...\n",
            " An apartment with an open floor plan,  that al...\n",
            " A black and white shot shows a  bus interior w...\n",
            " An image from a copyrighted foodie site shows ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MORaqFnzN-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "dd778695-188f-420b-87be-9ff702867aff"
      },
      "source": [
        "dfanno.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "      <th>caplen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>203564</td>\n",
              "      <td>37</td>\n",
              "      <td>A bicycle replica with a clock as the front wh...</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>322141</td>\n",
              "      <td>49</td>\n",
              "      <td>A room with blue walls and a white sink and door.</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id  id                                            caption  caplen\n",
              "0    203564  37  A bicycle replica with a clock as the front wh...      50\n",
              "1    322141  49  A room with blue walls and a white sink and door.      49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiXmp9mkkJ-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f413a1fa-5cc0-4616-df9b-ac1c23e8e3b4"
      },
      "source": [
        "dfanno = dfanno.merge(dfimages, left_on='image_id', right_on='id', how='inner')\n",
        "dfanno.drop(['id_y'], axis=1, inplace=True)\n",
        "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
        "dfanno.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['image_id', 'id', 'caption', 'caplen', 'file_name', 'height', 'width'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpdzGC1wkKBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0351a92e-378c-462e-cefc-86b7f3959865"
      },
      "source": [
        "dfanno.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "      <th>caplen</th>\n",
              "      <th>file_name</th>\n",
              "      <th>height</th>\n",
              "      <th>width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>203564</td>\n",
              "      <td>37</td>\n",
              "      <td>A bicycle replica with a clock as the front wh...</td>\n",
              "      <td>50</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>203564</td>\n",
              "      <td>181</td>\n",
              "      <td>The bike has a clock as a tire.</td>\n",
              "      <td>31</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>203564</td>\n",
              "      <td>478</td>\n",
              "      <td>A black metal bicycle with a clock inside the ...</td>\n",
              "      <td>58</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>203564</td>\n",
              "      <td>6637</td>\n",
              "      <td>A bicycle figurine in which the front wheel is...</td>\n",
              "      <td>69</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>203564</td>\n",
              "      <td>6802</td>\n",
              "      <td>A clock with the appearance of the wheel of a ...</td>\n",
              "      <td>54</td>\n",
              "      <td>000000203564.jpg</td>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id    id  ... height  width\n",
              "0    203564    37  ...    400    400\n",
              "1    203564   181  ...    400    400\n",
              "2    203564   478  ...    400    400\n",
              "3    203564  6637  ...    400    400\n",
              "4    203564  6802  ...    400    400\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64750op2mJBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6caa3de-7a1a-4e1d-bfe1-8b8e915709e9"
      },
      "source": [
        "len(dfanno)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "591753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TVPhOnymJEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61de95c3-7b66-4d89-ae05-11c588383e92"
      },
      "source": [
        "dfanno['file_name'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118287"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtQ2H-fFjcxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua2Wa77vjc0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBp-n7evQ7ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGxkgfm0Q73Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hu_85TNQ8dO",
        "colab_type": "text"
      },
      "source": [
        "# Training a NEW model - using COCO val2017 full data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTzvu_ZQe3C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "## Training a NEW model based on this code:\n",
        "## Image Captionining with AI\n",
        "## Link: https://blog.paperspace.com/image-captioning-with-ai/\n",
        "## This code used Flickr8k dataset.\n",
        "##\n",
        "## Using full 5k images data of COCO-VAL2017\n",
        "###############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGE6TVcUlHv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pickle\n",
        "#import itertools\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARGu8OC9lHzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ef08701-66b3-479a-862f-85d68105ca8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujlzgVc8lH17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifAWRujisW4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
        "IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
        "IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n",
        "WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYlWfq18sWIt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xBjxTXYGkFR",
        "colab_type": "text"
      },
      "source": [
        "## ONLY ONCE: Start with the COCO_2017 Annodations json file. Preprocess the captions and make entries in the hashmap datastructure of {'image_filename_without_extension': ['caption_1', 'etc']}. Finally pickle hashmap for reloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4C22xXusYtk",
        "colab_type": "text"
      },
      "source": [
        "### Load Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwxje0hYlOky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db494424-2f12-43c4-ff46-5f93ed2d835d"
      },
      "source": [
        "os.listdir('/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['captions_val2017.json', 'captions_train2017.json', 'sample_data_in_file.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JeRY9RilOnq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "76eb67d2-2d55-4e3f-dbb4-e953f2fb2979"
      },
      "source": [
        "## from the annotations file, load the captions\n",
        "\n",
        "with open(IPDIRANNO+'captions_val2017.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
        "  #type(data['annotations']) # is a list\n",
        "  #type(data['images'])      # also is a list\n",
        "\n",
        "dfanno = pd.DataFrame(data=data['annotations'])\n",
        "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
        "# dfanno.dtypes =\n",
        "#   image_id     int64\n",
        "#   id           int64\n",
        "#   caption     object\n",
        "#   dtype: object\n",
        "\n",
        "dfimages = pd.DataFrame(data=data['images'])\n",
        "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
        "# dfimages.dtypes =\n",
        "#   license           int64\n",
        "#   file_name        object\n",
        "#   coco_url         object\n",
        "#   height            int64\n",
        "#   width             int64\n",
        "#   date_captured    object\n",
        "#   flickr_url       object\n",
        "#   id                int64\n",
        "#   dtype: object\n",
        "## of above, am dropping useless columns\n",
        "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
        "\n",
        "## columns remaining in the dfs are:\n",
        "# dfanno columns are      image_id , id , caption\n",
        "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
        "# dfimages columns are    file_name        , height ,  width , id\n",
        "#                         000000397133.jpg , 427    ,  640   , 397133\n",
        "\n",
        "## the captions are not ordered for each image and seem to be randomly placed\n",
        "\n",
        "dfanno.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>179765</td>\n",
              "      <td>38</td>\n",
              "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>179765</td>\n",
              "      <td>182</td>\n",
              "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>190236</td>\n",
              "      <td>401</td>\n",
              "      <td>An office cubicle with four different types of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id   id                                            caption\n",
              "0    179765   38  A black Honda motorcycle parked in front of a ...\n",
              "1    179765  182      A Honda motorcycle parked in a grass driveway\n",
              "2    190236  401  An office cubicle with four different types of..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGvJDal_MS2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "12bf2363-378d-4e99-f2b1-718c79176092"
      },
      "source": [
        "## bring the actual filename from the other dataframe and clean up columns\n",
        "dfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\n",
        "dfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\n",
        "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
        "dfanno.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>179765</td>\n",
              "      <td>38</td>\n",
              "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>179765</td>\n",
              "      <td>182</td>\n",
              "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>179765</td>\n",
              "      <td>479</td>\n",
              "      <td>A black Honda motorcycle with a dark burgundy ...</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id  ...         file_name\n",
              "0    179765  ...  000000179765.jpg\n",
              "1    179765  ...  000000179765.jpg\n",
              "2    179765  ...  000000179765.jpg\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi4SKtG6MS5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrej2EBES6K5",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocess part 1  for captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxhBYLVYMuqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## preprocess part 1 using pandas\n",
        "##      lowercase, remove whitespaces in lead, end and mid\n",
        "## update the captions in the df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLql6eBoMS-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0fa876e7-ec5b-4475-9898-01a9b1c9b066"
      },
      "source": [
        "dfanno['caption'].head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     A black Honda motorcycle parked in front of a ...\n",
              "1         A Honda motorcycle parked in a grass driveway\n",
              "2     A black Honda motorcycle with a dark burgundy ...\n",
              "3     Ma motorcycle parked on the gravel in front of...\n",
              "4     A motorcycle with its brake extended standing ...\n",
              "5     An office cubicle with four different types of...\n",
              "6     The home office space seems to be very cluttered.\n",
              "7     an office with desk computer and chair and lap...\n",
              "8        Office setting with a lot of computer screens.\n",
              "9                A desk and chair in an office cubicle.\n",
              "10            A small closed toilet in a cramped space.\n",
              "11    A tan toilet and sink combination in a small r...\n",
              "12    This is an advanced toilet with a sink and con...\n",
              "13      A close-up picture of a toilet with a fountain.\n",
              "14        Off white toilet with a faucet and controls. \n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHk7ys8jyx6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8fb0d325-60b1-4d49-8dbb-b9726bc2d250"
      },
      "source": [
        "#for index_label, row_series in dfanno[6:13].iterrows():\n",
        "for index_label, row_series in dfanno.iterrows():\n",
        "  #inCap = row_series['caption']\n",
        "  #inCap = inCap.lower().strip()\n",
        "  #inCap = \" \".join(inCap.split())\n",
        "  #print(f\"{inCap} --- {type(inCap)}\")\n",
        "  \n",
        "  dfanno.at[index_label, 'caption'] = \" \".join( row_series['caption'].lower().strip().split() )\n",
        "dfanno['caption'].head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     a black honda motorcycle parked in front of a ...\n",
              "1         a honda motorcycle parked in a grass driveway\n",
              "2     a black honda motorcycle with a dark burgundy ...\n",
              "3     ma motorcycle parked on the gravel in front of...\n",
              "4     a motorcycle with its brake extended standing ...\n",
              "5     an office cubicle with four different types of...\n",
              "6     the home office space seems to be very cluttered.\n",
              "7     an office with desk computer and chair and lap...\n",
              "8        office setting with a lot of computer screens.\n",
              "9                a desk and chair in an office cubicle.\n",
              "10            a small closed toilet in a cramped space.\n",
              "11    a tan toilet and sink combination in a small r...\n",
              "12    this is an advanced toilet with a sink and con...\n",
              "13      a close-up picture of a toilet with a fountain.\n",
              "14         off white toilet with a faucet and controls.\n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv20FNXnlO7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "850865e3-0fdc-4a7f-f38c-14a3f38acd4f"
      },
      "source": [
        "dfanno.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>179765</td>\n",
              "      <td>38</td>\n",
              "      <td>a black honda motorcycle parked in front of a ...</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>179765</td>\n",
              "      <td>182</td>\n",
              "      <td>a honda motorcycle parked in a grass driveway</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>179765</td>\n",
              "      <td>479</td>\n",
              "      <td>a black honda motorcycle with a dark burgundy ...</td>\n",
              "      <td>000000179765.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id  ...         file_name\n",
              "0    179765  ...  000000179765.jpg\n",
              "1    179765  ...  000000179765.jpg\n",
              "2    179765  ...  000000179765.jpg\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlEni2AXROHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Lmh8eeQzPn",
        "colab_type": "text"
      },
      "source": [
        "### Mapping image with captions using dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auy-hikzPNDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Mapping image with captions using dictionary\n",
        "\n",
        "def image_to_captions(_dfin):\n",
        "    hash_map = {}\n",
        "    for row in _dfin.itertuples():\n",
        "      rowdict = row._asdict()\n",
        "      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n",
        "      img_caption = rowdict['caption']\n",
        "      if(img_filename not in hash_map):\n",
        "        hash_map[img_filename] = [img_caption]\n",
        "      else:\n",
        "        hash_map[img_filename].append(img_caption)\n",
        "    return hash_map\n",
        "\n",
        "hashmap_img_and_captions = image_to_captions(dfanno)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu6vlJfnYmte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "3c91a778-28b9-4087-acf4-a84fec00f985"
      },
      "source": [
        "dict(list(hashmap_img_and_captions.items())[12:19])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'000000012667': ['the telephone has a banana where the receiver should be.',\n",
              "  'a banana replacing the phone on an answering machine',\n",
              "  'a phone with a banana where the receiver should be.',\n",
              "  'a telephone has it receiver replaced with a banana.',\n",
              "  'a banana placed on a phone on a table.'],\n",
              " '000000065485': ['a dog driving an suv in an open grass covered field.',\n",
              "  'a dog sits in the front seat of a jeep.',\n",
              "  'a dog is sitting inside a red car.',\n",
              "  'a red truck has a black dog in the drivers chair.',\n",
              "  'a dog sitting in the front seat of a truck.'],\n",
              " '000000289393': ['set of toy animals sitting in front of a red wooden wagon.',\n",
              "  'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              "  'some toy animals on the ground near a picture',\n",
              "  \"children's toy animals are strewn across a floor.\",\n",
              "  'a display of vintage animal toys on the floor.'],\n",
              " '000000338325': ['a striped plane flying up into the sky as the sun shines behind it.',\n",
              "  'an airplane is ascending into the white sky',\n",
              "  'an american airlines plane is in the sky',\n",
              "  'a red, white, and blue plane is in the sky.',\n",
              "  'a plane flies through the sky at an angle.'],\n",
              " '000000482917': ['a dog sitting between its masters feet on a footstool watching tv',\n",
              "  'a dog between the feet of a person looking at a tv.',\n",
              "  'a dog and a person are watching television together.',\n",
              "  'a person is sitting with their dog watching tv.',\n",
              "  'a man relaxing at home, watching television with his dog.'],\n",
              " '000000534605': ['man in motorcycle leathers standing in front of a group of bikes',\n",
              "  'bikers, dressed in their gear, standing near their motorcycles.',\n",
              "  'a group of men stand next to their bicycles.',\n",
              "  'three men standing around their motorcycles in a parking lot.',\n",
              "  'group of three motorcyclists standing in front of their motorcycles.'],\n",
              " '000000561256': ['a woman wearing a hat & pink top takes a selfie in front of a bathroom mirror.',\n",
              "  'a woman in a flowered dress taking a selfie in a bathroom mirror.',\n",
              "  'an older woman in a flower dress takes a self photo in the mirror.',\n",
              "  'a woman in a yellow bathroom is holding a camera.',\n",
              "  'a woman taking a picture of herself in a bathroom.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuTRT2Yt50FO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c0c10494-f315-4dd3-d2c5-8539976755d1"
      },
      "source": [
        "## example of caption with a hyphen - in the text\n",
        "hashmap_img_and_captions['000000289393']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set of toy animals sitting in front of a red wooden wagon.',\n",
              " 'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              " 'some toy animals on the ground near a picture',\n",
              " \"children's toy animals are strewn across a floor.\",\n",
              " 'a display of vintage animal toys on the floor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JCI5otNTKKb",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocess part 2  for captions and create the hashmap of images with their captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An7uy0jQSqg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## preprocess part 2 within the hashmap data structure\n",
        "##      replace all hyphen with space  i.e.   -   replaced with   space\n",
        "##      replace all comma with space   i.e.   ,   replaced with   space\n",
        "##      remove any token with length < 2\n",
        "##      retain only alphabetic tokens\n",
        "##      NOT NOW, MAYBE DO IT LATER: add special token of end of sequence at the end represented as follows: <<EOS>>\n",
        "##      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHH5ufA5YwV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "708123dd-94a7-4e61-f34f-36df65ba5123"
      },
      "source": [
        "hashmap_img_and_captions['000000289393']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set of toy animals sitting in front of a red wooden wagon.',\n",
              " 'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              " 'some toy animals on the ground near a picture',\n",
              " \"children's toy animals are strewn across a floor.\",\n",
              " 'a display of vintage animal toys on the floor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybiys_3cT1jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_part2(_hashmapin):\n",
        "  preprocessed_captions = []\n",
        "  for key in _hashmapin.keys():\n",
        "    for idx in range(len(_hashmapin[key])):\n",
        "      inSentence = _hashmapin[key][idx]\n",
        "      if inSentence[-1] == '.':\n",
        "        inSentence = inSentence[:-1]\n",
        "      inSentence = re.sub( r'-' , r' ' , inSentence )\n",
        "      inSentence = re.sub( r',' , r' ' , inSentence )\n",
        "      tokens = [token for token in inSentence.split() if len(token)>1 if token.isalpha()]\n",
        "      #_hashmapin[key][idx] = ' '.join(tokens) + ' <<EOS>>'\n",
        "\n",
        "preprocess_part2(hashmap_img_and_captions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnD5UN4SSqjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "85dfc718-e414-49b6-b95e-b11af41e55fc"
      },
      "source": [
        "hashmap_img_and_captions['000000289393']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set of toy animals sitting in front of a red wooden wagon.',\n",
              " 'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              " 'some toy animals on the ground near a picture',\n",
              " \"children's toy animals are strewn across a floor.\",\n",
              " 'a display of vintage animal toys on the floor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_AAKliCGXtc",
        "colab_type": "text"
      },
      "source": [
        "### PICKLE the hasmap data structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afZHCgdFSqmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "683e91b2-657e-4bb7-bb69-211de09e510b"
      },
      "source": [
        "OPDIR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-TVn6WiDk6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9ba8c5d-857e-41ef-b4ad-285c0ff76475"
      },
      "source": [
        "## pickle the hashmap data\n",
        "with open(OPDIR+'val2017_all_5k_images_hashmap_pickled.pkl', 'wb') as handle:\n",
        "    pickle.dump(hashmap_img_and_captions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"Pickling of hashmap done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickling of hashmap done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_a2VDn4Dk9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0beffc01-0035-4156-a2bc-177da07f63d3"
      },
      "source": [
        "print(f\"Size of pickled data = {sys.getsizeof(hashmap_img_and_captions)} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of pickled data = 147560 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2rUljPIEJ1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del hashmap_img_and_captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbDJkaUEEK6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "363f85e4-3d46-494a-986d-b7a7a66d33b9"
      },
      "source": [
        "print(f\"Size of pickled data = {sys.getsizeof(hashmap_img_and_captions)} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a64122a3b349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Size of pickled data = {sys.getsizeof(hashmap_img_and_captions)} bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hashmap_img_and_captions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs5nZay2Eh1T",
        "colab_type": "text"
      },
      "source": [
        "## DO EVERYTIME - RELOAD FROM PICKLE FILE: Hashmap of images and their captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeMnakUkDlAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6ae2ac6a-bc54-4d92-89c9-01586fe39aca"
      },
      "source": [
        "## reload from picked file\n",
        "if True:\n",
        "  with open(OPDIR+'val2017_all_5k_images_hashmap_pickled.pkl', 'rb') as handle:\n",
        "    hashmap_img_and_captions = pickle.load(handle)\n",
        "\n",
        "hashmap_img_and_captions['000000289393']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set of toy animals sitting in front of a red wooden wagon.',\n",
              " 'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              " 'some toy animals on the ground near a picture',\n",
              " \"children's toy animals are strewn across a floor.\",\n",
              " 'a display of vintage animal toys on the floor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfTacBV7DPL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "151b048f-24b2-4abf-f47d-18c3f3f897bc"
      },
      "source": [
        "print(f\"Size of pickled data = {sys.getsizeof(hashmap_img_and_captions)} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of pickled data = 147560 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38VL-OM9DPOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiuCDq-xcqlH",
        "colab_type": "text"
      },
      "source": [
        "## DO EVERYTIME - Create vocabulary (set of unique tokens) from captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnxOwOawV27Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create vocabulary (set of unique tokens) from captions\n",
        "\n",
        "def create_vocabulary(_hashmap):\n",
        "    vocabulary = set()\n",
        "    for img_captions in _hashmap.values(): # list of up to 5 captions for each image\n",
        "        for caption in img_captions:\n",
        "            for token in caption.split():\n",
        "                vocabulary.add(token)    \n",
        "    return vocabulary\n",
        "\n",
        "vocabulary = create_vocabulary(hashmap_img_and_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpKC8b4seaX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4911261a-e004-4065-f4a0-fc90a123875a"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10007"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIbNAEeUejbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "37266807-d167-4de9-c49b-f19c1fd40697"
      },
      "source": [
        "## see first few entries in our vocabulary set\n",
        "[val for i, val in enumerate(vocabulary) if i < 11 ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drops',\n",
              " 'kiosk',\n",
              " 'feeding.',\n",
              " 'zone',\n",
              " 'competition.',\n",
              " 'olives',\n",
              " 'belt',\n",
              " 'clay.',\n",
              " 'preparation',\n",
              " 'ditch',\n",
              " 'weaving']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfnyivYmltxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NO6tGXnFzCh",
        "colab_type": "text"
      },
      "source": [
        "## ONLY ONCE: Process images, make Inception-v3 pre-trained model, use to create image feature vectors. Finally pickle for reloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8rjoRQ2f_d9",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing of images - do this only first time, later reload from picked file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfApK_3wfRgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Make images suitable for use by Inception-v3 model later\n",
        "##\n",
        "## Resize to (299, 299)\n",
        "## As model needs 4-dim input tensor, add one dimenion to make it (1, 299, 299, 3)\n",
        "## Preprocess the image using custom function of Inception-v3 model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjDBRnRbfRjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image_for_Incepv3(_img_path, _key = 'DUMMY', _DEBUG=False):\n",
        "  img = tf.keras.preprocessing.image.load_img(_img_path, target_size=(299, 299))\n",
        "  #print(f\"type={type(img)}\") # type(img): type=<class 'PIL.Image.Image'>\n",
        "  if _DEBUG:\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Original Image(Resized): ' + _key + '.jpg')\n",
        "\n",
        "  img = tf.keras.preprocessing.image.img_to_array(img) # Converts PIL Image instance to numpy array (299,299,3)\n",
        "  img = np.expand_dims(img, axis=0) #Add one more dimension: (1, 299, 299, 3) # Inception-V3 requires 4 dimensions\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img) # preprocess image as per Inception-V3 model\n",
        "  if _DEBUG:\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(img[0])\n",
        "    plt.title('Preprocessed image for Inception-V3: ' + _key + '.jpg')\n",
        "\n",
        "  return img  # shape will be (1, 299, 299, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygsHl-2lodb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "833726a8-c3e9-418f-8030-d620933756e4"
      },
      "source": [
        "hashmap_img_and_captions['000000289393']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['set of toy animals sitting in front of a red wooden wagon.',\n",
              " 'several toy animals - a bull, giraffe, deer and parakeet.',\n",
              " 'some toy animals on the ground near a picture',\n",
              " \"children's toy animals are strewn across a floor.\",\n",
              " 'a display of vintage animal toys on the floor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFIaQRLhl6vt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cfeec5bd-91ff-4eb3-b6e1-31d1274b42ed"
      },
      "source": [
        "## test out the preprocessing function on one image\n",
        "##     while calling fuction set debug flag to true to display the original and preprocessed images\n",
        "i = 0\n",
        "for key in hashmap_img_and_captions.keys():\n",
        "  if i > 0:\n",
        "    break\n",
        "  \n",
        "  #imgpath = ''.join([IPDIRIMGS, '000000289393', '.jpg'])\n",
        "  imgpath = ''.join([IPDIRIMGS, key, '.jpg'])\n",
        "  print(f\"Image path = {imgpath}\")\n",
        "  preproc_img = preprocess_image_for_Incepv3(imgpath, _key=key, _DEBUG=False)\n",
        "  print(f\"shape after preprocessing = {preproc_img.shape}\")\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image path = /content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/000000179765.jpg\n",
            "shape after preprocessing = (1, 299, 299, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6WoGx9Ul6yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vth7V8Ll0UjK",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-trained model of Inception-v3 pretrained on Imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwBbfRy3ru8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load pre-trained model and remove the last layer (Output layer)\n",
        "\n",
        "# Load Inception-V3 model\n",
        "model_inception_v3_pretrained_imagement = tf.keras.applications.InceptionV3(weights='imagenet')\n",
        "\n",
        "# Create new model, by removing last layer (output layer) from Inception-V3\n",
        "model_CNN_encoder = keras.Model(inputs=model_inception_v3_pretrained_imagement.input, outputs=model_inception_v3_pretrained_imagement.layers[-2].output)\n",
        "\n",
        "#model_new.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfdUsovYrvBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbT2AkO4w7R",
        "colab_type": "text"
      },
      "source": [
        "### Encode images into feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLo0uZrrB4Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Take each image and run it through the encoder. The feature of each image is a numpy array of 2048 values\n",
        "## Store all the info image-wise into a dict\n",
        "##       E.g. of an entry\n",
        "##            print(len(img_encodings['000000179765'])) = \n",
        "##                                                         {'000000179765': array([0.14290829, 0.14481416, 0.3019989 , ..., 0.20583093, 0.13783988,\n",
        "##                                                         0.05842407], dtype=float32), '000000190236': array([0.13693394, 0.44518015, 0.8012958 , ..., 0.09661996, 0.5428589 ,\n",
        "##                                                         0.12551732], dtype=float32)}\n",
        "## Takes forever (for 5k images took xxx seconds)\n",
        "##       So pickle for later use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5fgiO-r4wCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the function which will encode a given image into a vector of size (2048, 0).\n",
        "\n",
        "# Function to encode given image into a vector of size (2048, )\n",
        "def encode_image(_imgpath, _key = 'DUMMY', _DEBUG=False):\n",
        "    preproc_img = preprocess_image_for_Incepv3(_imgpath, _key = 'DUMMY', _DEBUG=False) # preprocess image per Inception-v3 requirements\n",
        "    encoded_features = model_CNN_encoder.predict(preproc_img) # Get encoding vector for image\n",
        "    encoded_features = encoded_features.reshape(encoded_features.shape[1], ) # reshape from (1, 2048) to (2048, )\n",
        "    return encoded_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKPXzIij4wFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a6b2b664-d1c9-47cd-9346-355b9d398ac7"
      },
      "source": [
        "## actually encode the images and capture entries in dictionary with key as the filename only without the full path or the .jpg extension\n",
        "##     using same key as the hashmap for captions uses\n",
        "\n",
        "## sending all the val2017 images\n",
        "start_time = time.time()\n",
        "img_encodings = {}\n",
        "for idx, img_filename_only in enumerate(hashmap_img_and_captions.keys()):\n",
        "  if False:  ## debugging\n",
        "    if idx > 1:\n",
        "      break\n",
        "  img_encodings[img_filename_only] = encode_image(''.join([IPDIRIMGS, img_filename_only, '.jpg']))\n",
        "  if( (idx+1)%500 == 0):\n",
        "    print(f'Count of images encoded = {idx+1}')\n",
        "\n",
        "print(f\"Encoded {idx} images; total time taken = {time.time()-start_time} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of images encoded = 500\n",
            "Count of images encoded = 1000\n",
            "Count of images encoded = 1500\n",
            "Count of images encoded = 2000\n",
            "Count of images encoded = 2500\n",
            "Count of images encoded = 3000\n",
            "Count of images encoded = 3500\n",
            "Count of images encoded = 4000\n",
            "Count of images encoded = 4500\n",
            "Count of images encoded = 5000\n",
            "Encoded 4999 images; total time taken = 2155.1622138023376 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obqJHRsT4wIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## pickle the image feature dictionary data structure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9Aic7IsD1HH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "174b19b5-0574-4327-edf4-38c73129f98d"
      },
      "source": [
        "len(img_encodings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv0aT24jCqb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b078d154-f147-4aef-9c94-9c2a4142f86e"
      },
      "source": [
        "OPDIR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPnxy5UQizdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94735856-ec71-4d4a-ade2-6d1779f3f44e"
      },
      "source": [
        "## pickle the data\n",
        "with open(OPDIR+'val2017_all_5k_images_encoded_features_pickled.pkl', 'wb') as handle:\n",
        "    pickle.dump(img_encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"Pickling done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickling done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOTXOpXZHe5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f70eb2db-a605-43d3-e960-9cd347e0322b"
      },
      "source": [
        "print(f\"Size of pickled data = {sys.getsizeof(img_encodings)} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of pickled data = 147560 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqTFmKA9H1F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del img_encodings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vUFz2yRFVET",
        "colab_type": "text"
      },
      "source": [
        "## DO EVERYTIME - RELOAD FROM PICKLE FILE: Image encodings info obtained from encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LFKNtvGizgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## reload from picked file\n",
        "if True:\n",
        "  with open(OPDIR+'val2017_all_5k_images_encoded_features_pickled.pkl', 'rb') as handle:\n",
        "    img_encodings = pickle.load(handle)\n",
        "\n",
        "#img_encodings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pei5ZjJVcmy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afece55d-7300-43f1-b5fd-67218f9aa30d"
      },
      "source": [
        "print(f\"Size of reloaded pickled data = {sys.getsizeof(img_encodings)} bytes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of reloaded pickled data = 147560 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEP9_umbDDhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpxOwVOvDGMK",
        "colab_type": "text"
      },
      "source": [
        "## DO EVERYTIME - GloVe vectors (Global Vectors for Word Representation)\n",
        "### Citing GloVe: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
        "### Link: More details on this here- https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJocw3c8EkGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Download pre-trained word vectors\n",
        "###    This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/.\n",
        "###    Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n",
        "###    Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n",
        "###    Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n",
        "###    Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
        "\n",
        "### Am using glove.6B.zip on 14.09.2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moq11gtbDDj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15b28a5b-f59a-4aff-dad6-bacb07affd48"
      },
      "source": [
        "#Load GloVe vectors (Global Vectors for Word Representation) - 200 dimensional representation being used\n",
        "\n",
        "embeddings_index_glove200 = {} # empty dictionary\n",
        "with open(WORD_EMBEDDINGS_DIR+'glove.6B.200d.txt', encoding=\"utf-8\") as fembed:\n",
        "  for line in fembed:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index_glove200[word] = coefs\n",
        "\n",
        "print(f\"Number of words in this embeddings matrix = {len(embeddings_index_glove200)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in this embeddings matrix = 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2kSHsrSLk3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28762eb5-ca75-49ab-e0a0-5be139268a61"
      },
      "source": [
        "print(f\"Size of word embeddings file = {sys.getsizeof(embeddings_index_glove200)/1000000} MB\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of word embeddings file = 20.971616 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNrRcWtLk6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj5nOYpuNd0I",
        "colab_type": "text"
      },
      "source": [
        "## Defining the DECODER RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebjB5FNPNw-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## parameters to define model\n",
        "EMBEDDING_DIMS = 200\n",
        "VOCAB_SIZE = ???"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gplBJKXMMtTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the model\n",
        "\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length_caption,))\n",
        "se1 = Embedding(VOCAB_SIZE, EMBEDDING_DIMS, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model_RNN_decoder = keras.Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf3nksIHMtV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcCovEmiMtYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length_caption,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbnH3UKjMtbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WIUI7peMtdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}