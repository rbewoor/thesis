{"cells":[{"metadata":{"id":"23dZtOGmFhQB","trusted":true},"cell_type":"code","source":"!ls '../'","execution_count":1,"outputs":[{"output_type":"stream","text":"input  lib  working\r\n","name":"stdout"}]},{"metadata":{"id":"RuFZIchrFhSp","trusted":true},"cell_type":"code","source":"!ls '../input/'","execution_count":2,"outputs":[{"output_type":"stream","text":"coco-2017-dataset  thesis-imgcapmydata-1\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/'","execution_count":3,"outputs":[{"output_type":"stream","text":"annotations  test2017  train2017  val2017\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/annotations/'","execution_count":4,"outputs":[{"output_type":"stream","text":"captions_train2017.json   instances_val2017.json\r\ncaptions_val2017.json\t  person_keypoints_train2017.json\r\ninstances_train2017.json  person_keypoints_val2017.json\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/thesis-imgcapmydata-1/'","execution_count":5,"outputs":[{"output_type":"stream","text":"gdrive_coco_train2017_118287_listdir.pkl\r\nglove200_embeddings_dict_1.pkl\r\ntrain2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl\r\nval2017_all_5k_images_encoded_features_pickled_2.pkl\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/train2017/' | wc -l","execution_count":6,"outputs":[{"output_type":"stream","text":"118287\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/val2017/' | wc -l","execution_count":7,"outputs":[{"output_type":"stream","text":"5000\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/'","execution_count":8,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir '../working/weights/'","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/weights/'","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"G-rBS0Z-E4p8"},"cell_type":"markdown","source":"# Training model on the Coco Train2017 dataset\n\n## Had already uploaded all 118287 images to Google drive and captures the os.listdir on the folder. Then in sets of 5k in that order of slicing the array created subsets. Ran those images through the bottleneck CNN-Encoder to get the image feature arrays. So will continue doing this.\n\n## Total Images = 20000 images\n## Train data = 19500 images\n## Test  data = 500  images\n\n## Note: each image will have multiple captions (up to 5 as some may be discarded)\n\n## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n## Image captioning with Keras"},{"metadata":{"id":"buTVBtveEpqr","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport json\nimport time\nimport datetime\nimport string\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport re\nimport pickle\n#import itertools\nfrom sklearn.model_selection import train_test_split\nimport PIL\nimport PIL.Image\n\n## import numpy as np\n## from numpy import array\n## import pandas as pd\n## import matplotlib.pyplot as plt\n## %matplotlib inline\n## import string\n## import os\n## from PIL import Image\n## import glob\n## from pickle import dump, load\n## from time import time\n## from keras.preprocessing import sequence\n## from keras.models import Sequential\n## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n## from keras.optimizers import Adam, RMSprop\n## from keras.layers.wrappers import Bidirectional\n## from keras.layers.merge import add\n## from keras.applications.inception_v3 import InceptionV3\n## from keras.preprocessing import image\n## from keras.models import Model\n## from keras import Input, layers\n## from keras import optimizers\n## from keras.applications.inception_v3 import preprocess_input\n## from keras.preprocessing.text import Tokenizer\n## from keras.preprocessing.sequence import pad_sequences\n## from keras.utils import to_categorical","execution_count":12,"outputs":[]},{"metadata":{"id":"ebw9HVoxEptM","outputId":"84656bc7-27ad-46cd-e2fe-bab33c5c0032","trusted":false},"cell_type":"code","source":"#from google.colab import drive\n#drive.flush_and_unmount()\n#drive.mount('/content/gdrive')","execution_count":null,"outputs":[]},{"metadata":{"id":"0b0aXNyCEd3p","trusted":true},"cell_type":"code","source":"## Kaggle versions\nOPDIR = r'../working/'\nIPDIR_IMGS = r'../input/coco-2017-dataset/coco2017/train2017/'\nIPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\nIPDIR_IMGENCODINGS = r'../input/thesis-imgcapmydata-1/'\nIPDIR_EMBEDMATRIX = r'../input/thesis-imgcapmydata-1/'\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n\n## Google drive versions\n#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"IGBalAoSEjmf","trusted":false},"cell_type":"markdown","source":"## ALWAYS - Data Preprocessing — Images - RELOAD FROM PICKLE FILE: Image encodings info obtained from encoder\n### e.g. a val2017 dataset image entry is: img_encodings['000000179765'] should be as below:\n\n#### array([0.14290808, 0.14481388, 0.30199888, ..., 0.20583029, 0.1378399 ,\n####        0.05842396], dtype=float32)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pickled_encodings_dicts = [\n    'train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl',\n    'train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl',\n    'train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl',\n    'train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl'\n]\n## reload from picked file and merge them all into one dict\nimg_encodings = {}\nfor idx, encodings_dict_pickle_file in enumerate(pickled_encodings_dicts):\n    with open(IPDIR_IMGENCODINGS+encodings_dict_pickle_file, 'rb') as handle:\n        img_encodings.update(pickle.load(handle))\n    print(f\"Number of entries in encodings dict = {len(img_encodings)}\")\n\nprint(f\"Final count = {len(img_encodings)}\")","execution_count":14,"outputs":[{"output_type":"stream","text":"Number of entries in encodings dict = 5000\nNumber of entries in encodings dict = 10000\nNumber of entries in encodings dict = 15000\nNumber of entries in encodings dict = 20000\nFinal count = 20000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## should be about = 147kB for each 5k subset\nprint(f\"Size of reloaded pickled data = {sys.getsizeof(img_encodings)} bytes\")","execution_count":15,"outputs":[{"output_type":"stream","text":"Size of reloaded pickled data = 589936 bytes\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_encodings['000000002892']","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"array([0.5667935 , 0.28265882, 0.07251582, ..., 0.5203242 , 0.05947859,\n       0.34467962], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"NWNaS_12FJts"},"cell_type":"markdown","source":"## ONLY ONCE - create \"descriptions\" dictionary"},{"metadata":{"id":"kU4qudInEpvf","trusted":true},"cell_type":"code","source":"## Now, we create a dictionary named “descriptions” which contains the name of the image\n## (without the .jpg extension) as keys and a list captions for the corresponding\n## image as values.","execution_count":17,"outputs":[]},{"metadata":{"id":"PMN9aeYBFaJa"},"cell_type":"markdown","source":"### Load the info from the Train2017 annotations file (i.e. captions_train2017.json) and process it"},{"metadata":{"id":"bO75f_hAEpx8","outputId":"2d1a878e-ae55-4e91-cb4f-e9c9e5a00f4e","trusted":true},"cell_type":"code","source":"os.listdir(IPDIR_ANNO)","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"['captions_val2017.json',\n 'person_keypoints_val2017.json',\n 'instances_val2017.json',\n 'captions_train2017.json',\n 'person_keypoints_train2017.json',\n 'instances_train2017.json']"},"metadata":{}}]},{"metadata":{"id":"14VHYPxjEp0f","outputId":"4039688e-0732-4743-a69f-d29ea933aa1f","trusted":true},"cell_type":"code","source":"## from the annotations file, load the captions\n\nwith open(IPDIR_ANNO+'captions_train2017.json', 'r') as f:\n  data = json.load(f)\n  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n  #type(data['annotations']) # is a list\n  #type(data['images'])      # also is a list\n\ndfanno = pd.DataFrame(data=data['annotations'])\n# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n# dfanno.dtypes =\n#   image_id     int64\n#   id           int64\n#   caption     object\n#   dtype: object\n\ndfimages = pd.DataFrame(data=data['images'])\n# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n# dfimages.dtypes =\n#   license           int64\n#   file_name        object\n#   coco_url         object\n#   height            int64\n#   width             int64\n#   date_captured    object\n#   flickr_url       object\n#   id                int64\n#   dtype: object\n## of above, am dropping useless columns\ndfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n\n## columns remaining in the dfs are:\n# dfanno columns are      image_id , id , caption\n#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n# dfimages columns are    file_name        , height ,  width , id\n#                         000000397133.jpg , 427    ,  640   , 397133\n\n## the captions are not ordered for each image and seem to be randomly placed\n\ndfanno.head(3)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"   image_id  id                                            caption\n0    203564  37  A bicycle replica with a clock as the front wh...\n1    322141  49  A room with blue walls and a white sink and door.\n2     16977  89  A car that seems to be parked illegally behind...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>203564</td>\n      <td>37</td>\n      <td>A bicycle replica with a clock as the front wh...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>322141</td>\n      <td>49</td>\n      <td>A room with blue walls and a white sink and door.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16977</td>\n      <td>89</td>\n      <td>A car that seems to be parked illegally behind...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"bh35WWkGEp3A","outputId":"5bd89a4b-84b5-4d4f-ce98-9a6db170c57d","trusted":true},"cell_type":"code","source":"## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n## the file_name column has the actual image name from the \"image\" key section\ndfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\ndfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\ndfanno.rename(columns={'id_x':'id'}, inplace=True)\ndfanno.head(3)","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"   image_id   id                                            caption  \\\n0    203564   37  A bicycle replica with a clock as the front wh...   \n1    203564  181                    The bike has a clock as a tire.   \n2    203564  478  A black metal bicycle with a clock inside the ...   \n\n          file_name  \n0  000000203564.jpg  \n1  000000203564.jpg  \n2  000000203564.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>203564</td>\n      <td>37</td>\n      <td>A bicycle replica with a clock as the front wh...</td>\n      <td>000000203564.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>203564</td>\n      <td>181</td>\n      <td>The bike has a clock as a tire.</td>\n      <td>000000203564.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>203564</td>\n      <td>478</td>\n      <td>A black metal bicycle with a clock inside the ...</td>\n      <td>000000203564.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"GljG2CRREp5s","trusted":true},"cell_type":"code","source":"## Mapping image with captions using dictionary\n\ndef create_descriptions_dictionary(_dfin):\n    descriptions = {}\n    for row in _dfin.itertuples():\n      rowdict = row._asdict()\n      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n      img_caption = rowdict['caption']\n      if(img_filename not in descriptions):\n        descriptions[img_filename] = [img_caption]\n      else:\n        descriptions[img_filename].append(img_caption)\n    return descriptions\n\ndescriptions = create_descriptions_dictionary(dfanno)","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## At this stage the descriptions dict has info for ALL train images.\n## Since I am only using the first 4 subsets, access those filenames and retain that info:\n### Using the img_encodings entries as reference (key is the file name, same key used by the descriptions dict), retain only in the descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(descriptions)  ## will be for all 118287 images of Train","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"118287"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"retain_dict = {}\nfor img in img_encodings.keys():\n    retain_dict.update({img:descriptions[img]})\ndescriptions = retain_dict.copy()\ndel retain_dict\nprint(f\"remaning entries in descriptions = {len(descriptions)}\")  ## will be for all 118287 images of Train","execution_count":23,"outputs":[{"output_type":"stream","text":"remaning entries in descriptions = 20000\n","name":"stdout"}]},{"metadata":{"id":"7IE6hPhcEp8A","outputId":"3e309d9b-780b-4686-b6f3-865ffc3704cf","trusted":true},"cell_type":"code","source":"dict(list(descriptions.items())[11:14])","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"{'000000533900': ['A wooden bathroom with a wooden toilet next to a window.',\n  'There is a wooden bench in a wooden room with a window. ',\n  'A wooden outhouse has a pretty white window.',\n  'An outhouse is shown with a toilet and window.',\n  'A wooden bench in a room made of wood with a white, painted window.'],\n '000000031519': ['A group of rescue workers helping an overturned car',\n  'A group of people are around the upside down car on the street. ',\n  'a car over turned on a city street',\n  'it is a scene of a crash with a vehicle overturned.',\n  'An overturned car at the intersection of a city street.'],\n '000000005802': ['Two men wearing aprons working in a commercial-style kitchen.',\n  'Chefs preparing food in a professional metallic style kitchen.',\n  'Two people standing around in a large kitchen.',\n  'A commercial kitchen with two men working to prepare several plates.',\n  'two men in white shirts in a large steel kitchen']}"},"metadata":{}}]},{"metadata":{"id":"u30hMY1uEp-t","outputId":"bc9d7b4a-7542-4ff1-ad6f-826c2e97cb46","trusted":true},"cell_type":"code","source":"## example of caption with a hyphen - in the text\ndescriptions['000000005802']","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"['Two men wearing aprons working in a commercial-style kitchen.',\n 'Chefs preparing food in a professional metallic style kitchen.',\n 'Two people standing around in a large kitchen.',\n 'A commercial kitchen with two men working to prepare several plates.',\n 'two men in white shirts in a large steel kitchen']"},"metadata":{}}]},{"metadata":{"id":"u0w7L185EqBU","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"pOVUkimpI819"},"cell_type":"markdown","source":"### Data cleaning on the \"descriptions\" dictionary values\n\n### Following clean up done:\n### 1. lower casing\n### 2. punctuation removal\n### 3. remove any words with length = 1\n### 4. remove any words with alphanumeric"},{"metadata":{"id":"vluW2RFnEqD4","trusted":true},"cell_type":"code","source":"# prepare translation table for removing punctuation\ntable = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        # tokenize\n        desc = desc.split()\n        # convert to lower case\n        desc = [word.lower() for word in desc]\n        # remove punctuation from each token\n        desc = [w.translate(table) for w in desc]\n        # remove hanging 's' and 'a'\n        desc = [word for word in desc if len(word)>1]\n        # remove tokens with numbers in them\n        desc = [word for word in desc if word.isalpha()]\n        # store as string\n        desc_list[i] =  ' '.join(desc)","execution_count":26,"outputs":[]},{"metadata":{"id":"lY0tQci4Ip_C","outputId":"57238993-d160-4007-b7a1-10c04942bc5b","trusted":true},"cell_type":"code","source":"## example of caption which had the hyphen - in the text\ndescriptions['000000005802']","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"['two men wearing aprons working in commercialstyle kitchen',\n 'chefs preparing food in professional metallic style kitchen',\n 'two people standing around in large kitchen',\n 'commercial kitchen with two men working to prepare several plates',\n 'two men in white shirts in large steel kitchen']"},"metadata":{}}]},{"metadata":{"id":"kFDf0Eq7QHdM","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"dfE6Q0FSQJbM"},"cell_type":"markdown","source":"## ALWAYS - Using the descriptions dictionary, split IMAGE NAMES data into a train and val sets.\n### Will make two lists containing only the file name without the .jpg"},{"metadata":{"id":"RY5I3-yZQHgT","outputId":"4eb1324a-40e7-4562-8e42-58518c2d0385","trusted":true},"cell_type":"code","source":"all_imgs_for_splitting = [key for key in descriptions.keys()]\n\nTRAIN_SIZE_N = 19500 ## means balance used as the val data size.\ntrain_imgs_arr, val_imgs_arr = train_test_split(all_imgs_for_splitting, train_size = TRAIN_SIZE_N, random_state=444)\n\nprint(f\"Sizes of: Train = {len(train_imgs_arr)},  Val = {len(val_imgs_arr)},  Train = {len(all_imgs_for_splitting)}\")\ndel all_imgs_for_splitting","execution_count":28,"outputs":[{"output_type":"stream","text":"Sizes of: Train = 19500,  Val = 500,  Train = 20000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPDIR","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"'../working/'"},"metadata":{}}]},{"metadata":{"id":"_ISreES_WuAE","trusted":true},"cell_type":"code","source":"## pickle the train and val lists\nwith open(OPDIR+'train_imgs_19500.pkl', 'wb') as handle:\n  pickle.dump(train_imgs_arr, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Train img arr Pickling done\")\nwith open(OPDIR+'val_imgs_500.pkl', 'wb') as handle:\n  pickle.dump(val_imgs_arr, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Val img arr Pickling done\")","execution_count":30,"outputs":[{"output_type":"stream","text":"Train img arr Pickling done\nVal img arr Pickling done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"aixmm4GbVf8n"},"cell_type":"markdown","source":"## ALWAYS - Add the    startseq    and    endseq    tokens to all the descriptions and create 3 sets: one for all the descriptions, one for the train, one for the test\n\n### At end of this section we will have four dictionaries of captions:\n###    variable --------------------------------------------------- start/end seq present ---- which images\n### 1) descriptions ------------------------------------------------------ NO ------------------------- all 5k\n### 2) descriptions_all_start_end_seq ------------------------ YES -------------------------- all 5k\n### 3) descriptions_train_start_end_seq -------------------- YES ----------------------- train number (4800)\n### 4) descriptions_val_start_end_seq ----------------------- YES ----------------------- val number (200)"},{"metadata":{"id":"9ZOSv26-VfLT","outputId":"6fbe0073-c3e7-4cf5-a850-c737171c69ae","trusted":true},"cell_type":"code","source":"## see the unchanged descriptions - note no startseq and endseq present now\ni=0\nfor k, v in descriptions.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":31,"outputs":[{"output_type":"stream","text":"000000071988 ['man in wheelchair and another sitting on bench that is overlooking the water', 'two people sitting on dock looking at the ocean', 'two older people sitting down in front of beach', 'an old couple at the beach during the day', 'person on bench and one on wheelchair sitting by seawall looking out toward the ocean']\n000000581177 ['kitchen is shown with wooden cabinets and wooden celling', 'recently remodeled kitchen with marble and wooden furnishings', 'well kept kitchen with marble counter tops and stainless steel fridge', 'beautiful new kitchen with natural wood cabinets', 'neat wood filled kitchen is photographed as if for real estate ad']\n000000208408 ['person walking in the rain on the sidewalk', 'person walking through the rain with an umbrella', 'person walking in the rain while holding an umbrella', 'man walks down the strip as it rains', 'european street scene with person and vehicles']\n000000505386 ['four urinals in public restroom with window', 'bathroom with four urinals on the wall', 'an old bathroom with row of urinals', 'row of urinals in with an open window above them', 'row of urinals stand beneath open window']\n000000321064 ['small wooden table covered with delicious vegetables', 'dining table with bowl of garlic cloves', 'kitchen table with juicer and vegetable on it', 'kitchen table with vegetables and food processor', 'food processor sit on small table in kitchen next to fresh vegetables and colander of mushrooms']\n","name":"stdout"}]},{"metadata":{"id":"s9anjGgafTg7","trusted":true},"cell_type":"code","source":"## add startseq and endseq to the captions of all descriptions\n\ndescriptions_all_start_end_seq = descriptions.copy()\n\nfor k, v in descriptions_all_start_end_seq.items():\n  updated_cap_arr = [''.join(['startseq ', caption, ' endseq']) for caption in v]\n  descriptions_all_start_end_seq.update( { k : updated_cap_arr } )","execution_count":32,"outputs":[]},{"metadata":{"id":"vGWuZDqKfTjp","outputId":"239b4fc9-7863-4223-9167-5a6339156720","trusted":true},"cell_type":"code","source":"## see the unchanged descriptions - note no startseq and endseq present STILL\ni=0\nfor k, v in descriptions.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":33,"outputs":[{"output_type":"stream","text":"000000071988 ['man in wheelchair and another sitting on bench that is overlooking the water', 'two people sitting on dock looking at the ocean', 'two older people sitting down in front of beach', 'an old couple at the beach during the day', 'person on bench and one on wheelchair sitting by seawall looking out toward the ocean']\n000000581177 ['kitchen is shown with wooden cabinets and wooden celling', 'recently remodeled kitchen with marble and wooden furnishings', 'well kept kitchen with marble counter tops and stainless steel fridge', 'beautiful new kitchen with natural wood cabinets', 'neat wood filled kitchen is photographed as if for real estate ad']\n000000208408 ['person walking in the rain on the sidewalk', 'person walking through the rain with an umbrella', 'person walking in the rain while holding an umbrella', 'man walks down the strip as it rains', 'european street scene with person and vehicles']\n000000505386 ['four urinals in public restroom with window', 'bathroom with four urinals on the wall', 'an old bathroom with row of urinals', 'row of urinals in with an open window above them', 'row of urinals stand beneath open window']\n000000321064 ['small wooden table covered with delicious vegetables', 'dining table with bowl of garlic cloves', 'kitchen table with juicer and vegetable on it', 'kitchen table with vegetables and food processor', 'food processor sit on small table in kitchen next to fresh vegetables and colander of mushrooms']\n","name":"stdout"}]},{"metadata":{"id":"02IQe_SRfTmR","outputId":"3ff0fdbc-8f01-4e2b-a120-0145d1ae590c","trusted":true},"cell_type":"code","source":"## see the changed descriptions - note startseq and endseq IS PRESENT in this copy\ni=0\nfor k, v in descriptions_all_start_end_seq.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":34,"outputs":[{"output_type":"stream","text":"000000071988 ['startseq man in wheelchair and another sitting on bench that is overlooking the water endseq', 'startseq two people sitting on dock looking at the ocean endseq', 'startseq two older people sitting down in front of beach endseq', 'startseq an old couple at the beach during the day endseq', 'startseq person on bench and one on wheelchair sitting by seawall looking out toward the ocean endseq']\n000000581177 ['startseq kitchen is shown with wooden cabinets and wooden celling endseq', 'startseq recently remodeled kitchen with marble and wooden furnishings endseq', 'startseq well kept kitchen with marble counter tops and stainless steel fridge endseq', 'startseq beautiful new kitchen with natural wood cabinets endseq', 'startseq neat wood filled kitchen is photographed as if for real estate ad endseq']\n000000208408 ['startseq person walking in the rain on the sidewalk endseq', 'startseq person walking through the rain with an umbrella endseq', 'startseq person walking in the rain while holding an umbrella endseq', 'startseq man walks down the strip as it rains endseq', 'startseq european street scene with person and vehicles endseq']\n000000505386 ['startseq four urinals in public restroom with window endseq', 'startseq bathroom with four urinals on the wall endseq', 'startseq an old bathroom with row of urinals endseq', 'startseq row of urinals in with an open window above them endseq', 'startseq row of urinals stand beneath open window endseq']\n000000321064 ['startseq small wooden table covered with delicious vegetables endseq', 'startseq dining table with bowl of garlic cloves endseq', 'startseq kitchen table with juicer and vegetable on it endseq', 'startseq kitchen table with vegetables and food processor endseq', 'startseq food processor sit on small table in kitchen next to fresh vegetables and colander of mushrooms endseq']\n","name":"stdout"}]},{"metadata":{"id":"VxQlzQbVflqc","trusted":true},"cell_type":"code","source":"## using the train and val arrays of image names (which match our keys in descriptions),\n## create the descriptiosn for the train and val","execution_count":35,"outputs":[]},{"metadata":{"id":"UZkXBERmgDfk","outputId":"0aefc1dc-1d50-4b56-ddb5-e5b9d9ce57b8","trusted":true},"cell_type":"code","source":"## make copies of all images descriptions, then remove suitable items from each\ndescriptions_train_start_end_seq = descriptions_all_start_end_seq.copy()\ndescriptions_val_start_end_seq = descriptions_all_start_end_seq.copy()\n\nfor key_train in train_imgs_arr:\n  del descriptions_val_start_end_seq[key_train]\nfor key_val in val_imgs_arr:\n  del descriptions_train_start_end_seq[key_val]\nprint(f\"Sizes of descriptions dicts for: Train = {len(descriptions_train_start_end_seq)},  Val = {len(descriptions_val_start_end_seq)},  Train = {len(descriptions_all_start_end_seq)}\")","execution_count":36,"outputs":[{"output_type":"stream","text":"Sizes of descriptions dicts for: Train = 19500,  Val = 500,  Train = 20000\n","name":"stdout"}]},{"metadata":{"id":"z_yXM-eQgDii","outputId":"f358f1cf-efce-4d05-ab96-3d4e049a8d07","trusted":true},"cell_type":"code","source":"## see the train descriptions - note startseq and endseq IS PRESENT\ni=0\nfor k, v in descriptions_train_start_end_seq.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":37,"outputs":[{"output_type":"stream","text":"000000071988 ['startseq man in wheelchair and another sitting on bench that is overlooking the water endseq', 'startseq two people sitting on dock looking at the ocean endseq', 'startseq two older people sitting down in front of beach endseq', 'startseq an old couple at the beach during the day endseq', 'startseq person on bench and one on wheelchair sitting by seawall looking out toward the ocean endseq']\n000000581177 ['startseq kitchen is shown with wooden cabinets and wooden celling endseq', 'startseq recently remodeled kitchen with marble and wooden furnishings endseq', 'startseq well kept kitchen with marble counter tops and stainless steel fridge endseq', 'startseq beautiful new kitchen with natural wood cabinets endseq', 'startseq neat wood filled kitchen is photographed as if for real estate ad endseq']\n000000208408 ['startseq person walking in the rain on the sidewalk endseq', 'startseq person walking through the rain with an umbrella endseq', 'startseq person walking in the rain while holding an umbrella endseq', 'startseq man walks down the strip as it rains endseq', 'startseq european street scene with person and vehicles endseq']\n000000505386 ['startseq four urinals in public restroom with window endseq', 'startseq bathroom with four urinals on the wall endseq', 'startseq an old bathroom with row of urinals endseq', 'startseq row of urinals in with an open window above them endseq', 'startseq row of urinals stand beneath open window endseq']\n000000321064 ['startseq small wooden table covered with delicious vegetables endseq', 'startseq dining table with bowl of garlic cloves endseq', 'startseq kitchen table with juicer and vegetable on it endseq', 'startseq kitchen table with vegetables and food processor endseq', 'startseq food processor sit on small table in kitchen next to fresh vegetables and colander of mushrooms endseq']\n","name":"stdout"}]},{"metadata":{"id":"GbWdnj-sfltJ","outputId":"605731da-e610-4d47-a50d-76e93cf08772","trusted":true},"cell_type":"code","source":"## see the val descriptions - note startseq and endseq IS PRESENT\ni=0\nfor k, v in descriptions_val_start_end_seq.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":38,"outputs":[{"output_type":"stream","text":"000000242363 ['startseq bathroom with sink toilet and shower bathtub combination endseq', 'startseq bathroom with vanity sink and toilet but empty walls endseq', 'startseq bathroom complete with tub toilet and sink endseq', 'startseq picture of three piece bathroom that is all white and beige endseq', 'startseq an empty bathroom with toilet bathtub and sink endseq']\n000000146112 ['startseq man standing next to bike inside of building endseq', 'startseq man is standing on platform with his bicycle dressed in jersey and bike shorts with two women at table behind him endseq', 'startseq man wearing biking apparel standing inside building with his bike endseq', 'startseq man speaking into microphone on stage with bicycle and dressed in cyclist gear endseq', 'startseq man in cycling clothes drinking beside his bicycle endseq']\n000000469840 ['startseq man and child on red and black motorcycle endseq', 'startseq man in overalls sitting on motorcycle with baby endseq', 'startseq man and boy get ready to ride motorcycle endseq', 'startseq man and young boy sitting on black and red motorcycle endseq', 'startseq man riding red motorcycle with young boy on back endseq']\n000000047263 ['startseq man stops by truck with dog endseq', 'startseq man and his dog stand near truck on side of road endseq', 'startseq man standing next to truck parked on the side of road endseq', 'startseq small dog is running up to truck endseq', 'startseq man outside along road with truck and dog endseq']\n000000248381 ['startseq white kitten standing in front of water bowl endseq', 'startseq cat on the ground eating food out of bowl endseq', 'startseq white cat looking into its water bowl endseq', 'startseq cat is looking at its water dish endseq', 'startseq white cat staring at green bowl filled with water endseq']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(descriptions_train_start_end_seq.keys())[:5]","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"['000000071988',\n '000000581177',\n '000000208408',\n '000000505386',\n '000000321064']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## NOTE : I think i need to only descriptions_train_start_end_seq to build vocab (all or culled). Then on its wordtoix, add 1 to make that VOCAB_LEN for the model. So deleting uselss:\ndel descriptions_val_start_end_seq\ndel descriptions_all_start_end_seq","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"8DURpFw2hpSX","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"rfk-y5SjLKlo"},"cell_type":"markdown","source":"# Vocabulary building\n## ALWAYS - Create the vocabulary based on the captions in the \"descriptions_train_start_end_seq\" dictionary\n\n###**NOTE: FOR NOW USING THE ALL THE 5K IMAGES CAPTIONS**\n###**LATER CONSIDER USING ONLY THE TRAIN 4800 CAPTIONS!!!!!!!**\n\n### First for all the words\n### Then culled based on frequency > threshold"},{"metadata":{"id":"CRAYM1REIqEk","outputId":"ce647ba0-2357-4fc4-e59f-75129a2ac8a0","trusted":true},"cell_type":"code","source":"## earlier was using descriptions BUT NOW using descriptions_train_start_end_seq\nvocabulary = set()\n#for key in descriptions.keys():\n#    [vocabulary.update(d.split()) for d in descriptions[key]]\nfor key in descriptions_train_start_end_seq.keys():\n    [vocabulary.update(d.split()) for d in descriptions_train_start_end_seq[key]]\nprint(f\"Original Vocabulary Size: {len(vocabulary)}\")","execution_count":41,"outputs":[{"output_type":"stream","text":"Original Vocabulary Size: 12982\n","name":"stdout"}]},{"metadata":{"id":"5lBBO5n0IqHG","trusted":true},"cell_type":"code","source":"## This means we have 12982 unique words across all the captions for the 19500 train set images\n## \n## However, if we think about it, many of these words will occur very few times, \n## say 1, 2 or 3 times. Since we are creating a predictive model, we would not like \n## to have all the words present in our vocabulary but the words which are more \n## likely to occur or which are common. This helps the model become more robust \n## to outliers and make less mistakes.\n##\n## Hence we consider only those words which occur at least 10 times in the entire corpus.","execution_count":42,"outputs":[]},{"metadata":{"id":"n5aOjK8aqRgj","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xcfUg_3NqUT1"},"cell_type":"markdown","source":"### Cull vocabulary based on frequency > threshold\n### Capturing new data into variable   vocab_threshold"},{"metadata":{"id":"gFP5GJaVIqJ-","outputId":"cf5f68dd-e2da-48ca-827a-5fe7cb98a168","trusted":true},"cell_type":"code","source":"# Create a list of all the training captions\n## for now treating all the 5k images in coco val2017 as training data\n##     so using all the captions\n\nall_captions = []\n#for key, val in descriptions.items():   ## LATER MAYBE USE descriptions_train_start_end_seq - YES TRYING THAT NOW\nfor key, val in descriptions_train_start_end_seq.items():   ## LATER MAYBE USE descriptions_train_start_end_seq - YES TRYING THAT NOW\n    for cap in val:\n        all_captions.append(cap)\n\n# Consider only words which occur at least 10 times in the corpus\nword_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab_threshold = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n\nprint(f\"Preprocessed words occurring more than threshold = {word_count_threshold} times , \\n={len(vocab_threshold)}\")","execution_count":43,"outputs":[{"output_type":"stream","text":"Preprocessed words occurring more than threshold = 10 times , \n=3207\n","name":"stdout"}]},{"metadata":{"id":"-KdRyQzEK4Ke","trusted":true},"cell_type":"code","source":"## Thus from 12982 unique words in original vocabulary,\n## the count of unique words is now 3207 in the vocab_threshold\n\n## So we now have two vocabulary variables:\n#### 1) vocabulary\n####    list of ALL unique words in our 5k captions\n#### 2) vocab_threshold\n####    list of unique words occuring MORE than the threshold limit\n\n## NOTE: Eventually though we will be zero-padding, so the total words = len(vocab_threshold) + 1\n##       Additional one index for the 0\n###      i.e. 3207 + 1 = 3208","execution_count":44,"outputs":[]},{"metadata":{"id":"-SIaSsAMK4QA","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"mrKPGXxBlqk3"},"cell_type":"markdown","source":"## ALWAYS - Data Preprocessing — Captions\n### 1) Build the word and index mapping dictionaries based on the reduced vocabulary.\n### 2) Find the maximum length of captions\n###    NOTE: This is using the train images descriptions WITH the startseq and endseq word insertions"},{"metadata":{"id":"zal9N9qbhQli","trusted":true},"cell_type":"code","source":"### We want to predict the captions. So while training, the captions are the target variables (Y)\n### But the prediction of caption happens word by word, not in one go.\n### So encode each word into a fixed sized vector later (GloVe).\n### Here create the word-to-index and index-to-word dictionaries.\n###\n### We represent every unique word in the vocabulary by an integer (index).\n### From the culled vocab (high freq words including the zero), we have 1523 unique words\n###      in our corpus and thus each word will be represented by an integer index between 1 to 1523.\n###\n### Create two dictionaries:\n###        wordtoix[‘abc’] -> returns index of the word ‘abc’\n###        ixtoword[k] -> returns the word whose index is ‘k’","execution_count":45,"outputs":[]},{"metadata":{"id":"wfpa8FOVhQof","trusted":true},"cell_type":"code","source":"## NOTE: using the vocab_threhold\nixtoword = {}\nwordtoix = {}\nix = 1\nfor w in vocab_threshold:\n  wordtoix[w] = ix\n  ixtoword[ix] = w\n  ix += 1","execution_count":46,"outputs":[]},{"metadata":{"id":"rHgD3I5nhQrg","trusted":true},"cell_type":"code","source":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n all_desc = list()\n for key in descriptions.keys():\n  [all_desc.append(d) for d in descriptions[key]]\n return all_desc\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n lines = to_lines(descriptions)\n return max(len(d.split()) for d in lines)","execution_count":47,"outputs":[]},{"metadata":{"id":"1CulNF4sthpN"},"cell_type":"markdown","source":"### Find value of MAX_LENGTH_CAPTION\n\n#### MAYBE TRY LATER WITH THE DESCRIPTIONS WITH THE STARTSEQ AND ENDSEQ -- trying that now\n#### I.E. VARAIBLE descriptions_all_start_end_seq - BUT with descriptions_train_start_end_seq as the vocab_threhold built with it too"},{"metadata":{"id":"hO34f9oQtg8z","outputId":"a52d98ef-7fe5-488a-a731-e09ac331c539","trusted":true},"cell_type":"code","source":"# determine the maximum sequence length\n#MAX_LENGTH_CAPTION = max_length(descriptions)  ## will be used directly later while defining Decoder model\nMAX_LENGTH_CAPTION = max_length(descriptions_train_start_end_seq)  ## will be used directly later while defining Decoder model\nprint(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")","execution_count":48,"outputs":[{"output_type":"stream","text":"Max Description Length: 43\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"[cap for val in descriptions_train_start_end_seq.values() for cap in val][:8]","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"['startseq man in wheelchair and another sitting on bench that is overlooking the water endseq',\n 'startseq two people sitting on dock looking at the ocean endseq',\n 'startseq two older people sitting down in front of beach endseq',\n 'startseq an old couple at the beach during the day endseq',\n 'startseq person on bench and one on wheelchair sitting by seawall looking out toward the ocean endseq',\n 'startseq kitchen is shown with wooden cabinets and wooden celling endseq',\n 'startseq recently remodeled kitchen with marble and wooden furnishings endseq',\n 'startseq well kept kitchen with marble counter tops and stainless steel fridge endseq']"},"metadata":{}}]},{"metadata":{"id":"doPdv9RAK4im","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=[cap for val in descriptions_train_start_end_seq.values() for cap in val],columns=['sent'])\ndf['caplen'] = df['sent'].str.split().apply(len)\ndf['sent'][df['caplen'] == 43]","execution_count":50,"outputs":[{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"33587    startseq dining car of train with people seate...\n95376    startseq pink door way shows white walled room...\nName: sent, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=[cap for val in descriptions_train_start_end_seq.values() for cap in val],columns=['sent'])\ncount = df['sent'].str.split().apply(len).value_counts()\ncount.index = count.index.astype(str)# + ' words:'\ncount.sort_index(inplace=True, ascending=False)\ncount.head(10)","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"9     19041\n8      7648\n7       707\n6         1\n43        2\n42        2\n41        4\n4         1\n39        3\n38        4\nName: sent, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"EJgRpN5MjJD3"},"cell_type":"markdown","source":"## Data Preparation using Generator Function"},{"metadata":{"id":"7v8nlMCHYock","trusted":true},"cell_type":"code","source":"## Hereafter, I will try to explain the remaining steps by taking a sample example as follows:\n## Consider we have 3 images and their 3 corresponding captions as follows:\n## \n## (Train image 1) Caption -> The black cat sat on grass\n## (Train image 2) Caption -> The white cat is walking on road\n## (Test image) Caption -> The black cat is walking on grass\n## \n## Now, let’s say we use the first two images and their captions to train the model and the third image to test our model.\n## Now the questions that will be answered are: how do we frame this as a supervised learning problem?, what does the data matrix look like? how many data points do we have?, etc.\n## First we need to convert both the images to their corresponding 2048 length feature vector as discussed above. Let “Image_1” and “Image_2” be the feature vectors of the first two images respectively\n## Secondly, let’s build the vocabulary for the first two (train) captions by adding the two tokens “startseq” and “endseq” in both of them: (Assume we have already performed the basic cleaning steps)\n## \n## Caption_1 -> “startseq the black cat sat on grass endseq”\n## Caption_2 -> “startseq the white cat is walking on road endseq”\n## \n## vocab = {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n## \n## Let’s give an index to each word in the vocabulary:\n## black -1, cat -2, endseq -3, grass -4, is -5, on -6, road -7, sat -8, startseq -9, the -10, walking -11, white -12\n## \n## Now let’s try to frame it as a supervised learning problem where we have a set of data points D = {Xi, Yi}, where Xi is the feature vector of data point ‘i’ and Yi is the corresponding target variable.\n## \n## Let’s take the first image vector Image_1 and its corresponding caption “startseq the black cat sat on grass endseq”. Recall that, Image vector is the input and the caption is what we need to predict. But the way we predict the caption is as follows:\n## For the first time, we provide the image vector and the first word as input and try to predict the second word, i.e.:\n## Input = Image_1 + ‘startseq’; Output = ‘the’\n## Then we provide image vector and the first two words as input and try to predict the third word, i.e.:\n## Input = Image_1 + ‘startseq the’; Output = ‘cat’\n## And so on . . .\n## \n## Thus, we can summarize the data matrix for one image and its corresponding caption as follows:\n## Step 1 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq\" :: Target Word = \"the\"\n## Step 2 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the\" :: Target Word = \"black\"\n## Step 3 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black\" :: Target Word = \"cat\"\n## Step 4 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat\" :: Target Word = \"sat\"\n## Step 5 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat\" :: Target Word = \"on\"\n## Step 6 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on\" :: Target Word = \"grass\"\n## Step 7 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on grass\" :: Target Word = \"endseq\"\n## \n## It must be noted that, one image+caption is not a single data point but are multiple data points depending on the length of the caption.\n## All the above 7 data points together constitute the full data for one image and its caption!!!!\n## Similarly for second images, there will be multiple steps together that consitute its full data point.\n## \n## We must now understand that in every data point, it’s not just the image which goes as input to the system, but also, a partial caption which helps to predict the next word in the sequence.\n## Since we are processing sequences, we will employ a Recurrent Neural Network to read these partial captions (more on this later).\n## However, we have already discussed that we are not going to pass the actual English text of the caption, rather we are going to pass the sequence of indices where each index represents a unique word.\n## \n## Since we have already created an index for each word, let’s now replace the words with their indices and understand how the data matrix will look like:\n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6 4\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"9 10\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6 3\" :: Target Word = \"3\"\n## \n## Since we would be doing batch processing (explained later), we need to make sure that each sequence is of equal length. Hence we need to append 0’s (zero padding) at the end of each sequence. But how many zeros should we append in each sequence?\n## Well, this is the reason we had calculated the maximum length of a caption. So we will append those many number of zeros which will lead to every sequence having a length = maximum length of caption.\n## \n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 0 0 ...]\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 0 0 ...]\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 0 0 ...]\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 0 0 ...]\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 4 0 0 ...]\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 0 0 ...]\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 0 0 ...]\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 0 0 ...]\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 0 0 ...]\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 0 0 ...]\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 3 0 0 ...]\" :: Target Word = \"3\"\n## Appended adequete 0's to each partial caption to make its length = max length\n## \n## \n## \n## Need for a Data Generator:\n## In the above example, I have only considered 2 images and captions which have lead to 15 data points.\n## However, in our actual training dataset we have 6000 images, each having 5 captions. This makes a total of 30000 images and captions.\n## Even if we assume that each caption on an average is just 7 words long, it will lead to a total of 30000*7 i.e. 210000 data points.\n## \n## Compute the size of the data matrix:\n## \n## \n##    img_vector = 2048        partial caption (length = max caption length)\n## ---------------------------------------------------------------------------\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## ---------------------------------------------------------------------------\n## Say above matrix has n rows, m columns, so Size of the data matrix = n*m\n## n-> number of data points (assumed as 210000)  (6000 images * 5 captions per image * 7 words average legnth of each caption)\n## m-> length of each data point\n##     = 2048 + length of partial caption (say something)\n##     = 2048 + something\n## \n## Now this \"something\"  NOTE EQUAL to the max length of caption!\n## Every word (or index) will be mapped (embedded) to higher dimensional space through one of the word embedding techniques.\n## As we using GloVe-200, each embedding has 200 floats representing each number.\n## \n## So with each caption sentence consisting of max length caption size word-indexes, each word represented by 200 dimensional value:\n##    Assuming max lenght of caption = 45\n##    means \"something\" = 2048 + ( 45 * 200 ) = 2048 + 9000 = 11048 float values to represent each sentence\n## \n## Therefore, size of data matrix = m*n = 210000 * 11048 = xxx float values!!!\n## Assuming float takes 2 bytes (very conservative), that still means xxx * 2 = xxx GB\n## \n## This is pretty huge requirement and even if we are able to manage to load this much data into the RAM, it will make the system very slow.\n## For this reason we use data generators a lot in Deep Learning. Data Generators are a functionality which is natively implemented in Python. The ImageDataGenerator class provided by the Keras API is nothing but an implementation of generator function in Python.\n## \n## So how does using a generator function solve this problem?\n## If you know the basics of Deep Learning, then you must know that to train a model on a particular dataset, we use some version of Stochastic Gradient Descent (SGD) like Adam, Rmsprop, Adagrad, etc.\n## With SGD, we do not calculate the loss on the entire data set to update the gradients. Rather in every iteration, we calculate the loss on a batch of data points (typically 64, 128, 256, etc.) to update the gradients.\n## \n## This means that we do not require to store the entire dataset in the memory at once. Even if we have the current batch of points in the memory, it is sufficient for our purpose.\n## A generator function in Python is used exactly for this purpose. It’s like an iterator which resumes the functionality from the point it left the last time it was called.\n## To understand more about Generators, please read here (https://wiki.python.org/moin/Generators).\n## ","execution_count":53,"outputs":[]},{"metadata":{"id":"dQkTM-yDjHkU","trusted":true},"cell_type":"code","source":"# data generator, use during the call to model.fit_generator() to create batchwise data\ndef data_generator_1(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in _descriptions.items():\n            n+=1\n            # retrieve the encoded features of image\n            img_feat = _imgs_features_arr[ key ] # + '.jpg' ]  ## I am not using the .jpg in the key of image features array\n            for desc in desc_list:\n                # encode the sequence\n                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n                    # encode output sequence\n                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n                    # store\n                    X1.append(img_feat)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            # yield the batch data\n            if n == _images_per_batch:\n                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n                yield [np.array(X1), np.array(X2)], np.array(y)\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data generator, use during the call to model.fit_generator() to create batchwise data\ndef data_generator_2(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in _descriptions.items():\n            n+=1\n            # retrieve the encoded features of image\n            img_feat = _imgs_features_arr[ key ] # + '.jpg' ]  ## I am not using the .jpg in the key of image features array\n            for desc in desc_list:\n                # encode the sequence\n                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n                    # encode output sequence\n                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n                    # store\n                    X1.append(img_feat)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            # yield the batch data\n            if n == _images_per_batch:\n                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n                yield [np.array(X1), np.array(X2)], np.array(y)\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":55,"outputs":[]},{"metadata":{"id":"DGg7LIE5bHfV","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"rJ6W0IYDJVHZ"},"cell_type":"markdown","source":"## ALWAYS - RELOAD FROM PICKLE FILE:  Reload the GloVe vectors related data structures\n\n### Sizes of reloaded pickled data should be =\n#### Size of embeddings_dict_glove200 = 20.971616 MB\n\n### Citing GloVe: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]GloVe: Global Vectors for Word Representation. [pdf] [bib]\n### Link: More details on this here- https://nlp.stanford.edu/projects/glove/"},{"metadata":{"id":"pSnmzft5JROf","trusted":true},"cell_type":"code","source":"## Already downloaded the glove file and used it to create the embeddings dict and pickled it. Reloading now.\n###    This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/.\n###    Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n###    Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n###    Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n###    Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n\n### Am using glove.6B.zip on 14.09.2020\n### Then its glove.6B.200d.txt file for the 200 dimensional word embeddings","execution_count":56,"outputs":[]},{"metadata":{"id":"mYWwBYpgJRRI","outputId":"b849b063-ee52-47f4-c2fc-9790897c9cdd","trusted":true},"cell_type":"code","source":"IPDIR_EMBEDMATRIX","execution_count":57,"outputs":[{"output_type":"execute_result","execution_count":57,"data":{"text/plain":"'../input/thesis-imgcapmydata-1/'"},"metadata":{}}]},{"metadata":{"id":"vfblkwZHJRTw","outputId":"cbba178e-2e77-4121-a7fb-0eba5661b7e0","trusted":true},"cell_type":"code","source":"## reload from picked file\n\nif True:\n  with open(IPDIR_EMBEDMATRIX+'glove200_embeddings_dict_1.pkl', 'rb') as handle:\n    embeddings_dict_glove200 = pickle.load(handle)\n\nprint(f\"Number of words in this embeddings file = {len(embeddings_dict_glove200)}\")\nprint(f\"Shape of random word = {embeddings_dict_glove200['1917'].shape}\")\nprint(f\"Shape of another random word = {embeddings_dict_glove200['sculpture'].shape}\")\nprint(f\"Size of embeddings_dict_glove200 = {sys.getsizeof(embeddings_dict_glove200)/1000000} MB\")","execution_count":58,"outputs":[{"output_type":"stream","text":"Number of words in this embeddings file = 400000\nShape of random word = (200,)\nShape of another random word = (200,)\nSize of embeddings_dict_glove200 = 20.971624 MB\n","name":"stdout"}]},{"metadata":{"id":"yFVSHJCcJRWE","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"POFL697yPHMV"},"cell_type":"markdown","source":"### Now create the EMBEDDINGS MATRIX using the embeddings dict"},{"metadata":{"id":"Zi6FyTCPjHsd","trusted":true},"cell_type":"code","source":"## Now, for all the 1522 unique words in our vocabulary, we create an embedding matrix\n##      which will be loaded into the model before training.","execution_count":59,"outputs":[]},{"metadata":{"id":"VORQIdfbh0G4","outputId":"3a9fc04f-19a9-4b3b-c648-53cea7b636a6","trusted":true},"cell_type":"code","source":"len(wordtoix)","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"3207"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )","execution_count":62,"outputs":[{"output_type":"stream","text":"1 15 530\n","name":"stdout"}]},{"metadata":{"id":"Q7o8RlR4PFkJ","outputId":"d8b92bb4-cbe9-4955-f684-30356744f8e9","trusted":true},"cell_type":"code","source":"EMBEDDING_DIMS = 200\n#VOCAB_SIZE = 7244 # number of unique words in the vocabulary\nVOCAB_SIZE = len(wordtoix) + 1 # number of unique high freq words in the vocabulary + 1 (one index for the 0)\n\nembedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIMS))\nfor word, i in wordtoix.items():\n    #if i < max_words:\n    ## using .get(word) method and not [word] as second method throws key error, while first returns none.\n    ##        E.g.    print(embeddings_dict_glove200.get('sculture')) = None\n    ##            But print(embeddings_dict_glove200['sculture'}) will break with key error\n    embedding_vector = embeddings_dict_glove200.get(word)\n    if embedding_vector is not None:\n        # Words not found in the embedding index will be all zeros\n        embedding_matrix[i] = embedding_vector\n\nprint(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")","execution_count":63,"outputs":[{"output_type":"stream","text":"Shape of embedding_matrix = (3208, 200)\n","name":"stdout"}]},{"metadata":{"id":"1zczwbWePFmn","outputId":"b5c9c135-2849-45e3-d47b-4881d766a1b2","trusted":true},"cell_type":"code","source":"## e.g. of a word PRESENT the GoVe200 data file\nmytestword = 'sculpture'\nprint(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\nprint(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\nprint(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")\n## it should match","execution_count":64,"outputs":[{"output_type":"stream","text":"Index position of word sculpture in the vocabulary = 2580\n\nEmbedding vector for word 'sculpture' from GloVe directly = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n -0.1479    -0.62678  ]\n\nembedding_matrix entry for 'sculpture' = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n -0.1479    -0.62678  ]\n","name":"stdout"}]},{"metadata":{"id":"pnQW2VwOPFo_","outputId":"4d9bcb11-17c4-47d9-daf5-e0610ed6d8b0","trusted":true},"cell_type":"code","source":"## e.g. of a word NOT PRESENT the GoVe200 data file\n## answer None in all cases since the word is not in the vocabulary itself the other two will throw key error\nmytestword = 'sculture'\nprint(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\nprint(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\nprint(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")","execution_count":65,"outputs":[{"output_type":"stream","text":"Index position of word sculture in the vocabulary = None\n\nEmbedding vector for word 'sculpture' from GloVe directly = None\n\nembedding_matrix entry for 'sculture' = None\n","name":"stdout"}]},{"metadata":{"id":"MTxczXFBRp85","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CGzQr4tiiWDc"},"cell_type":"markdown","source":"## ALWAYS - Define the RNN Decoder model\n## Use the GloVe embeddings matrix values to update layer weights and freeze ONLY those weights\n\n\n### About Keras Emebedding layer\n### \n#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n######tf.keras.layers.Embedding(\n######    input_dim, output_dim,\n######    embeddings_initializer=\"uniform\",\n######    embeddings_regularizer=None,\n######    activity_regularizer=None,\n######    embeddings_constraint=None,\n######    mask_zero=False,\n######    input_length=None,\n######    **kwargs\n######)"},{"metadata":{"id":"_EVJLMeGRp_Y","trusted":true},"cell_type":"code","source":"# Decoder Model defining\n\n## parameters to define model\n#EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n#VOCAB_SIZE is initialised earlier\n#MAX_LENGTH_CAPTION is initialised earlier\n\ninputs1 = keras.Input(shape=(2048,))\nfe1 = keras.layers.Dropout(0.5)(inputs1)\nfe2 = keras.layers.Dense(256, activation='relu')(fe1)\n\n# partial caption sequence model\ninputs2 = keras.Input(shape=(MAX_LENGTH_CAPTION,))\nse1 = keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIMS, mask_zero=True)(inputs2)\nse2 = keras.layers.Dropout(0.5)(se1)\nse3 = keras.layers.LSTM(256)(se2)\n\n# decoder (feed forward) model\ndecoder1 = keras.layers.add([fe2, se3])\ndecoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\noutputs = keras.layers.Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n\n# merge the two input models\nmodel_RNN_decoder = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)","execution_count":66,"outputs":[]},{"metadata":{"id":"OHWDqfjiRqB4","outputId":"e55a7148-8854-431d-c667-785f81063188","trusted":false},"cell_type":"code","source":"#model_RNN_decoder.summary()\n\n#This was the output:\n#Model: \"functional_3\"\n#__________________________________________________________________________________________________\n#Layer (type)                    Output Shape         Param #     Connected to                     \n#==================================================================================================\n#input_7 (InputLayer)            [(None, 45)]         0                                            \n#__________________________________________________________________________________________________\n#input_6 (InputLayer)            [(None, 2048)]       0                                            \n#__________________________________________________________________________________________________\n#embedding_1 (Embedding)         (None, 45, 200)      304600      input_7[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_3 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_4 (Dropout)             (None, 45, 200)      0           embedding_1[0][0]                \n#__________________________________________________________________________________________________\n#dense_2 (Dense)                 (None, 256)          524544      dropout_3[0][0]                  \n#__________________________________________________________________________________________________\n#lstm_1 (LSTM)                   (None, 256)          467968      dropout_4[0][0]                  \n#__________________________________________________________________________________________________\n#add (Add)                       (None, 256)          0           dense_2[0][0]                    \n#                                                                 lstm_1[0][0]                     \n#__________________________________________________________________________________________________\n#dense_3 (Dense)                 (None, 256)          65792       add[0][0]                        \n#__________________________________________________________________________________________________\n#dense_4 (Dense)                 (None, 1523)         391411      dense_3[0][0]                    \n#==================================================================================================\n#Total params: 1,754,315\n#Trainable params: 1,754,315\n#Non-trainable params: 0\n#__________________________________________________________________________________________________","execution_count":null,"outputs":[]},{"metadata":{"id":"1rFiBoJyRqHg","outputId":"cf620edb-6810-45b9-fec1-5d9d637c9442","trusted":false},"cell_type":"code","source":"#tf.keras.utils.plot_model(model_RNN_decoder, to_file=OPDIR+'RNN_decoder_model_plot_1.jpg', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ETH9PxyGxpt-","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CMKYtXCQxexx"},"cell_type":"markdown","source":"### ALWAYS - set weights using the GloVe embedding matrix and freeze it"},{"metadata":{"id":"6c0oR60fRqJZ","outputId":"9dbf8c22-edfb-428b-e172-7c7b03bc60c5","trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":67,"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"(3208, 200)"},"metadata":{}}]},{"metadata":{"id":"naYfB0FjwocE","trusted":true},"cell_type":"code","source":"model_RNN_decoder.layers[2].set_weights([embedding_matrix])\nmodel_RNN_decoder.layers[2].trainable = False","execution_count":68,"outputs":[]},{"metadata":{"id":"G4HreAB0woha","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CMFFw_g5xy7r"},"cell_type":"markdown","source":"### DO ONCE - RNN Decoder summary and plot\n#### AFTER loading GloVe embeddings and Freezing"},{"metadata":{"id":"K_mLYJjsRqMO","outputId":"809ebcdd-f972-4f47-f60f-a207404054a5","trusted":false},"cell_type":"code","source":"#model_RNN_decoder.summary()\n\n#This was the output:\n#Model: \"functional_3\"\n#__________________________________________________________________________________________________\n#Layer (type)                    Output Shape         Param #     Connected to                     \n#==================================================================================================\n#input_7 (InputLayer)            [(None, 45)]         0                                            \n#__________________________________________________________________________________________________\n#input_6 (InputLayer)            [(None, 2048)]       0                                            \n#__________________________________________________________________________________________________\n#embedding_1 (Embedding)         (None, 45, 200)      304600      input_7[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_3 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_4 (Dropout)             (None, 45, 200)      0           embedding_1[0][0]                \n#__________________________________________________________________________________________________\n#dense_2 (Dense)                 (None, 256)          524544      dropout_3[0][0]                  \n#__________________________________________________________________________________________________\n#lstm_1 (LSTM)                   (None, 256)          467968      dropout_4[0][0]                  \n#__________________________________________________________________________________________________\n#add (Add)                       (None, 256)          0           dense_2[0][0]                    \n#                                                                 lstm_1[0][0]                     \n#__________________________________________________________________________________________________\n#dense_3 (Dense)                 (None, 256)          65792       add[0][0]                        \n#__________________________________________________________________________________________________\n#dense_4 (Dense)                 (None, 1523)         391411      dense_3[0][0]                    \n#==================================================================================================\n#Total params: 1,754,315\n#Trainable params: 1,449,715\n#Non-trainable params: 304,600\n#__________________________________________________________________________________________________","execution_count":null,"outputs":[]},{"metadata":{"id":"8lX2xaz1xxrF","outputId":"12854ec9-d37b-4a85-b364-ed191b6613ee","trusted":false},"cell_type":"code","source":"#tf.keras.utils.plot_model(model_RNN_decoder, to_file=OPDIR+'RNN_decoder_model_plot_AFTER_GLOVE_1.jpg', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vxU_TgOGxxt6","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"iNo79p0Az37z","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"XB81Ohfoz4lL"},"cell_type":"markdown","source":"### ALWAYS - Compile RNN model"},{"metadata":{"id":"CF5Dw9brxxwh","trusted":true},"cell_type":"code","source":"model_RNN_decoder.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":69,"outputs":[]},{"metadata":{"id":"c6aKMocpRqXH","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hIqp5fxB0ldP","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xdJW1J2YmrxO"},"cell_type":"markdown","source":"## ALWAYS - Train the RNN Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(OPDIR + 'weights/')","execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":"[]"},"metadata":{}}]},{"metadata":{"id":"EXJ2Yjuz05wG","outputId":"c0ab13d8-b8d2-4450-ef87-589ebd49b46c","trusted":true},"cell_type":"code","source":"## values set earlier - for reference\n\nprint(f\"MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nprint(f\"VOCAB_SIZE = {VOCAB_SIZE}\")","execution_count":70,"outputs":[{"output_type":"stream","text":"MAX_LENGTH_CAPTION = 43\nVOCAB_SIZE = 3208\n","name":"stdout"}]},{"metadata":{"id":"J5Dx5QnMmrJ1","outputId":"ec46884f-bb34-4244-8b6f-ff525a537581","trusted":true},"cell_type":"code","source":"## For reference: def data_generator(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch):\n## Notes:\n##      1) Ok to use the img_encodings array of Train and Test images. COZ function starts with the image name as key from the descriptions.\n\n########### Phase 1 #########\n\nprint(f\"\\n\\nTraining Phase 1 started at :: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\nLR_1 = 0.0005\nBATCH_SIZE_1 = 8   ## how many images per batch\nN_EPOCHS_1 = 15\nSTEPS_PER_EPOCH_1 = len(descriptions_train_start_end_seq) // BATCH_SIZE_1\nprint(f\"\\nPhase 1 parameters:\")\nprint(f\"STEPS_PER_EPOCH_1 = {STEPS_PER_EPOCH_1}\")\nprint(f\"BATCH_SIZE_1 = {BATCH_SIZE_1}\")\nprint(f\"N_EPOCHS_1 = {N_EPOCHS_1}\")\n\nmodel_RNN_decoder.optimizer.lr = LR_1\nfor i in range(N_EPOCHS_1):\n    generator_1 = data_generator_1(descriptions_train_start_end_seq, img_encodings, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_1, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_1, verbose=1)\n    if (i+1) >= 5 or i//2 == 1:\n        model_RNN_decoder.save_weights( OPDIR + r'weights/' + 'RNNDecoder_Weights_Ph1_ep' + str(i) + '.h5' )","execution_count":null,"outputs":[{"output_type":"stream","text":"\n\nTraining Phase 1 started at :: 2020-09-19 11:44:27\n\n\n\nPhase 1 parameters:\nSTEPS_PER_EPOCH_1 = 2437\nBATCH_SIZE_1 = 8\nN_EPOCHS_1 = 15\n  80/2437 [..............................] - ETA: 23:29 - loss: 5.9900","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Phase 2 #########\n\nprint(f\"\\n\\nTraining Phase 2 started at :: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\nLR_2 = 0.0001\nBATCH_SIZE_2 = 16   ## how many images per batch\nN_EPOCHS_2 = 15\nSTEPS_PER_EPOCH_2 = len(descriptions_train_start_end_seq) // BATCH_SIZE_2\nprint(f\"\\nPhase 2 parameters:\")\nprint(f\"STEPS_PER_EPOCH_2 = {STEPS_PER_EPOCH_2}\")\nprint(f\"BATCH_SIZE_2 = {BATCH_SIZE_2}\")\nprint(f\"N_EPOCHS_2 = {N_EPOCHS_2}\")\nprint(f\"LR_2 = {LR_2}\")\n\nmodel_RNN_decoder.optimizer.lr = LR_2\nfor i in range(N_EPOCHS_2):\n    generator_2 = data_generator_2(descriptions_train_start_end_seq, img_encodings, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_2, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_2, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_2, verbose=1)\n    if True:# (i+1) >= 5 or i//2 == 1:\n        model_RNN_decoder.save_weights( OPDIR + r'weights/' + 'RNNDecoder_Weights_Ph2_ep' + str(i) + '.h5' )","execution_count":null,"outputs":[]},{"metadata":{"id":"4Xc08wQoRqZx","trusted":false},"cell_type":"code","source":"#model_reset_states\n#lr optimizer\n#tuner","execution_count":null,"outputs":[]},{"metadata":{"id":"WOxOw-XI31j9"},"cell_type":"markdown","source":"### Save the Model Weights after training!"},{"metadata":{"id":"UncVLQn738_7","outputId":"dbec1f1e-f668-49a0-d518-38801fdb05ac","trusted":true},"cell_type":"code","source":"OPDIR","execution_count":null,"outputs":[]},{"metadata":{"id":"Iob2tOZWYvxu","outputId":"39bd7967-bac5-4414-a2a9-f260f57b9c8c","trusted":false},"cell_type":"code","source":"#OPDIR + r'Weights/' + 'rbewoorpil_RNNDecoder_Weights_ep10.h5'","execution_count":null,"outputs":[]},{"metadata":{"id":"7BdC8m2VztDp","outputId":"3da9bde0-5c58-4aac-9fc6-e43fbf3f2616","trusted":false},"cell_type":"code","source":"SAVE_TO = OPDIR + r'weights/' + 'kagg_RNNDecoder_Weights_final.h5'\nmodel_RNN_decoder.save_weights( SAVE_TO )\nprint(f\"At {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} :: saved weights to file = { SAVE_TO }\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_XHGDOZaztGQ","trusted":false},"cell_type":"code","source":"## /content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/Weights/rbewoorpil_Encoder_Weights_ep10.h5\n## 10 epochs, all with LR = 0.0001, batch size = 4, MAX_LENGTH_CAPTION = 45, VOCAB_SIZE = 1523, loss='categorical_crossentropy', optimizer='adam', train=4800 images","execution_count":null,"outputs":[]},{"metadata":{"id":"a9kP4Sj01FJM","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"htXEBYJQBocZ"},"cell_type":"markdown","source":"## RELOAD - MODEL 2\n### Weights File: /content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/Weights/rbewoor_Encoder_Weights_Ph2_ep4.h5\n\n### Using the last epochs weight file\n\n### Training Specs:\n### Total 10 epochs, first 5 epochs with smaller BS and larger LR, last 5 epochs with larger BS but smaller LR, MAX_LENGTH_CAPTION = 45, VOCAB_SIZE = 1523, loss='categorical_crossentropy', optimizer='adam', train=4800 images\n\n\n### The weights file was exported earlier using: model_RNN_decoder.save_weights(SAVE_TO_PATH)"},{"metadata":{"id":"bWfws4rTBlxb","trusted":false},"cell_type":"code","source":"Training Phase 1 started at :: 2020-09-18 22:10:51\n\nPhase 1 parameters:\nSTEPS_PER_EPOCH_1 = 1200\nBATCH_SIZE_1 = 4\nN_EPOCHS_1 = 5\nWARNING:tensorflow:From <ipython-input-47-83b52223b490>:21: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use Model.fit, which supports generators.\n1200/1200 [==============================] - 585s 488ms/step - loss: 4.7801\n1200/1200 [==============================] - 599s 499ms/step - loss: 3.8539\n1200/1200 [==============================] - 584s 487ms/step - loss: 3.5118\n1200/1200 [==============================] - 585s 487ms/step - loss: 3.3086\n1200/1200 [==============================] - 579s 483ms/step - loss: 3.1668\n\n\nTraining Phase 2 started at :: 2020-09-18 22:59:49\n\nPhase 2 parameters:\nSTEPS_PER_EPOCH_2 = 600\nBATCH_SIZE_2 = 8\nN_EPOCHS_2 = 5\n600/600 [==============================] - 536s 894ms/step - loss: 3.0436\n600/600 [==============================] - 543s 904ms/step - loss: 2.9905\n600/600 [==============================] - 550s 916ms/step - loss: 2.9629\n600/600 [==============================] - 553s 922ms/step - loss: 2.9448\n600/600 [==============================] - 566s 943ms/step - loss: 2.9222","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reload_rnn_encoder_saved_weights(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n  if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n    ## Decoder Model defining\n\n    ## parameters to define model\n    #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n    #VOCAB_SIZE is initialised earlier\n    #MAX_LENGTH_CAPTION is initialised earlier\n\n    inputs1 = keras.Input(shape=(2048,))\n    fe1 = keras.layers.Dropout(0.5)(inputs1)\n    fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n\n    # partial caption sequence model\n    inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n    se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n    se2 = keras.layers.Dropout(0.5)(se1)\n    se3 = keras.layers.LSTM(256)(se2)\n\n    # decoder (feed forward) model\n    decoder1 = keras.layers.add([fe2, se3])\n    decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n    outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n\n    # merge the two input models\n    reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n\n    print(f\"\\nRNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\\nAttempting to load weights...\")\n    \n    ## load the weights\n    reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n    print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n    return reloaded_rnn_decoder_model\n  else:\n    print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n    return None\n\ndef greedySearch(_decoder_model, _img_encoding, _max_length):\n  in_text = 'startseq'\n  for i in range(_max_length):\n    sequence = [ wordtoix[w] for w in in_text.split() if w in wordtoix ]\n    sequence = keras.preprocessing.sequence.pad_sequences([sequence], maxlen=_max_length)\n    yhat = _decoder_model.predict([_img_encoding,sequence], verbose=0)\n    yhat = np.argmax(yhat)\n    word = ixtoword[yhat]\n    in_text += ' ' + word\n    if word == 'endseq':\n      break\n  caption_out = in_text.split()\n  #caption_out = caption_out[1:-1]  ## drop the startseq and endseq words at either end\n  caption_out = ' '.join(caption_out)\n  return caption_out\n\ndef do_inference_one_image(_infer_idx_pos, _imgs_arr, _IPDIRIMGS, _MAX_LENGTH_CAPTION, _model_RNN_decoder, _descriptions):\n  infer_image_path = _IPDIRIMGS + _imgs_arr[_infer_idx_pos] + '.jpg'\n\n  ## show the original image\n  image = img_encodings[ _imgs_arr[_infer_idx_pos] ].reshape((1,2048))\n  x = plt.imread(infer_image_path)\n  plt.imshow(x)\n  plt.show()\n\n  ## get the prediction caption using greedy search\n  predicted_caption = greedySearch(_model_RNN_decoder, image, _MAX_LENGTH_CAPTION)\n  print(f\"\\nFor image :: {infer_image_path}\\n\\nInference caption output:\\n{ predicted_caption }\")\n  print(\"\")\n  ## show the original captions\n  for idx, orig_cap in enumerate(_descriptions.get(_imgs_arr[_infer_idx_pos])):\n    print(f\"Original caption {idx+1}  :::  {orig_cap}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"HtWOanhYBl0u","outputId":"fcfe63fe-2528-4db7-e690-5686a88bd111","trusted":false},"cell_type":"code","source":"SAVED_WEIGHTS_PATH = OPDIR + r'Weights/' + 'rbewoor_Encoder_Weights_Ph2_ep4.h5'\nSAVED_WEIGHTS_PATH","execution_count":null,"outputs":[]},{"metadata":{"id":"dqVH4VSXDFpv","outputId":"c6b17049-17d8-4c81-b865-cfe1daa783b5","trusted":false},"cell_type":"code","source":"print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}, VOCAB_SIZE = {VOCAB_SIZE}, MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nreloaded_RNN_decoder = reload_rnn_encoder_saved_weights(SAVED_WEIGHTS_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\ntype(reloaded_RNN_decoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"qiPebXBDDFso","outputId":"f67e52c3-4a5f-4026-a74e-c7f29728e5f0","trusted":false},"cell_type":"code","source":"for infer_idx_pos in range(10):\n  print(f\"--------  Inference for position {infer_idx_pos}  --------\")\n  do_inference_one_image(infer_idx_pos, val_imgs_arr, IPDIRIMGS, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, descriptions)\n  print(f\"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"vvjcJEUnDFvO","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"WU29bCwH0qrS","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"BtW2CPgu5q1f","outputId":"499df196-334a-4553-e15a-89bed35b1031","trusted":false},"cell_type":"code","source":"print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}, VOCAB_SIZE = {VOCAB_SIZE}, MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nreloaded_RNN_decoder = reload_rnn_encoder_saved_weights(SAVED_WEIGHTS_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\ntype(reloaded_RNN_decoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"QEHRYSDQ0qvE","outputId":"f581572a-a5c8-4893-afa2-4def47dca1eb","trusted":false},"cell_type":"code","source":"for infer_idx_pos in range(10):\n  print(f\"--------  Inference for position {infer_idx_pos}  --------\")\n  do_inference_one_image(infer_idx_pos, val_imgs_arr, IPDIRIMGS, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, descriptions)\n  print(f\"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"C1SIDyCr0qxi","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"P8ZbxSYJ5YWi","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"vQ7lNrqA5YZb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"4gVwIe1_IvDd"},"cell_type":"markdown","source":"## NEXT SECTION - PENDING"},{"metadata":{"id":"V2-Eq09sIqPH","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hWFW9ZU2IqRx","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"0D3b7izSIqUb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}