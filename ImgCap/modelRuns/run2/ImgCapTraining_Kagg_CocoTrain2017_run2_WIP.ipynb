{"cells":[{"metadata":{"id":"23dZtOGmFhQB","trusted":true},"cell_type":"code","source":"!ls '../'","execution_count":43,"outputs":[{"output_type":"stream","text":"input  lib  working\r\n","name":"stdout"}]},{"metadata":{"id":"RuFZIchrFhSp","trusted":true},"cell_type":"code","source":"!ls '../input/'","execution_count":44,"outputs":[{"output_type":"stream","text":"coco-2017-dataset    thesis-imgcap-imgencodings-1\r\nembeddings-glove200  thesis-imgcap-weights-in-1\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/'","execution_count":45,"outputs":[{"output_type":"stream","text":"annotations  test2017  train2017  val2017\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/train2017/' | wc -l","execution_count":46,"outputs":[{"output_type":"stream","text":"118287\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/val2017/' | wc -l","execution_count":47,"outputs":[{"output_type":"stream","text":"5000\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/coco-2017-dataset/coco2017/annotations/'","execution_count":48,"outputs":[{"output_type":"stream","text":"captions_train2017.json   instances_val2017.json\r\ncaptions_val2017.json\t  person_keypoints_train2017.json\r\ninstances_train2017.json  person_keypoints_val2017.json\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/thesis-imgcap-imgencodings-1/'","execution_count":49,"outputs":[{"output_type":"stream","text":"train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl\r\ntrain2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl\r\nval2017_all_5k_images_encoded_features_pickled_2.pkl\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/thesis-imgcap-imgencodings-1/' | wc -l","execution_count":50,"outputs":[{"output_type":"stream","text":"21\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/embeddings-glove200/'","execution_count":51,"outputs":[{"output_type":"stream","text":"glove200_embeddings_dict_1.pkl\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/thesis-imgcap-weights-in-1/'","execution_count":52,"outputs":[{"output_type":"stream","text":" RNNDecoder_Weights_Ph1_ep11.h5   RNNDecoder_Weights_Ph1_ep6.h5\r\n RNNDecoder_Weights_Ph1_ep14.h5  'ph1 losses.txt'\r\n RNNDecoder_Weights_Ph1_ep2.h5\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/'","execution_count":53,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  weights_out\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir '../working/weights_out/'","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/'","execution_count":57,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  weights_out\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/weights_out/' | wc -l","execution_count":58,"outputs":[{"output_type":"stream","text":"0\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!rm -r '../working/weights_out/'","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"G-rBS0Z-E4p8"},"cell_type":"markdown","source":"# Training model on 97k of some 100k images of Coco_Train2017 dataset. Using the balance 3k as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n\n## Created subsets of the total data and used them to make the image feature encodings during preprocessing to avoid bottlenecking during training.\n\n### Had already uploaded all 118287 images to Google drive.\n### Made folders for subsets of 5k images.\n### Split done using the list from os.listdir(gdrive folder)[ slice_start : slice_end ].\n### Ran those subsets of 5k images through the CNN-Encoder to get one file of encodings of the image feature arrays.\n### The encoder is pre-trained Google Inception-v3 trained on Imagenet.\n\n## AVAILABLE data :\n### Coco_Train2017     = has 118287 images\n### Coco_Val2017       = has 5000   images\n### Combined total     = 123287     images\n\n## USED data :\n### Coco_Train2017     = 100000  images\n### Coco_Val2017       = 5000    images\n### Combined total     = 105000  images\n\n### Using only the first 100k images of Train2017 + all 5k images of Val2017\n### Thus total data available for training = 100k + 5k = 105k\n### Details of split of data:\n### Training   data = 97000 images from Coco_Train2017\n### Validation data = 3000  images from Coco_Train2017\n### Test       data = 5000  images from Coco_Val2017\n\n### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n\n## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n## Image captioning with Keras"},{"metadata":{"id":"buTVBtveEpqr","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport json\nimport time\nimport datetime\nimport string\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport re\nimport pickle\n#import itertools\nimport PIL\nimport PIL.Image\n\n## import numpy as np\n## from numpy import array\n## import pandas as pd\n## import matplotlib.pyplot as plt\n## %matplotlib inline\n## import string\n## import os\n## from PIL import Image\n## import glob\n## from pickle import dump, load\n## from time import time\n## from keras.preprocessing import sequence\n## from keras.models import Sequential\n## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n## from keras.optimizers import Adam, RMSprop\n## from keras.layers.wrappers import Bidirectional\n## from keras.layers.merge import add\n## from keras.applications.inception_v3 import InceptionV3\n## from keras.preprocessing import image\n## from keras.models import Model\n## from keras import Input, layers\n## from keras import optimizers\n## from keras.applications.inception_v3 import preprocess_input\n## from keras.preprocessing.text import Tokenizer\n## from keras.preprocessing.sequence import pad_sequences\n## from keras.utils import to_categorical","execution_count":59,"outputs":[]},{"metadata":{"id":"ebw9HVoxEptM","outputId":"84656bc7-27ad-46cd-e2fe-bab33c5c0032","trusted":true},"cell_type":"code","source":"#from google.colab import drive\n#drive.flush_and_unmount()\n#drive.mount('/content/gdrive')","execution_count":null,"outputs":[]},{"metadata":{"id":"0b0aXNyCEd3p","trusted":true},"cell_type":"code","source":"## Kaggle versions\n\n## Weights from training till now\nOPDIR = r'../working/'\n\n## New weights to save here\nOPDIR_WEIGHTS = r'../working/weights_out/'\n\n## Images locations\nIPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\nIPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n\n## Annotations json file location from where to pick up the captions\nIPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\n\n## Bottleneck CNN Encoder output for all the images to be used in training\nIPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\n\n## using GloVe - 200 file, already created the embedding matrix and pickled\nIPDIR_EMBED_DICT = r'../input/embeddings-glove200/'\n\n## misc files like the combined caption descriptions, etc\nIPDIR_MISC = r'../input/thesis-imgcap-misc-1/'\n\n\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n\n## Google drive versions\n#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"IGBalAoSEjmf","trusted":false},"cell_type":"markdown","source":"## ALWAYS - Data Preprocessing â€” Images - RELOAD FROM PICKLE FILE: Image encodings info obtained from encoder\n\n### Create two encodings dicts from the already picked files of encodings:\n###   1) img_encodings_train_and_val from the Train2017 dataset - used for the training and validation data DURING model training.\n###      Will contain 100k image feature vectors\n###   2) img_encodings_test from the Val2017   dataset - used only for the Testing phase AFTER model training is completed.\n###      Will contain 5k image feature vectors\n\n### e.g. a val2017 dataset image entry is: img_encodings['000000179765'] should be as below:\n#### array([0.14290808, 0.14481388, 0.30199888, ..., 0.20583029, 0.1378399 ,\n####        0.05842396], dtype=float32)"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(IPDIR_IMG_ENCODINGS)","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"['train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl',\n 'train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl',\n 'val2017_all_5k_images_encoded_features_pickled_2.pkl',\n 'train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## From Train2017 images pickled files\nprint(f\"\\n\\nLoading encodings for the train+val images\")\npickled_encodings_files_train2017 = os.listdir(IPDIR_IMG_ENCODINGS)\npickled_encodings_files_train2017.remove('val2017_all_5k_images_encoded_features_pickled_2.pkl')  ## remove the val2017 pickle file\nimg_encodings_train_and_val = {}\nfor idx, subset_encodings_file_train2017 in enumerate(pickled_encodings_files_train2017):\n    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_train2017, 'rb') as handle:\n        img_encodings_train_and_val.update(pickle.load(handle))\n        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_train2017}\")\n    print(f\"Number of entries in img_encodings_train2017 = {len(img_encodings_train_and_val)}\")\nprint(f\"\\nFinal count img_encodings_train_and_val = {len(img_encodings_train_and_val)}. Should be = 100k.\")\n\n\n\n## From Val2017 images pickled files\nprint(f\"\\n\\nLoading encodings for the test images\")\npickled_encodings_files_val2017 = ['val2017_all_5k_images_encoded_features_pickled_2.pkl']\nimg_encodings_test = {}\nfor idx, subset_encodings_file_val2017 in enumerate(pickled_encodings_files_val2017):\n    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_val2017, 'rb') as handle:\n        img_encodings_test.update(pickle.load(handle))\n        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_val2017}\")\n    print(f\"Number of entries in img_encodings_test = {len(img_encodings_test)}\")\nprint(f\"\\nFinal count img_encodings_test = {len(img_encodings_test)}. Should be = 5k.\")\n\ndel pickled_encodings_files_train2017, pickled_encodings_files_val2017","execution_count":62,"outputs":[{"output_type":"stream","text":"\n\nLoading encodings for the train+val images\nLoaded file num 1 :: train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 5000\nLoaded file num 2 :: train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 10000\nLoaded file num 3 :: train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 15000\nLoaded file num 4 :: train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 20000\nLoaded file num 5 :: train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 25000\nLoaded file num 6 :: train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 30000\nLoaded file num 7 :: train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 35000\nLoaded file num 8 :: train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 40000\nLoaded file num 9 :: train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 45000\nLoaded file num 10 :: train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 50000\nLoaded file num 11 :: train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 55000\nLoaded file num 12 :: train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 60000\nLoaded file num 13 :: train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 65000\nLoaded file num 14 :: train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 70000\nLoaded file num 15 :: train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 75000\nLoaded file num 16 :: train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 80000\nLoaded file num 17 :: train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 85000\nLoaded file num 18 :: train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 90000\nLoaded file num 19 :: train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 95000\nLoaded file num 20 :: train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl\nNumber of entries in img_encodings_train2017 = 100000\n\nFinal count img_encodings_train_and_val = 100000. Should be = 100k.\n\n\nLoading encodings for the test images\nLoaded file num 1 :: val2017_all_5k_images_encoded_features_pickled_2.pkl\nNumber of entries in img_encodings_test = 5000\n\nFinal count img_encodings_test = 5000. Should be = 5k.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"NWNaS_12FJts"},"cell_type":"markdown","source":"## ALWAYS - create \"descriptions\" dictionaries mapping an image to its list of captions\n## A descriptions dict has the key   = image name without the .jpg extension\n##                         the value = [list of 5 captions for that image]\n\n### Uses the annotations json files for COCO_Val2017 and COCO_Train2017\n\n### Create two dicts:\n###   1) \"descriptions_train_and_val_dataset\" dict from the Train2017 captions file - used for training and validation data DURING model training.\n###      Will contain 118k images info when first loaded.\n###      As only 100k of this is being used, will cull it to the correct 100k images and their descriptions later.\n###   2) \"descriptions_test_dataset\" from the Val2017 captions file - used only for the Testing phase AFTER model training is completed.\n###      Will contain 5k images info"},{"metadata":{"id":"PMN9aeYBFaJa"},"cell_type":"markdown","source":"### process the captions_train2017.json file - will be used to create our TRAIN AND VALIDATION DATASET of 97k and 3k images respectively.\n### For now, it will have 100k values, but later it will be split using the train_test_split function of scikit_learn"},{"metadata":{"id":"bO75f_hAEpx8","outputId":"2d1a878e-ae55-4e91-cb4f-e9c9e5a00f4e","trusted":true},"cell_type":"code","source":"os.listdir(IPDIR_ANNO)","execution_count":63,"outputs":[{"output_type":"execute_result","execution_count":63,"data":{"text/plain":"['captions_train2017.json',\n 'instances_train2017.json',\n 'captions_val2017.json',\n 'person_keypoints_train2017.json',\n 'instances_val2017.json',\n 'person_keypoints_val2017.json']"},"metadata":{}}]},{"metadata":{"id":"14VHYPxjEp0f","outputId":"4039688e-0732-4743-a69f-d29ea933aa1f","trusted":true},"cell_type":"code","source":"## build the descriptions_train_and_val_dataset dict\n\nwith open(IPDIR_ANNO+'captions_train2017.json', 'r') as f:\n  data = json.load(f)\n  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n  #type(data['annotations']) # is a list\n  #type(data['images'])      # also is a list\n\ndfanno = pd.DataFrame(data=data['annotations'])\n# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n# dfanno.dtypes =\n#   image_id     int64\n#   id           int64\n#   caption     object\n#   dtype: object\n\ndfimages = pd.DataFrame(data=data['images'])\n# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n# dfimages.dtypes =\n#   license           int64\n#   file_name        object\n#   coco_url         object\n#   height            int64\n#   width             int64\n#   date_captured    object\n#   flickr_url       object\n#   id                int64\n#   dtype: object\n## of above, am dropping useless columns\ndfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n\n## columns remaining in the dfs are:\n# dfanno columns are      image_id , id , caption\n#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n# dfimages columns are    file_name        , height ,  width , id\n#                         000000397133.jpg , 427    ,  640   , 397133\n\n## the captions are not ordered for each image and seem to be randomly placed\n\ndfanno.head(3)","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"   image_id  id                                            caption\n0    203564  37  A bicycle replica with a clock as the front wh...\n1    322141  49  A room with blue walls and a white sink and door.\n2     16977  89  A car that seems to be parked illegally behind...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>203564</td>\n      <td>37</td>\n      <td>A bicycle replica with a clock as the front wh...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>322141</td>\n      <td>49</td>\n      <td>A room with blue walls and a white sink and door.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16977</td>\n      <td>89</td>\n      <td>A car that seems to be parked illegally behind...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"bh35WWkGEp3A","outputId":"5bd89a4b-84b5-4d4f-ce98-9a6db170c57d","trusted":true},"cell_type":"code","source":"## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n## the file_name column has the actual image name from the \"image\" key section\ndfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\ndfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\ndfanno.rename(columns={'id_x':'id'}, inplace=True)\ndfanno.head(3)","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"   image_id   id                                            caption  \\\n0    203564   37  A bicycle replica with a clock as the front wh...   \n1    203564  181                    The bike has a clock as a tire.   \n2    203564  478  A black metal bicycle with a clock inside the ...   \n\n          file_name  \n0  000000203564.jpg  \n1  000000203564.jpg  \n2  000000203564.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>203564</td>\n      <td>37</td>\n      <td>A bicycle replica with a clock as the front wh...</td>\n      <td>000000203564.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>203564</td>\n      <td>181</td>\n      <td>The bike has a clock as a tire.</td>\n      <td>000000203564.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>203564</td>\n      <td>478</td>\n      <td>A black metal bicycle with a clock inside the ...</td>\n      <td>000000203564.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"GljG2CRREp5s","trusted":true},"cell_type":"code","source":"## Mapping image with captions using dictionary\n\ndef create_descriptions_dictionary(_dfin):\n    descriptions = {}\n    for row in _dfin.itertuples():\n      rowdict = row._asdict()\n      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n      img_caption = rowdict['caption']\n      if(img_filename not in descriptions):\n        descriptions[img_filename] = [img_caption]\n      else:\n        descriptions[img_filename].append(img_caption)\n    return descriptions\n\ndescriptions_train_and_val = create_descriptions_dictionary(dfanno)\nprint(f\"len(descriptions_train_and_val) = {len(descriptions_train_and_val)}\")\n\ndel dfanno","execution_count":66,"outputs":[{"output_type":"stream","text":"len(descriptions_train_and_val) = 118287\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### process the captions_val2017.json file - will be used to create our TEST DATASET of 5k images"},{"metadata":{"trusted":true},"cell_type":"code","source":"## build the descriptions_test_dataset dict\n\nwith open(IPDIR_ANNO+'captions_val2017.json', 'r') as f:\n  data = json.load(f)\n  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n  #type(data['annotations']) # is a list\n  #type(data['images'])      # also is a list\n\ndfanno = pd.DataFrame(data=data['annotations'])\n# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n# dfanno.dtypes =\n#   image_id     int64\n#   id           int64\n#   caption     object\n#   dtype: object\n\ndfimages = pd.DataFrame(data=data['images'])\n# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n# dfimages.dtypes =\n#   license           int64\n#   file_name        object\n#   coco_url         object\n#   height            int64\n#   width             int64\n#   date_captured    object\n#   flickr_url       object\n#   id                int64\n#   dtype: object\n## of above, am dropping useless columns\ndfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n\n## columns remaining in the dfs are:\n# dfanno columns are      image_id , id , caption\n#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n# dfimages columns are    file_name        , height ,  width , id\n#                         000000397133.jpg , 427    ,  640   , 397133\n\n## the captions are not ordered for each image and seem to be randomly placed\n\ndfanno.head(3)","execution_count":67,"outputs":[{"output_type":"execute_result","execution_count":67,"data":{"text/plain":"   image_id   id                                            caption\n0    179765   38  A black Honda motorcycle parked in front of a ...\n1    179765  182      A Honda motorcycle parked in a grass driveway\n2    190236  401  An office cubicle with four different types of...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>179765</td>\n      <td>38</td>\n      <td>A black Honda motorcycle parked in front of a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>179765</td>\n      <td>182</td>\n      <td>A Honda motorcycle parked in a grass driveway</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>190236</td>\n      <td>401</td>\n      <td>An office cubicle with four different types of...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n## the file_name column has the actual image name from the \"image\" key section\ndfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\ndfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\ndfanno.rename(columns={'id_x':'id'}, inplace=True)\ndfanno.head(3)","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"   image_id   id                                            caption  \\\n0    179765   38  A black Honda motorcycle parked in front of a ...   \n1    179765  182      A Honda motorcycle parked in a grass driveway   \n2    179765  479  A black Honda motorcycle with a dark burgundy ...   \n\n          file_name  \n0  000000179765.jpg  \n1  000000179765.jpg  \n2  000000179765.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>id</th>\n      <th>caption</th>\n      <th>file_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>179765</td>\n      <td>38</td>\n      <td>A black Honda motorcycle parked in front of a ...</td>\n      <td>000000179765.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>179765</td>\n      <td>182</td>\n      <td>A Honda motorcycle parked in a grass driveway</td>\n      <td>000000179765.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>179765</td>\n      <td>479</td>\n      <td>A black Honda motorcycle with a dark burgundy ...</td>\n      <td>000000179765.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Mapping image with captions using dictionary\n\ndef create_descriptions_dictionary(_dfin):\n    descriptions = {}\n    for row in _dfin.itertuples():\n      rowdict = row._asdict()\n      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n      img_caption = rowdict['caption']\n      if(img_filename not in descriptions):\n        descriptions[img_filename] = [img_caption]\n      else:\n        descriptions[img_filename].append(img_caption)\n    return descriptions\n\ndescriptions_test = create_descriptions_dictionary(dfanno)\nprint(f\"len(descriptions_test) = {len(descriptions_test)}\")\n\ndel dfanno","execution_count":69,"outputs":[{"output_type":"stream","text":"len(descriptions_test) = 5000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## At this stage the \"descriptions_train_and_val\"  dict has info for ALL Coco_Train2017 images = 118287 images. Since, for model training only using 100k images, the extra images info needs to be removed.\n### The image encodings contain the 100k image to be retained. So use its keys to retain the correct info in descriptions_train_and_val as they have same key i.e. image file name"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total entries in descriptions_train_and_val_dataset BEFORE culling = {len(descriptions_train_and_val)}\")\nretain_dict = {}\nfor key in img_encodings_train_and_val.keys():\n    retain_dict.update({key:descriptions_train_and_val[key]})\ndescriptions_train_and_val = retain_dict.copy() ## replace with the just identifed 100k images for Train+Validation model training\ndel retain_dict\nprint(f\"Retained entries in descriptions_train_and_val = {len(descriptions_train_and_val)}\")  ## will be for all 100k","execution_count":70,"outputs":[{"output_type":"stream","text":"Total entries in descriptions_train_and_val_dataset BEFORE culling = 118287\nRetained entries in descriptions_train_and_val = 100000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"pOVUkimpI819"},"cell_type":"markdown","source":"## Data cleaning on the descriptions dicts for descriptions_train_and_val\n## First on the descriptions_train_and_val_dataset\n##       Later same cleanup on descriptions_test\n\n### Clean up actions done:\n### 1. lower casing\n### 2. punctuation removal i.e. string.punctuation which coevers '!\" HASH DOLLAR_SIGN %&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' are replaced with a space\n### 3. remove any words with length = 1\n### 4. remove any words now left as non-alphabetic"},{"metadata":{"trusted":true},"cell_type":"code","source":"## example of caption with a hyphen - in the caption\ndescriptions_train_and_val['000000005802']","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"['Two men wearing aprons working in a commercial-style kitchen.',\n 'Chefs preparing food in a professional metallic style kitchen.',\n 'Two people standing around in a large kitchen.',\n 'A commercial kitchen with two men working to prepare several plates.',\n 'two men in white shirts in a large steel kitchen']"},"metadata":{}}]},{"metadata":{"id":"vluW2RFnEqD4","trusted":true},"cell_type":"code","source":"# prepare translation table for removing punctuation\n## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\n## commented table made \"commercial-style kitchen\" into \"commercialstyle kitchen\"\n#tran_table = str.maketrans('', '', string.punctuation)\n## but this table makes \"commercial-style kitchen\" into \"commercial style kitchen\"\ntran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\nfor key, desc_list in descriptions_train_and_val.items():\n    for idx in range(len(desc_list)):\n        desc = desc_list[idx]\n        # replace all punctuation with space in description before tokenizing\n        desc = desc.translate(tran_table)\n        # tokenize\n        desc = desc.split()\n        # convert to lower case\n        desc = [word.lower() for word in desc]\n        # remove hanging 's' and 'a'\n        desc = [word for word in desc if len(word)>1]\n        # remove any non-alphabetic tokens\n        desc = [word for word in desc if word.isalpha()]\n        # overwrite with cleaned description\n        desc_list[idx] =  ' '.join(desc)","execution_count":72,"outputs":[]},{"metadata":{"id":"lY0tQci4Ip_C","outputId":"57238993-d160-4007-b7a1-10c04942bc5b","trusted":true},"cell_type":"code","source":"## example of caption which had the hyphen - in the caption  -- POST CLEANUP\ndescriptions_train_and_val['000000005802']","execution_count":73,"outputs":[{"output_type":"execute_result","execution_count":73,"data":{"text/plain":"['two men wearing aprons working in commercial style kitchen',\n 'chefs preparing food in professional metallic style kitchen',\n 'two people standing around in large kitchen',\n 'commercial kitchen with two men working to prepare several plates',\n 'two men in white shirts in large steel kitchen']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning on the descriptions dicts for descriptions_test_dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(list(descriptions_test.items())[:2])","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"{'000000179765': ['A black Honda motorcycle parked in front of a garage.',\n  'A Honda motorcycle parked in a grass driveway',\n  'A black Honda motorcycle with a dark burgundy seat.',\n  'Ma motorcycle parked on the gravel in front of a garage',\n  'A motorcycle with its brake extended standing outside'],\n '000000190236': ['An office cubicle with four different types of computers.',\n  'The home office space seems to be very cluttered.',\n  'an office with desk computer and chair and laptop.',\n  'Office setting with a lot of computer screens.',\n  'A desk and chair in an office cubicle.']}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## example of caption with accidental newline \\n in the caption\ndescriptions_test['000000482917']","execution_count":75,"outputs":[{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"['A dog sitting between its masters feet on a footstool watching tv\\n',\n 'A dog between the feet of a person looking at a TV.',\n 'A dog and a person are watching television together.',\n 'A person is sitting with their dog watching tv.',\n 'A man relaxing at home, watching television with his dog.']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare translation table for removing punctuation\n## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\ntran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\nfor key, desc_list in descriptions_test.items():\n    for idx in range(len(desc_list)):\n        desc = desc_list[idx]\n        # replace all punctuation with space in description before tokenizing\n        desc = desc.translate(tran_table)\n        # tokenize\n        desc = desc.split()\n        # convert to lower case\n        desc = [word.lower() for word in desc]\n        # remove hanging 's' and 'a'\n        desc = [word for word in desc if len(word)>1]\n        # remove any non-alphabetic tokens\n        desc = [word for word in desc if word.isalpha()]\n        # overwrite with cleaned description\n        desc_list[idx] =  ' '.join(desc)","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## example of caption with accidental newline \\n in the caption  -- POST CLEANUP\ndescriptions_test['000000482917']","execution_count":77,"outputs":[{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"['dog sitting between its masters feet on footstool watching tv',\n 'dog between the feet of person looking at tv',\n 'dog and person are watching television together',\n 'person is sitting with their dog watching tv',\n 'man relaxing at home watching television with his dog']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"dfE6Q0FSQJbM"},"cell_type":"markdown","source":"## ALWAYS - create the Train and Validation datasets using the 100k images and OPTIONALLY pickle the files for reload later\n### TRAIN_SIZE = 97k, balance 3k for VALIDATION_SIZE\n### Split descriptions data i.e. \"descriptions_train_and_val\"   into \"descriptions_train\"  and \"descriptions_val\"\n### Split encodings data    i.e. \"img_encodings_train_and_val\"  into \"img_encodings_train\" and \"img_encodings_val\""},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_and_val_keys = [key for key in img_encodings_train_and_val.keys()]\nprint(f\"Number of keys to be used for the split = {len(all_train_and_val_keys)}\")\n\nTRAIN_SIZE_N = 97000\ntrain_imgs_keys, val_imgs_keys = train_test_split(all_train_and_val_keys, train_size = TRAIN_SIZE_N, random_state=444)\n\nprint(f\"Sizes of: Train dataset = {len(train_imgs_keys)},  Validation dataset = {len(val_imgs_keys)}\")\ndel all_train_and_val_keys","execution_count":78,"outputs":[{"output_type":"stream","text":"Number of keys to be used for the split = 100000\nSizes of: Train dataset = 97000,  Validation dataset = 3000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## use the keys of the just created split files containing the keys to makes the splits for the descriptions and the encodings dicts\n\ndescriptions_train = dict()\ndescriptions_val = dict()\nimg_encodings_train = dict()\nimg_encodings_val = dict()\n\nfor key in train_imgs_keys:\n    descriptions_train.update({key:descriptions_train_and_val.get(key)})\n    img_encodings_train.update({key:img_encodings_train_and_val.get(key)})\n\nfor key in val_imgs_keys:\n    descriptions_val.update({key:descriptions_train_and_val.get(key)})\n    img_encodings_val.update({key:img_encodings_train_and_val.get(key)})\n\nprint(f\"Sizes of: Train description = {len(descriptions_train)},  Validation description = {len(descriptions_val)}\")\nprint(f\"Sizes of: Train encodings   = {len(img_encodings_train)}, Validation encodings = {len(img_encodings_val)}\")\n\ndel train_imgs_keys, val_imgs_keys","execution_count":79,"outputs":[{"output_type":"stream","text":"Sizes of: Train description = 97000,  Validation description = 3000\nSizes of: Train encodings   = 97000, Validation encodings = 3000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPDIR","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"'../working/'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## pickle the descriptions and encodings data - split is 97k Train 3k for Validation\n\n\"\"\"\nPICKLE_FILENAME_descriptions_train  = r'descriptions_train_97000.pkl'\nPICKLE_FILENAME_descriptions_val    = r'descriptions_val_3000.pkl'\nPICKLE_FILENAME_img_encodings_train = r'img_encodings_train_97000.pkl'\nPICKLE_FILENAME_img_encodings_val   = r'img_encodings_val_3000.pkl'\n\nwith open( OPDIR + PICKLE_FILENAME_descriptions_train , 'wb') as handle:\n  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Data for descriptions_train pickled to :: {PICKLE_FILENAME_descriptions_train}\")\n\nwith open( OPDIR + PICKLE_FILENAME_descriptions_val , 'wb') as handle:\n  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Data for descriptions_val pickled to :: {PICKLE_FILENAME_descriptions_val}\")\n\nwith open( OPDIR + PICKLE_FILENAME_img_encodings_train , 'wb') as handle:\n  pickle.dump(img_encodings_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Data for img_encodings_train pickled to :: {PICKLE_FILENAME_descriptions_train}\")\n\nwith open( OPDIR + PICKLE_FILENAME_img_encodings_val , 'wb') as handle:\n  pickle.dump(img_encodings_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprint(f\"Data for img_encodings_val pickled to :: {PICKLE_FILENAME_descriptions_val}\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"aixmm4GbVf8n"},"cell_type":"markdown","source":"## ALWAYS - Add the    \"startseq\"    and    \"endseq\"    tokens to all the training example descriptions"},{"metadata":{"id":"9ZOSv26-VfLT","outputId":"6fbe0073-c3e7-4cf5-a850-c737171c69ae","trusted":true},"cell_type":"code","source":"## see the unchanged descriptions - note no startseq and endseq present now\ni=0\nfor k, v in descriptions_train.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":81,"outputs":[{"output_type":"stream","text":"000000284902 ['people gather around large bunches of green bananas', 'village women are looking at bunches of bananas', 'people in market around table of bananas', 'older women in an outdoor food market filled with bananas', 'bananas after being harvested in an african village']\n000000041027 ['cluttered collection of cooking oil spices and sauces', 'cluttered kitchen shelf with spices and cooking oils', 'shelf with variety of bottled and jarred condiments on it', 'an extreme close up of many different types of bottles', 'cluttered counter has lots of bottles on it including cooking oils']\n000000384924 ['woman walking up to tennis ball so she can hit it', 'woman is playing tennis on court', 'young woman playing tennis on tennis court', 'woman wearing short shorts on top of tennis court', 'woman is preparing to hit tennis ball with racket']\n000000578650 ['dried up riverbed could be dangerous place to play frisbee', 'boy three dogs and frisbee in dried up creek bed', 'boy is playing frisbee in the woods while dogs play nearby', 'some dogs standing by boy waiting for frisbee', 'boy with frisbee and dogs in the bush']\n000000283631 ['pizza has cheese and herbs on it', 'pastry with cheese and greens sitting on plate', 'this is picture of pizza with tomatoes cheese garlic and seasoning', 'pizza with different vegetables on top of it', 'cooked cheese pizza with vegetables and green herbs']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add startseq and endseq to the captions of all entries in descriptions_train\n\nfor k, v in descriptions_train.items():\n    updated_values = [''.join(['startseq ', each_desc, ' endseq']) for each_desc in v]\n    descriptions_train.update( { k : updated_values } )","execution_count":82,"outputs":[]},{"metadata":{"id":"vGWuZDqKfTjp","outputId":"239b4fc9-7863-4223-9167-5a6339156720","trusted":true},"cell_type":"code","source":"## see the updated descriptions - note startseq and endseq IS PRESENT at both ends for all the descriptions\ni=0\nfor k, v in descriptions_train.items():\n  i+=1\n  if i>5:\n    break\n  print(k, v)","execution_count":83,"outputs":[{"output_type":"stream","text":"000000284902 ['startseq people gather around large bunches of green bananas endseq', 'startseq village women are looking at bunches of bananas endseq', 'startseq people in market around table of bananas endseq', 'startseq older women in an outdoor food market filled with bananas endseq', 'startseq bananas after being harvested in an african village endseq']\n000000041027 ['startseq cluttered collection of cooking oil spices and sauces endseq', 'startseq cluttered kitchen shelf with spices and cooking oils endseq', 'startseq shelf with variety of bottled and jarred condiments on it endseq', 'startseq an extreme close up of many different types of bottles endseq', 'startseq cluttered counter has lots of bottles on it including cooking oils endseq']\n000000384924 ['startseq woman walking up to tennis ball so she can hit it endseq', 'startseq woman is playing tennis on court endseq', 'startseq young woman playing tennis on tennis court endseq', 'startseq woman wearing short shorts on top of tennis court endseq', 'startseq woman is preparing to hit tennis ball with racket endseq']\n000000578650 ['startseq dried up riverbed could be dangerous place to play frisbee endseq', 'startseq boy three dogs and frisbee in dried up creek bed endseq', 'startseq boy is playing frisbee in the woods while dogs play nearby endseq', 'startseq some dogs standing by boy waiting for frisbee endseq', 'startseq boy with frisbee and dogs in the bush endseq']\n000000283631 ['startseq pizza has cheese and herbs on it endseq', 'startseq pastry with cheese and greens sitting on plate endseq', 'startseq this is picture of pizza with tomatoes cheese garlic and seasoning endseq', 'startseq pizza with different vegetables on top of it endseq', 'startseq cooked cheese pizza with vegetables and green herbs endseq']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## So now following data is created:\n\n### Image ecnodings from Inception-v3 pretrained on Imagenet\n### Train      data = 97000 images = img_encodings_train variable\n### Validation data = 3000  images = img_encodings_val   variable\n### Test       data = 5000  images = img_encodings_test  variable\n\n### Image names with their descriptions - after cleaning up\n### Train      data = 97000 images = descriptions_train variable : only this has the startseq and endseq tokens inserted at either end\n### Validation data = 3000  images = descriptions_val   variable\n### Test       data = 5000  images = descriptions_test  variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\tlen(img_encodings_val) = {len(img_encodings_val)}\\tlen(img_encodings_test) = {len(img_encodings_test)}\")\nprint(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\tlen(descriptions_val) = {len(descriptions_val)}\\tlen(descriptions_test) = {len(descriptions_test)}\")","execution_count":84,"outputs":[{"output_type":"stream","text":"Encodings data:\nlen(img_encodings_train) = 97000\tlen(img_encodings_val) = 3000\tlen(img_encodings_test) = 5000\nDescriptions data:\nlen(descriptions_train) = 97000\tlen(descriptions_val) = 3000\tlen(descriptions_test) = 5000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"8DURpFw2hpSX","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"rfk-y5SjLKlo"},"cell_type":"markdown","source":"## ALWAYS - Vocabulary building\n## Create the vocabulary based on the captions in the training descriptions with the start and end sequence tokens added at either end\n\n### First for all the words i.e. variable \"vocabulary\"\n### Then culled based on frequency > threshold i.e. variable \"vocab_threshold\"\n#### Later used the size of the wordtoix list (built using vocab_threshold), added 1 to make that equal to VOCAB_LEN for the model. The additional of one is to account for the 0 padding value."},{"metadata":{"id":"CRAYM1REIqEk","outputId":"ce647ba0-2357-4fc4-e59f-75129a2ac8a0","trusted":true},"cell_type":"code","source":"## at this stage the descriptions_train already has the start and end tokens added to it\nvocabulary = set()\nfor key in descriptions_train.keys():\n    [vocabulary.update(d.split()) for d in descriptions_train[key]]\nprint(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")","execution_count":85,"outputs":[{"output_type":"stream","text":"Original Vocabulary Size with all words = 24344\n","name":"stdout"}]},{"metadata":{"id":"5lBBO5n0IqHG","trusted":true},"cell_type":"code","source":"## This means we have 24344 unique words in the corpus - across all the descriptions for the 97000 train set images and including the start and end sequence words\n## \n## However, if we think about it, many of these words will occur very few times, \n## say 1, 2 or 3 times. Since we are creating a predictive model, we would not like \n## to have all the words present in our vocabulary but the words which are more \n## likely to occur or which are common. This helps the model become more robust \n## to outliers and make less mistakes.\n##\n## Hence we consider only those words which more often in the corpus than some chosen threshold value.","execution_count":null,"outputs":[]},{"metadata":{"id":"n5aOjK8aqRgj","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xcfUg_3NqUT1"},"cell_type":"markdown","source":"### Cull vocabulary based on frequency > threshold\n### Capturing new data into variable   vocab_threshold"},{"metadata":{"id":"gFP5GJaVIqJ-","outputId":"cf5f68dd-e2da-48ca-827a-5fe7cb98a168","trusted":true},"cell_type":"code","source":"# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n\nall_desc_in_training_samples = []\nfor key, val in descriptions_train.items():\n    for cap in val:\n        all_desc_in_training_samples.append(cap)\n\nMIN_WORD_COUNT_THRESHOLD = 10\nword_counts = {}\nnsents = 0\nfor each_desc in all_desc_in_training_samples:\n    nsents += 1\n    for w in each_desc.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n\nprint(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")","execution_count":86,"outputs":[{"output_type":"stream","text":"Culled vocabulary to only retain words occurring more than threshold = 10 times.\n=New vocab size , len(vocab_threshold) = 6760\n","name":"stdout"}]},{"metadata":{"id":"-KdRyQzEK4Ke","trusted":true},"cell_type":"code","source":"## Thus from 24344 unique words in original vocabulary,\n## the count of unique words in culled vocab vocab_threshold = 6760\n\n## So we now have two vocabulary variables:\n#### 1) vocabulary\n####    list of ALL unique words in our 97k training captions\n#### 2) vocab_threshold\n####    list of unique words occuring MORE than the threshold limit\n\n## NOTE: Eventually though we will be zero-padding, so the total words = len(vocab_threshold) + 1\n##       Additional one index for the 0-pad value\n###      i.e. 6760 + 1 = 6761","execution_count":87,"outputs":[]},{"metadata":{"id":"-SIaSsAMK4QA","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"mrKPGXxBlqk3"},"cell_type":"markdown","source":"## ALWAYS - Data Preprocessing â€” Captions\n### 1) Build the word and index mapping dictionaries based on the reduced vocabulary.\n### 2) Find the maximum length of caption description\n###    NOTE: This is using the train images descriptions WITH the startseq and endseq word insertions"},{"metadata":{"id":"zal9N9qbhQli","trusted":true},"cell_type":"code","source":"### We want to predict the captions. So while training, the captions are the target variables (Y)\n### But the prediction of caption happens word by word, not in one go.\n### So encode each word into a fixed sized vector later (GloVe).\n### Here create the word-to-index and index-to-word dictionaries.\n###\n### We represent every unique word in the vocabulary by an integer (index).\n### From the culled vocab (high freq words including the zero), we have 6760 unique words\n###      in our corpus and thus each word will be represented by an integer index between 1 to 6760.\n###\n### Create two dictionaries: using the final vocabulary that model should output including the start and end sequence special tokens\n###        wordtoix[â€˜walkingâ€™] -> returns index of the word â€˜walkingâ€™ e.g. it is 32\n###        ixtoword[i] -> returns the word whose index is â€˜iâ€™ e.g. ixtoword[32] will be 'walking'\n### Later, while building the RNN-Decoder model, its parameter VOCAB_SIZE is calculated as = len(wordtoix) + 1","execution_count":89,"outputs":[]},{"metadata":{"id":"wfpa8FOVhQof","trusted":true},"cell_type":"code","source":"## NOTE: using the vocab_threhold\nixtoword = dict()\nwordtoix = dict()\nidx = 1 ## index 1 is starting value as the 0 will be mapped to the 0-pad value\nfor w in vocab_threshold:\n    wordtoix[w] = idx\n    ixtoword[idx] = w\n    idx += 1","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n\n## convert a dictionary of clean descriptions to a list of descriptions\ndef extract_each_desc(_descriptions):\n    all_desc = list()\n    for key in _descriptions.keys():\n        [all_desc.append(d) for d in _descriptions[key]]\n    return all_desc\n\n## find the longest description length\ndef find_max_length_desc(_descriptions):\n    desc_sentences = extract_each_desc(_descriptions)\n    return max(len(d.split()) for d in desc_sentences)\n\nMAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\nprint(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")","execution_count":91,"outputs":[{"output_type":"stream","text":"Max Description Length: 49\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Checking what sentences had the max length and get an idea of how many sentences had what lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"[cap for val in descriptions_train.values() for cap in val][:8]","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"['startseq people gather around large bunches of green bananas endseq',\n 'startseq village women are looking at bunches of bananas endseq',\n 'startseq people in market around table of bananas endseq',\n 'startseq older women in an outdoor food market filled with bananas endseq',\n 'startseq bananas after being harvested in an african village endseq',\n 'startseq cluttered collection of cooking oil spices and sauces endseq',\n 'startseq cluttered kitchen shelf with spices and cooking oils endseq',\n 'startseq shelf with variety of bottled and jarred condiments on it endseq']"},"metadata":{}}]},{"metadata":{"id":"doPdv9RAK4im","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=[cap for val in descriptions_train.values() for cap in val], columns=['sent'])\ndf['caplen'] = df['sent'].str.split().apply(len)\ndf['sent'][df['caplen'] == MAX_LENGTH_CAPTION]","execution_count":93,"outputs":[{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"182480    startseq two men holding tennis rackets up in ...\nName: sent, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data=[cap for val in descriptions_train.values() for cap in val], columns=['sent'])\ncount = df['sent'].str.split().apply(len).value_counts()\ncount.index = count.index.astype(str)\ncount.sort_index(inplace=True, ascending=False)\ncount.head(15)","execution_count":94,"outputs":[{"output_type":"execute_result","execution_count":94,"data":{"text/plain":"9     94580\n8     37816\n7      3537\n6         3\n5         1\n49        1\n48        1\n46        4\n45        1\n44        7\n43        8\n42       14\n41       14\n40       14\n4         2\nName: sent, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"rJ6W0IYDJVHZ"},"cell_type":"markdown","source":"## ALWAYS - RELOAD FROM PICKLE FILE:  Reload the GloVe vectors related data structures\n\n### Sizes of reloaded pickled data should be =\n#### Size of embeddings_dict_glove200 = 20.971616 MB\n\n### Citing GloVe: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]GloVe: Global Vectors for Word Representation. [pdf] [bib]\n### Link: More details on this here- https://nlp.stanford.edu/projects/glove/"},{"metadata":{"id":"pSnmzft5JROf","trusted":true},"cell_type":"code","source":"## Already downloaded the glove file and used it to create the embeddings dict and pickled it. Reloading now.\n###    This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/.\n###    Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n###    Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n###    Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n###    Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n\n### Am using glove.6B.zip on 14.09.2020\n### Then its glove.6B.200d.txt file for the 200 dimensional word embeddings","execution_count":96,"outputs":[]},{"metadata":{"id":"mYWwBYpgJRRI","outputId":"b849b063-ee52-47f4-c2fc-9790897c9cdd","trusted":true},"cell_type":"code","source":"IPDIR_EMBED_DICT","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"'../input/embeddings-glove200/'"},"metadata":{}}]},{"metadata":{"id":"vfblkwZHJRTw","outputId":"cbba178e-2e77-4121-a7fb-0eba5661b7e0","trusted":true},"cell_type":"code","source":"## reload from picked file\n\nif True:\n  with open( IPDIR_EMBED_DICT + 'glove200_embeddings_dict_1.pkl', 'rb') as handle:\n    embeddings_dict_glove200 = pickle.load(handle)\n\nprint(f\"Number of words in this embeddings file = {len(embeddings_dict_glove200)}\")\nprint(f\"Shape of random word = {embeddings_dict_glove200['1917'].shape}\")\nprint(f\"Shape of another random word = {embeddings_dict_glove200['sculpture'].shape}\")\nprint(f\"Size of embeddings_dict_glove200 = {sys.getsizeof(embeddings_dict_glove200)/1000000} MB\")","execution_count":100,"outputs":[{"output_type":"stream","text":"Number of words in this embeddings file = 400000\nShape of random word = (200,)\nShape of another random word = (200,)\nSize of embeddings_dict_glove200 = 20.971624 MB\n","name":"stdout"}]},{"metadata":{"id":"POFL697yPHMV"},"cell_type":"markdown","source":"### Now create the EMBEDDINGS MATRIX using the embeddings dict"},{"metadata":{"id":"Zi6FyTCPjHsd","trusted":true},"cell_type":"code","source":"## Now, for all the unique words the model should possibly output as its vocabulary, we create an embedding matrix","execution_count":101,"outputs":[]},{"metadata":{"id":"VORQIdfbh0G4","outputId":"3a9fc04f-19a9-4b3b-c648-53cea7b636a6","trusted":true},"cell_type":"code","source":"## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\nlen(wordtoix)","execution_count":103,"outputs":[{"output_type":"execute_result","execution_count":103,"data":{"text/plain":"6760"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\nprint( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )","execution_count":104,"outputs":[{"output_type":"stream","text":"1 10 204\n","name":"stdout"}]},{"metadata":{"id":"Q7o8RlR4PFkJ","outputId":"d8b92bb4-cbe9-4955-f684-30356744f8e9","trusted":true},"cell_type":"code","source":"## since using GloVe-200, each word is represented as a 200 dimensional vector\nEMBEDDING_DIMS = 200\n## VOCAB_SIZE = the parameter to be used while defining the decoder model\nVOCAB_SIZE = len(wordtoix) + 1 # number of unique high freq words in the vocabulary + 1 (one index for the 0)\n\nembedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIMS))\nfor word, i in wordtoix.items():\n    #if i < max_words:\n    ## using .get(word) method and not [word] as second method throws key error, while first returns none.\n    ##        E.g.    print(embeddings_dict_glove200.get('sculture')) = None\n    ##            But print(embeddings_dict_glove200['sculture'}) will break with key error\n    embedding_vector = embeddings_dict_glove200.get(word)\n    if embedding_vector is not None:\n        # Words not found in the embedding index will be all zeros\n        embedding_matrix[i] = embedding_vector\n\nprint(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")","execution_count":105,"outputs":[{"output_type":"stream","text":"Shape of embedding_matrix = (6761, 200)\n","name":"stdout"}]},{"metadata":{"id":"1zczwbWePFmn","outputId":"b5c9c135-2849-45e3-d47b-4881d766a1b2","trusted":true},"cell_type":"code","source":"## e.g. of a word PRESENT the GoVe200 data file\nmytestword = 'sculpture'\nprint(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\nprint(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\nprint(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")\n## it should match","execution_count":106,"outputs":[{"output_type":"stream","text":"Index position of word sculpture in the vocabulary = 768\n\nEmbedding vector for word 'sculpture' from GloVe directly = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n -0.1479    -0.62678  ]\n\nembedding_matrix entry for 'sculpture' = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n -0.1479    -0.62678  ]\n","name":"stdout"}]},{"metadata":{"id":"pnQW2VwOPFo_","outputId":"4d9bcb11-17c4-47d9-daf5-e0610ed6d8b0","trusted":true},"cell_type":"code","source":"## e.g. of a word NOT PRESENT the GoVe200 data file\n## answer None in all cases since the word is not in the vocabulary itself the other two will throw key error\nmytestword = 'sculture'\nprint(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\nprint(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\nprint(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")","execution_count":107,"outputs":[{"output_type":"stream","text":"Index position of word sculture in the vocabulary = None\n\nEmbedding vector for word 'sculpture' from GloVe directly = None\n\nembedding_matrix entry for 'sculture' = None\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_dict_glove200","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"EJgRpN5MjJD3"},"cell_type":"markdown","source":"## Data Preparation using Generator Function"},{"metadata":{"id":"7v8nlMCHYock","trusted":true},"cell_type":"code","source":"## Hereafter, I will try to explain the remaining steps by taking a sample example as follows:\n## Consider we have 3 images and their 3 corresponding captions as follows:\n## \n## (Train image 1) Caption -> The black cat sat on grass\n## (Train image 2) Caption -> The white cat is walking on road\n## (Test image) Caption -> The black cat is walking on grass\n## \n## Now, letâ€™s say we use theÂ first two imagesÂ and their captions toÂ trainÂ the model and theÂ third imageÂ toÂ testÂ our model.\n## Now the questions that will be answered are: how do we frame this as a supervised learning problem?, what does the data matrix look like? how many data points do we have?, etc.\n## First we need to convert both the images to their corresponding 2048 length feature vector as discussed above. Let â€œImage_1â€ and â€œImage_2â€ be the feature vectors of the first two images respectively\n## Secondly, letâ€™s build the vocabulary for the first two (train) captions by adding the two tokens â€œstartseqâ€ and â€œendseqâ€ in both of them: (Assume we have already performed the basic cleaning steps)\n## \n## Caption_1 -> â€œstartseq the black cat sat on grass endseqâ€\n## Caption_2 -> â€œstartseq the white cat is walking on road endseqâ€\n## \n## vocab = {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n## \n## Letâ€™s give an index to each word in the vocabulary:\n## black -1, cat -2, endseq -3, grass -4, is -5, on -6, road -7, sat -8, startseq -9, the -10, walking -11, white -12\n## \n## Now letâ€™s try to frame it as aÂ supervised learning problemÂ where we have a set of data points D = {Xi, Yi}, where Xi is the feature vector of data point â€˜iâ€™ and Yi is the corresponding target variable.\n## \n## Letâ€™s take the first image vectorÂ Image_1Â and its corresponding caption â€œstartseq the black cat sat on grass endseqâ€. Recall that, Image vector is the input and the caption is what we need to predict. But the way we predict the caption is as follows:\n## For the first time, we provide the image vector and the first word as input and try to predict the second word, i.e.:\n## Input = Image_1 + â€˜startseqâ€™; Output = â€˜theâ€™\n## Then we provide image vector and the first two words as input and try to predict the third word, i.e.:\n## Input = Image_1 + â€˜startseq theâ€™; Output = â€˜catâ€™\n## And so on . . .\n## \n## Thus, we can summarize the data matrix for one image and its corresponding caption as follows:\n## Step 1 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq\" :: Target Word = \"the\"\n## Step 2 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the\" :: Target Word = \"black\"\n## Step 3 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black\" :: Target Word = \"cat\"\n## Step 4 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat\" :: Target Word = \"sat\"\n## Step 5 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat\" :: Target Word = \"on\"\n## Step 6 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on\" :: Target Word = \"grass\"\n## Step 7 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on grass\" :: Target Word = \"endseq\"\n## \n## It must be noted that, one image+caption isÂ not a single data pointÂ but are multiple data points depending on the length of the caption.\n## All the above 7 data points together constitute the full data for one image and its caption!!!!\n## Similarly for second images, there will be multiple steps together that consitute its full data point.\n## \n## We must now understand that in every data point, itâ€™s not just the image which goes as input to the system, but also, a partial caption which helps toÂ predict the next word in the sequence.\n## Since we are processingÂ sequences, we will employ aÂ Recurrent Neural NetworkÂ to read these partial captions (more on this later).\n## However, we have already discussed that we are not going to pass the actual English text of the caption, rather we are going to pass the sequence of indices where each index represents a unique word.\n## \n## Since we have already created an index for each word, letâ€™s now replace the words with their indices and understand how the data matrix will look like:\n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6 4\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"9 10\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6 3\" :: Target Word = \"3\"\n## \n## Since we would be doingÂ batch processingÂ (explained later), we need to make sure that each sequence is ofÂ equal length. Hence we need toÂ append 0â€™sÂ (zero padding) at the end of each sequence. ButÂ how manyÂ zeros should we append in each sequence?\n## Well, this is the reason we had calculated the maximum length of a caption. So we will append those many number of zeros which will lead to every sequence having a length = maximum length of caption.\n## \n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 0 0 ...]\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 0 0 ...]\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 0 0 ...]\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 0 0 ...]\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 4 0 0 ...]\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 0 0 ...]\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 0 0 ...]\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 0 0 ...]\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 0 0 ...]\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 0 0 ...]\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 3 0 0 ...]\" :: Target Word = \"3\"\n## Appended adequete 0's to each partial caption to make its length = max length\n## \n## \n## \n## Need for a Data Generator:\n## In the above example, I have only considered 2 images and captions which have lead to 15 data points.\n## However, in our actual training dataset we have 6000 images, each having 5 captions. This makes a total ofÂ 30000Â images and captions.\n## Even if we assume that each caption on an average is just 7 words long, it will lead to a total of 30000*7 i.e.Â 210000Â data points.\n## \n## Compute the size of the data matrix:\n## \n## \n##    img_vector = 2048        partial caption (length = max caption length)\n## ---------------------------------------------------------------------------\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## ---------------------------------------------------------------------------\n## Say above matrix has n rows, m columns, so Size of the data matrix = n*m\n## n-> number of data points (assumed as 210000)  (6000 images * 5 captions per image * 7 words average legnth of each caption)\n## m-> length of each data point\n##     = 2048 + length of partial caption (say something)\n##     = 2048 + something\n## \n## Now this \"something\"  NOTE EQUAL to the max length of caption!\n## Every word (or index) will be mapped (embedded) to higher dimensional space through one of the word embedding techniques.\n## As we using GloVe-200, each embedding has 200 floats representing each number.\n## \n## So with each caption sentence consisting of max length caption size word-indexes, each word represented by 200 dimensional value:\n##    Assuming max lenght of caption = 45\n##    means \"something\" = 2048 + ( 45 * 200 ) = 2048 + 9000 = 11048 float values to represent each sentence\n## \n## Therefore, size of data matrix = m*n = 210000 * 11048 = xxx float values!!!\n## Assuming float takes 2 bytes (very conservative), that still means xxx * 2 = xxx GB\n## \n## This is pretty huge requirement and even if we are able to manage to load this much data into the RAM, it will make the system very slow.\n## For this reason we use data generators a lot in Deep Learning. Data Generators are a functionality which is natively implemented in Python. The ImageDataGenerator class provided by the Keras API is nothing but an implementation of generator function in Python.\n## \n## So how does using a generator function solve this problem?\n## If you know the basics of Deep Learning, then you must know that to train a model on a particular dataset, we use some version of Stochastic Gradient Descent (SGD) like Adam, Rmsprop, Adagrad, etc.\n## WithÂ SGD, we do not calculate the loss on the entire data set to update the gradients. Rather in every iteration, we calculate the loss on aÂ batchÂ of data points (typically 64, 128, 256, etc.) to update the gradients.\n## \n## This means that we do not require to store the entire dataset in the memory at once. Even if we have the current batch of points in the memory, it is sufficient for our purpose.\n## A generator function in Python is used exactly for this purpose. Itâ€™s like an iterator which resumes the functionality from the point it left the last time it was called.\n## To understand more about Generators, please readÂ here (https://wiki.python.org/moin/Generators).\n## ","execution_count":109,"outputs":[]},{"metadata":{"id":"dQkTM-yDjHkU","trusted":true},"cell_type":"code","source":"# data generator, use during the call to model.fit_generator() to create batchwise data\ndef data_generator_1(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in _descriptions.items():\n            n+=1\n            # retrieve the encoded features of image\n            img_feat = _imgs_features_arr[ key ] ## keys in the image encodings dict and descriptions dict use image_filename without the .jpg\n            for desc in desc_list:\n                # encode the sequence\n                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n                    # encode output sequence\n                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n                    # store\n                    X1.append(img_feat)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            # yield the batch data\n            if n == _images_per_batch:\n                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n                yield [np.array(X1), np.array(X2)], np.array(y)\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":127,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data generator, use during the call to model.fit_generator() to create batchwise data\ndef data_generator_2(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in _descriptions.items():\n            n+=1\n            # retrieve the encoded features of image\n            img_feat = _imgs_features_arr[ key ]  ## keys in the image encodings dict and descriptions dict use image_filename without the .jpg\n            for desc in desc_list:\n                # encode the sequence\n                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n                    # encode output sequence\n                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n                    # store\n                    X1.append(img_feat)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            # yield the batch data\n            if n == _images_per_batch:\n                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n                yield [np.array(X1), np.array(X2)], np.array(y)\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":128,"outputs":[]},{"metadata":{"id":"DGg7LIE5bHfV","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"MTxczXFBRp85","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CGzQr4tiiWDc"},"cell_type":"markdown","source":"## ALWAYS - Define the RNN Decoder model\n## Use the GloVe embeddings matrix values to update layer weights and freeze ONLY those weights\n\n\n### About Keras Emebedding layer\n### \n#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n######tf.keras.layers.Embedding(\n######    input_dim, output_dim,\n######    embeddings_initializer=\"uniform\",\n######    embeddings_regularizer=None,\n######    activity_regularizer=None,\n######    embeddings_constraint=None,\n######    mask_zero=False,\n######    input_length=None,\n######    **kwargs\n######)"},{"metadata":{"id":"_EVJLMeGRp_Y","trusted":true},"cell_type":"code","source":"## Decoder Model defining\n\n## parameters to define model - all these three populated earlier\n## EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n\ninputs1 = keras.Input(shape=(2048,))\nfe1 = keras.layers.Dropout(0.5)(inputs1)\nfe2 = keras.layers.Dense(256, activation='relu')(fe1)\n\n# partial caption sequence model\ninputs2 = keras.Input(shape=(MAX_LENGTH_CAPTION,))\nse1 = keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIMS, mask_zero=True)(inputs2)\nse2 = keras.layers.Dropout(0.5)(se1)\nse3 = keras.layers.LSTM(256)(se2)\n\n# decoder (feed forward) model\ndecoder1 = keras.layers.add([fe2, se3])\ndecoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\noutputs = keras.layers.Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n\n# merge the two input models\nmodel_RNN_decoder = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)","execution_count":112,"outputs":[]},{"metadata":{"id":"OHWDqfjiRqB4","outputId":"e55a7148-8854-431d-c667-785f81063188","trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() BEFORE FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params\nmodel_RNN_decoder.summary()","execution_count":113,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 49)]         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 49, 200)      1352200     input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 6761)         1737577     dense_1[0][0]                    \n==================================================================================================\nTotal params: 4,148,081\nTrainable params: 4,148,081\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() BEFORE FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params\n\n#Model: \"functional_1\"\n#__________________________________________________________________________________________________\n#Layer (type)                    Output Shape         Param #     Connected to                     \n#==================================================================================================\n#input_2 (InputLayer)            [(None, 49)]         0                                            \n#__________________________________________________________________________________________________\n#input_1 (InputLayer)            [(None, 2048)]       0                                            \n#__________________________________________________________________________________________________\n#embedding (Embedding)           (None, 49, 200)      1352200     input_2[0][0]                    \n#__________________________________________________________________________________________________\n#dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n#__________________________________________________________________________________________________\n#dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n#__________________________________________________________________________________________________\n#lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n#__________________________________________________________________________________________________\n#add (Add)                       (None, 256)          0           dense[0][0]                      \n#                                                                 lstm[0][0]                       \n#__________________________________________________________________________________________________\n#dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n#__________________________________________________________________________________________________\n#dense_2 (Dense)                 (None, 6761)         1737577     dense_1[0][0]                    \n#==================================================================================================\n#Total params: 4,148,081\n#Trainable params: 4,148,081\n#Non-trainable params: 0\n#__________________________________________________________________________________________________","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPDIR","execution_count":116,"outputs":[{"output_type":"execute_result","execution_count":116,"data":{"text/plain":"'../working/'"},"metadata":{}}]},{"metadata":{"id":"1rFiBoJyRqHg","outputId":"cf620edb-6810-45b9-fec1-5d9d637c9442","trusted":true},"cell_type":"code","source":"#RNN_decoder_model_plot_filename = r'RNN_decoder_model_plot_97kTrain_1.jpg'\n#tf.keras.utils.plot_model(model_RNN_decoder, to_file= OPDIR + RNN_decoder_model_plot_filename, show_shapes=True, show_layer_names=True)","execution_count":117,"outputs":[{"output_type":"stream","text":"\"dot\" with args ['-Tjpg', '/tmp/tmp7qbfu59h'] returned code: 1\n\nstdout, stderr:\n b''\nb'Format: \"jpg\" not recognized. Use one of: canon cmap cmapx cmapx_np dot dot_json eps fig gv imap imap_np ismap json json0 mp pdf pic plain plain-ext png pov ps ps2 svg svgz tk vdx vml vmlz xdot xdot1.2 xdot1.4 xdot_json\\n'\n\n","name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-4e2d404ee570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mRNN_decoder_model_plot_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'RNN_decoder_model_plot_97kTrain_1.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_RNN_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mOPDIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mRNN_decoder_model_plot_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0;31m# Save image to disk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m   \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m   \u001b[0;31m# Return the image as a Jupyter Image object, to be displayed in-line.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m   \u001b[0;31m# Note that we cannot easily detect whether the code is running in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, path, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1815\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1943\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstdout_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: 1"]}]},{"metadata":{"id":"ETH9PxyGxpt-","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CMKYtXCQxexx"},"cell_type":"markdown","source":"### ALWAYS - set weights using the GloVe embedding matrix and freeze it"},{"metadata":{"id":"6c0oR60fRqJZ","outputId":"9dbf8c22-edfb-428b-e172-7c7b03bc60c5","trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"(6761, 200)"},"metadata":{}}]},{"metadata":{"id":"naYfB0FjwocE","trusted":true},"cell_type":"code","source":"model_RNN_decoder.layers[2].set_weights([embedding_matrix])\nmodel_RNN_decoder.layers[2].trainable = False","execution_count":119,"outputs":[]},{"metadata":{"id":"G4HreAB0woha","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CMFFw_g5xy7r"},"cell_type":"markdown","source":"### DO ONCE - RNN Decoder summary and plot\n#### AFTER loading GloVe embeddings and Freezing"},{"metadata":{"id":"K_mLYJjsRqMO","outputId":"809ebcdd-f972-4f47-f60f-a207404054a5","trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params has REDUCED\nmodel_RNN_decoder.summary()","execution_count":121,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 49)]         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 49, 200)      1352200     input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 6761)         1737577     dense_1[0][0]                    \n==================================================================================================\nTotal params: 4,148,081\nTrainable params: 2,795,881\nNon-trainable params: 1,352,200\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params has REDUCED\n\n#Model: \"functional_1\"\n#__________________________________________________________________________________________________\n#Layer (type)                    Output Shape         Param #     Connected to                     \n#==================================================================================================\n#input_2 (InputLayer)            [(None, 49)]         0                                            \n#__________________________________________________________________________________________________\n#input_1 (InputLayer)            [(None, 2048)]       0                                            \n#__________________________________________________________________________________________________\n#embedding (Embedding)           (None, 49, 200)      1352200     input_2[0][0]                    \n#__________________________________________________________________________________________________\n#dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n#__________________________________________________________________________________________________\n#dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n#__________________________________________________________________________________________________\n#lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n#__________________________________________________________________________________________________\n#add (Add)                       (None, 256)          0           dense[0][0]                      \n#                                                                 lstm[0][0]                       \n#__________________________________________________________________________________________________\n#dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n#__________________________________________________________________________________________________\n#dense_2 (Dense)                 (None, 6761)         1737577     dense_1[0][0]                    \n#==================================================================================================\n#Total params: 4,148,081\n#Trainable params: 2,795,881\n#Non-trainable params: 1,352,200\n#__________________________________________________________________________________________________","execution_count":122,"outputs":[]},{"metadata":{"id":"8lX2xaz1xxrF","outputId":"12854ec9-d37b-4a85-b364-ed191b6613ee","trusted":false},"cell_type":"code","source":"#tf.keras.utils.plot_model(model_RNN_decoder, to_file=OPDIR+'RNN_decoder_model_plot_AFTER_GLOVE_1.jpg', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"vxU_TgOGxxt6","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"iNo79p0Az37z","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"XB81Ohfoz4lL"},"cell_type":"markdown","source":"### ALWAYS - Compile RNN model"},{"metadata":{"id":"CF5Dw9brxxwh","trusted":true},"cell_type":"code","source":"model_RNN_decoder.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":123,"outputs":[]},{"metadata":{"id":"c6aKMocpRqXH","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hIqp5fxB0ldP","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xdJW1J2YmrxO"},"cell_type":"markdown","source":"## ALWAYS - Train the RNN Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"OPDIR_WEIGHTS","execution_count":126,"outputs":[{"output_type":"execute_result","execution_count":126,"data":{"text/plain":"'../working/weights_out/'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir( OPDIR_WEIGHTS )","execution_count":124,"outputs":[{"output_type":"execute_result","execution_count":124,"data":{"text/plain":"[]"},"metadata":{}}]},{"metadata":{"id":"EXJ2Yjuz05wG","outputId":"c0ab13d8-b8d2-4450-ef87-589ebd49b46c","trusted":true},"cell_type":"code","source":"## values setup earlier - for reference\n\nprint(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\tlen(img_encodings_val) = {len(img_encodings_val)}\\tlen(img_encodings_test) = {len(img_encodings_test)}\")\nprint(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\tlen(descriptions_val) = {len(descriptions_val)}\\tlen(descriptions_test) = {len(descriptions_test)}\")\n\nprint(f\"MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nprint(f\"VOCAB_SIZE = {VOCAB_SIZE}\")","execution_count":125,"outputs":[{"output_type":"stream","text":"Encodings data:\nlen(img_encodings_train) = 97000\tlen(img_encodings_val) = 3000\tlen(img_encodings_test) = 5000\nDescriptions data:\nlen(descriptions_train) = 97000\tlen(descriptions_val) = 3000\tlen(descriptions_test) = 5000\nMAX_LENGTH_CAPTION = 49\nVOCAB_SIZE = 6761\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_RUN_NUMBER = 2 ## large training data","execution_count":null,"outputs":[]},{"metadata":{"id":"J5Dx5QnMmrJ1","outputId":"ec46884f-bb34-4244-8b6f-ff525a537581","trusted":true},"cell_type":"code","source":"## For reference: def data_generator(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch):\n## Notes:\n##      1) The data generator function needs exactly the CORRECT descriptions for the training (or validation data). It takes the keys from the descriptions and then\n##         searches the image encodings array. So it is fine even if we use an image encodings with more images.\n##         But we already made the dedicated arrays for Train and Validation datasets, so use them explicity!!!\n\n########### Phase 1 #########\n\nprint(f\"\\n\\nTraining Phase 1 started at :: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\nLR_1 = 0.0001\nBATCH_SIZE_1 = 32   ## how many images per batch\nN_EPOCHS_1 = 5\nSTEPS_PER_EPOCH_1 = len(descriptions_train) // BATCH_SIZE_1\nprint(f\"\\nPhase 1 parameters:\")\nprint(f\"STEPS_PER_EPOCH_1 = {STEPS_PER_EPOCH_1}\")\nprint(f\"BATCH_SIZE_1 = {BATCH_SIZE_1}\")\nprint(f\"N_EPOCHS_1 = {N_EPOCHS_1}\")\n\nmodel_RNN_decoder.optimizer.lr = LR_1\nfor i in range(N_EPOCHS_1):\n    print(f\"Epoch {i+1} started at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    generator_1 = data_generator_1(descriptions_train, img_encodings_train, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_1, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_1, verbose=1)\n    model_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'Decoder_Wts_ep_' + str(i+1) + '.h5' )\n    #if (i+1) >= 5 or i//2 == 1:\n    #    model_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'RNNDecoder_Weights_Ph1_ep' + str(i) + '.h5' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Phase 2 #########\npurposely let it fail not now\nprint(f\"\\n\\nTraining Phase 2 started at :: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\nLR_2 = 0.0001\nBATCH_SIZE_2 = 16   ## how many images per batch\nN_EPOCHS_2 = 15\nSTEPS_PER_EPOCH_2 = len(descriptions_train_start_end_seq) // BATCH_SIZE_2\nprint(f\"\\nPhase 2 parameters:\")\nprint(f\"STEPS_PER_EPOCH_2 = {STEPS_PER_EPOCH_2}\")\nprint(f\"BATCH_SIZE_2 = {BATCH_SIZE_2}\")\nprint(f\"N_EPOCHS_2 = {N_EPOCHS_2}\")\nprint(f\"LR_2 = {LR_2}\")\n\nmodel_RNN_decoder.optimizer.lr = LR_2\nfor i in range(N_EPOCHS_2):\n    generator_2 = data_generator_2(descriptions_train_start_end_seq, img_encodings, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_2, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_2, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_2, verbose=1)\n    if True:# (i+1) >= 5 or i//2 == 1:\n        model_RNN_decoder.save_weights( OPDIR + r'weights/' + 'RNNDecoder_Weights_Ph2_ep' + str(i) + '.h5' )","execution_count":null,"outputs":[]},{"metadata":{"id":"4Xc08wQoRqZx","trusted":false},"cell_type":"code","source":"#model_reset_states\n#lr optimizer\n#tuner","execution_count":null,"outputs":[]},{"metadata":{"id":"WOxOw-XI31j9"},"cell_type":"markdown","source":"### Save the Model Weights after training!"},{"metadata":{"id":"UncVLQn738_7","outputId":"dbec1f1e-f668-49a0-d518-38801fdb05ac","trusted":true},"cell_type":"code","source":"OPDIR","execution_count":null,"outputs":[]},{"metadata":{"id":"Iob2tOZWYvxu","outputId":"39bd7967-bac5-4414-a2a9-f260f57b9c8c","trusted":false},"cell_type":"code","source":"#OPDIR + r'Weights/' + 'rbewoorpil_RNNDecoder_Weights_ep10.h5'","execution_count":null,"outputs":[]},{"metadata":{"id":"7BdC8m2VztDp","outputId":"3da9bde0-5c58-4aac-9fc6-e43fbf3f2616","trusted":false},"cell_type":"code","source":"SAVE_TO = OPDIR + r'weights/' + 'kagg_RNNDecoder_Weights_final.h5'\nmodel_RNN_decoder.save_weights( SAVE_TO )\nprint(f\"At {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} :: saved weights to file = { SAVE_TO }\")","execution_count":null,"outputs":[]},{"metadata":{"id":"_XHGDOZaztGQ","trusted":false},"cell_type":"code","source":"## /content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/Weights/rbewoorpil_Encoder_Weights_ep10.h5\n## 10 epochs, all with LR = 0.0001, batch size = 4, MAX_LENGTH_CAPTION = 45, VOCAB_SIZE = 1523, loss='categorical_crossentropy', optimizer='adam', train=4800 images","execution_count":null,"outputs":[]},{"metadata":{"id":"a9kP4Sj01FJM","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RELOAD - MODEL 3\n### Weights File: /content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/Weights/rbewoor_Encoder_Weights_Ph2_ep4.h5\n\n### Using the last epochs weight file\n\n### Training Specs:\n### Total 15 epochs, all LR=0.005, MAX_LENGTH_CAPTION = 43, VOCAB_SIZE = 3208, loss='categorical_crossentropy', optimizer='adam', train=19800 images\n\n\n### The weights file was exported earlier using: model_RNN_decoder.save_weights(SAVE_TO_PATH)\n\n\n## For reference: def data_generator(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch):\n## Notes:\n##      1) Ok to use the img_encodings array of Train and Test images. COZ function starts with the image name as key from the descriptions.\n\n########### Phase 1 #########\n\nprint(f\"\\n\\nTraining Phase 1 started at :: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\nLR_1 = 0.0005\nBATCH_SIZE_1 = 8   ## how many images per batch\nN_EPOCHS_1 = 15\nSTEPS_PER_EPOCH_1 = len(descriptions_train_start_end_seq) // BATCH_SIZE_1\nprint(f\"\\nPhase 1 parameters:\")\nprint(f\"STEPS_PER_EPOCH_1 = {STEPS_PER_EPOCH_1}\")\nprint(f\"BATCH_SIZE_1 = {BATCH_SIZE_1}\")\nprint(f\"N_EPOCHS_1 = {N_EPOCHS_1}\")\n\nmodel_RNN_decoder.optimizer.lr = LR_1\nfor i in range(N_EPOCHS_1):\n    generator_1 = data_generator_1(descriptions_train_start_end_seq, img_encodings, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_1, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_1, verbose=1)\n    if (i+1) >= 5 or i//2 == 1:\n        model_RNN_decoder.save_weights( OPDIR + r'weights/' + 'RNNDecoder_Weights_Ph1_ep' + str(i) + '.h5' )"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reload_rnn_encoder_saved_weights(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n  if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n    ## Decoder Model defining\n\n    ## parameters to define model\n    #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n    #VOCAB_SIZE is initialised earlier\n    #MAX_LENGTH_CAPTION is initialised earlier\n\n    inputs1 = keras.Input(shape=(2048,))\n    fe1 = keras.layers.Dropout(0.5)(inputs1)\n    fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n\n    # partial caption sequence model\n    inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n    se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n    se2 = keras.layers.Dropout(0.5)(se1)\n    se3 = keras.layers.LSTM(256)(se2)\n\n    # decoder (feed forward) model\n    decoder1 = keras.layers.add([fe2, se3])\n    decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n    outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n\n    # merge the two input models\n    reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n\n    print(f\"\\nRNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\\nAttempting to load weights...\")\n    \n    ## load the weights\n    reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n    print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n    return reloaded_rnn_decoder_model\n  else:\n    print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n    return None\n\ndef greedySearch(_decoder_model, _img_encoding, _max_length):\n  in_text = 'startseq'\n  for i in range(_max_length):\n    sequence = [ wordtoix[w] for w in in_text.split() if w in wordtoix ]\n    sequence = keras.preprocessing.sequence.pad_sequences([sequence], maxlen=_max_length)\n    yhat = _decoder_model.predict([_img_encoding,sequence], verbose=0)\n    yhat = np.argmax(yhat)\n    word = ixtoword[yhat]\n    in_text += ' ' + word\n    if word == 'endseq':\n      break\n  caption_out = in_text.split()\n  #caption_out = caption_out[1:-1]  ## drop the startseq and endseq words at either end\n  caption_out = ' '.join(caption_out)\n  return caption_out\n\ndef do_inference_one_image(_infer_idx_pos, _imgs_arr, _IPDIRIMGS, _MAX_LENGTH_CAPTION, _model_RNN_decoder, _descriptions):\n  infer_image_path = _IPDIRIMGS + _imgs_arr[_infer_idx_pos] + '.jpg'\n\n  ## show the original image\n  image = img_encodings[ _imgs_arr[_infer_idx_pos] ].reshape((1,2048))\n  x = plt.imread(infer_image_path)\n  plt.imshow(x)\n  plt.show()\n\n  ## get the prediction caption using greedy search\n  predicted_caption = greedySearch(_model_RNN_decoder, image, _MAX_LENGTH_CAPTION)\n  print(f\"\\nFor image :: {infer_image_path}\\n\\nInference caption output:\\n{ predicted_caption }\")\n  print(\"\")\n  ## show the original captions\n  for idx, orig_cap in enumerate(_descriptions.get(_imgs_arr[_infer_idx_pos])):\n    print(f\"Original caption {idx+1}  :::  {orig_cap}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/modelweights/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SAVED_WEIGHTS_PATH = OPDIR + r'Weights/' + 'rbewoor_Encoder_Weights_Ph2_ep4.h5'\nSAVED_WEIGHTS_PATH = r'../input/modelweights/RNNDecoder_Weights_Ph1_ep14.h5'\nSAVED_WEIGHTS_PATH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}, VOCAB_SIZE = {VOCAB_SIZE}, MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reloaded_RNN_decoder = reload_rnn_encoder_saved_weights(SAVED_WEIGHTS_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\ntype(reloaded_RNN_decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for infer_idx_pos in range(50):\n  print(f\"--------  Inference for position {infer_idx_pos}  --------\")\n  do_inference_one_image(infer_idx_pos, val_imgs_arr, IPDIR_IMGS, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, descriptions)\n  print(f\"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"htXEBYJQBocZ"},"cell_type":"markdown","source":"## RELOAD - MODEL 2\n### Weights File: /content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/Weights/rbewoor_Encoder_Weights_Ph2_ep4.h5\n\n### Using the last epochs weight file\n\n### Training Specs:\n### Total 10 epochs, first 5 epochs with smaller BS and larger LR, last 5 epochs with larger BS but smaller LR, MAX_LENGTH_CAPTION = 45, VOCAB_SIZE = 1523, loss='categorical_crossentropy', optimizer='adam', train=4800 images\n\n\n### The weights file was exported earlier using: model_RNN_decoder.save_weights(SAVE_TO_PATH)"},{"metadata":{"id":"bWfws4rTBlxb","trusted":false},"cell_type":"code","source":"Training Phase 1 started at :: 2020-09-18 22:10:51\n\nPhase 1 parameters:\nSTEPS_PER_EPOCH_1 = 1200\nBATCH_SIZE_1 = 4\nN_EPOCHS_1 = 5\nWARNING:tensorflow:From <ipython-input-47-83b52223b490>:21: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use Model.fit, which supports generators.\n1200/1200 [==============================] - 585s 488ms/step - loss: 4.7801\n1200/1200 [==============================] - 599s 499ms/step - loss: 3.8539\n1200/1200 [==============================] - 584s 487ms/step - loss: 3.5118\n1200/1200 [==============================] - 585s 487ms/step - loss: 3.3086\n1200/1200 [==============================] - 579s 483ms/step - loss: 3.1668\n\n\nTraining Phase 2 started at :: 2020-09-18 22:59:49\n\nPhase 2 parameters:\nSTEPS_PER_EPOCH_2 = 600\nBATCH_SIZE_2 = 8\nN_EPOCHS_2 = 5\n600/600 [==============================] - 536s 894ms/step - loss: 3.0436\n600/600 [==============================] - 543s 904ms/step - loss: 2.9905\n600/600 [==============================] - 550s 916ms/step - loss: 2.9629\n600/600 [==============================] - 553s 922ms/step - loss: 2.9448\n600/600 [==============================] - 566s 943ms/step - loss: 2.9222","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reload_rnn_encoder_saved_weights(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n  if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n    ## Decoder Model defining\n\n    ## parameters to define model\n    #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n    #VOCAB_SIZE is initialised earlier\n    #MAX_LENGTH_CAPTION is initialised earlier\n\n    inputs1 = keras.Input(shape=(2048,))\n    fe1 = keras.layers.Dropout(0.5)(inputs1)\n    fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n\n    # partial caption sequence model\n    inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n    se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n    se2 = keras.layers.Dropout(0.5)(se1)\n    se3 = keras.layers.LSTM(256)(se2)\n\n    # decoder (feed forward) model\n    decoder1 = keras.layers.add([fe2, se3])\n    decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n    outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n\n    # merge the two input models\n    reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n\n    print(f\"\\nRNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\\nAttempting to load weights...\")\n    \n    ## load the weights\n    reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n    print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n    return reloaded_rnn_decoder_model\n  else:\n    print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n    return None\n\ndef greedySearch(_decoder_model, _img_encoding, _max_length):\n  in_text = 'startseq'\n  for i in range(_max_length):\n    sequence = [ wordtoix[w] for w in in_text.split() if w in wordtoix ]\n    sequence = keras.preprocessing.sequence.pad_sequences([sequence], maxlen=_max_length)\n    yhat = _decoder_model.predict([_img_encoding,sequence], verbose=0)\n    yhat = np.argmax(yhat)\n    word = ixtoword[yhat]\n    in_text += ' ' + word\n    if word == 'endseq':\n      break\n  caption_out = in_text.split()\n  #caption_out = caption_out[1:-1]  ## drop the startseq and endseq words at either end\n  caption_out = ' '.join(caption_out)\n  return caption_out\n\ndef do_inference_one_image(_infer_idx_pos, _imgs_arr, _IPDIRIMGS, _MAX_LENGTH_CAPTION, _model_RNN_decoder, _descriptions):\n  infer_image_path = _IPDIRIMGS + _imgs_arr[_infer_idx_pos] + '.jpg'\n\n  ## show the original image\n  image = img_encodings[ _imgs_arr[_infer_idx_pos] ].reshape((1,2048))\n  x = plt.imread(infer_image_path)\n  plt.imshow(x)\n  plt.show()\n\n  ## get the prediction caption using greedy search\n  predicted_caption = greedySearch(_model_RNN_decoder, image, _MAX_LENGTH_CAPTION)\n  print(f\"\\nFor image :: {infer_image_path}\\n\\nInference caption output:\\n{ predicted_caption }\")\n  print(\"\")\n  ## show the original captions\n  for idx, orig_cap in enumerate(_descriptions.get(_imgs_arr[_infer_idx_pos])):\n    print(f\"Original caption {idx+1}  :::  {orig_cap}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"HtWOanhYBl0u","outputId":"fcfe63fe-2528-4db7-e690-5686a88bd111","trusted":false},"cell_type":"code","source":"SAVED_WEIGHTS_PATH = OPDIR + r'Weights/' + 'rbewoor_Encoder_Weights_Ph2_ep4.h5'\nSAVED_WEIGHTS_PATH","execution_count":null,"outputs":[]},{"metadata":{"id":"dqVH4VSXDFpv","outputId":"c6b17049-17d8-4c81-b865-cfe1daa783b5","trusted":false},"cell_type":"code","source":"print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}, VOCAB_SIZE = {VOCAB_SIZE}, MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nreloaded_RNN_decoder = reload_rnn_encoder_saved_weights(SAVED_WEIGHTS_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\ntype(reloaded_RNN_decoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"qiPebXBDDFso","outputId":"f67e52c3-4a5f-4026-a74e-c7f29728e5f0","trusted":false},"cell_type":"code","source":"for infer_idx_pos in range(10):\n  print(f\"--------  Inference for position {infer_idx_pos}  --------\")\n  do_inference_one_image(infer_idx_pos, val_imgs_arr, IPDIRIMGS, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, descriptions)\n  print(f\"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"vvjcJEUnDFvO","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"WU29bCwH0qrS","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"BtW2CPgu5q1f","outputId":"499df196-334a-4553-e15a-89bed35b1031","trusted":false},"cell_type":"code","source":"print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}, VOCAB_SIZE = {VOCAB_SIZE}, MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nreloaded_RNN_decoder = reload_rnn_encoder_saved_weights(SAVED_WEIGHTS_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\ntype(reloaded_RNN_decoder)","execution_count":null,"outputs":[]},{"metadata":{"id":"QEHRYSDQ0qvE","outputId":"f581572a-a5c8-4893-afa2-4def47dca1eb","trusted":false},"cell_type":"code","source":"for infer_idx_pos in range(10):\n  print(f\"--------  Inference for position {infer_idx_pos}  --------\")\n  do_inference_one_image(infer_idx_pos, val_imgs_arr, IPDIRIMGS, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, descriptions)\n  print(f\"\\n\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"id":"C1SIDyCr0qxi","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"P8ZbxSYJ5YWi","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"vQ7lNrqA5YZb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"4gVwIe1_IvDd"},"cell_type":"markdown","source":"## NEXT SECTION - PENDING"},{"metadata":{"id":"V2-Eq09sIqPH","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"hWFW9ZU2IqRx","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"0D3b7izSIqUb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}