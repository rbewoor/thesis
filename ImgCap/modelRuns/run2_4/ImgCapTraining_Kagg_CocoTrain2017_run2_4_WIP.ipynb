{"cells":[{"metadata":{"id":"23dZtOGmFhQB","trusted":true},"cell_type":"code","source":"!ls '../'","execution_count":1,"outputs":[{"output_type":"stream","text":"input  lib  working\r\n","name":"stdout"}]},{"metadata":{"id":"RuFZIchrFhSp","trusted":true},"cell_type":"code","source":"!ls '../input/'","execution_count":2,"outputs":[{"output_type":"stream","text":"coco-2017-dataset\t     thesis-imgcap-run2-deterministic-info\r\nimgcap-kagg-run2-weights-in\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/thesis-imgcap-run2-deterministic-info/'","execution_count":3,"outputs":[{"output_type":"stream","text":"descriptions_test.pkl\t\t       img_encodings_train_97000.pkl\r\ndescriptions_train_97000.pkl\t       img_encodings_val_3000.pkl\r\ndescriptions_train_97000_startend.pkl  ixtoword_train_97000.pkl\r\ndescriptions_train_and_val.pkl\t       train_imgs_keys_97000_randomSplit444.pkl\r\ndescriptions_val_3000.pkl\t       val_imgs_keys_3000_randomSplit444.pkl\r\nembedding_matrix_6758_200.pkl\t       wordtoix_train_97000.pkl\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../input/imgcap-kagg-run2-weights-in/'","execution_count":4,"outputs":[{"output_type":"stream","text":"Decoder_Run_2_Wt_ep_1.h5  Decoder_Run_2_Wt_ep_4.h5  Decoder_Run_2_Wt_ep_7.h5\r\nDecoder_Run_2_Wt_ep_2.h5  Decoder_Run_2_Wt_ep_5.h5\r\nDecoder_Run_2_Wt_ep_3.h5  Decoder_Run_2_Wt_ep_6.h5\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/'","execution_count":5,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir '../working/weights_out/'","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/'","execution_count":7,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb  weights_out\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '../working/weights_out/'","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!rm -r '../working/weights_out/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"G-rBS0Z-E4p8"},"cell_type":"markdown","source":"# Training model on 97k of some 100k images of Coco_Train2017 dataset. Using the balance 3k as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n\n## First phase of training already done and pickled various files to resume training with the data being split in a deterministic manner.\n\n## Will not do all the steps again and simply load all the info from the pickled files:\n## Descriptions, image_encodings, embedding_matrix, etc.\n\n### The encoder is pre-trained Google Inception-v3 trained on Imagenet.\n\n## AVAILABLE data :\n### Coco_Train2017     = has 118287 images\n### Coco_Val2017       = has 5000   images\n### Combined total     = 123287     images\n\n## USED data :\n### Coco_Train2017     = 100000  images\n### Coco_Val2017       = 5000    images\n### Combined total     = 105000  images\n\n### Using only the first 100k images of Train2017 + all 5k images of Val2017\n### Thus total data available for training = 100k + 5k = 105k\n### Details of split of data:\n### Training   data = 97000 images from Coco_Train2017\n### Validation data = 3000  images from Coco_Train2017\n### Test       data = 5000  images from Coco_Val2017\n\n### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n\n## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n## Image captioning with Keras"},{"metadata":{"id":"buTVBtveEpqr","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport json\nimport time\nimport datetime\nimport string\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport re\nimport pickle\n#import itertools\nimport PIL\nimport PIL.Image\n\n## import numpy as np\n## from numpy import array\n## import pandas as pd\n## import matplotlib.pyplot as plt\n## %matplotlib inline\n## import string\n## import os\n## from PIL import Image\n## import glob\n## from pickle import dump, load\n## from time import time\n## from keras.preprocessing import sequence\n## from keras.models import Sequential\n## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n## from keras.optimizers import Adam, RMSprop\n## from keras.layers.wrappers import Bidirectional\n## from keras.layers.merge import add\n## from keras.applications.inception_v3 import InceptionV3\n## from keras.preprocessing import image\n## from keras.models import Model\n## from keras import Input, layers\n## from keras import optimizers\n## from keras.applications.inception_v3 import preprocess_input\n## from keras.preprocessing.text import Tokenizer\n## from keras.preprocessing.sequence import pad_sequences\n## from keras.utils import to_categorical","execution_count":9,"outputs":[]},{"metadata":{"id":"ebw9HVoxEptM","outputId":"84656bc7-27ad-46cd-e2fe-bab33c5c0032","trusted":false},"cell_type":"code","source":"#from google.colab import drive\n#drive.flush_and_unmount()\n#drive.mount('/content/gdrive')","execution_count":null,"outputs":[]},{"metadata":{"id":"0b0aXNyCEd3p","trusted":true},"cell_type":"code","source":"## Kaggle versions\n\n## Weights from training till now\nOPDIR = r'../working/'\n\n## New weights to save here\nOPDIR_WEIGHTS = r'../working/weights_out/'\n\n## Images locations\nIPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\nIPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n\n## to make deterministic - saved all the variables based on the data used to train during phase 1\n## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\nIPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\n\n## Weights from previous stage of training\nIPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run2-weights-in/'\n\n\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n\n## Google drive versions\n#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'","execution_count":10,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"EJgRpN5MjJD3"},"cell_type":"markdown","source":"## Data Preparation using Generator Function"},{"metadata":{"id":"7v8nlMCHYock","trusted":true},"cell_type":"code","source":"## Hereafter, I will try to explain the remaining steps by taking a sample example as follows:\n## Consider we have 3 images and their 3 corresponding captions as follows:\n## \n## (Train image 1) Caption -> The black cat sat on grass\n## (Train image 2) Caption -> The white cat is walking on road\n## (Test image) Caption -> The black cat is walking on grass\n## \n## Now, let’s say we use the first two images and their captions to train the model and the third image to test our model.\n## Now the questions that will be answered are: how do we frame this as a supervised learning problem?, what does the data matrix look like? how many data points do we have?, etc.\n## First we need to convert both the images to their corresponding 2048 length feature vector as discussed above. Let “Image_1” and “Image_2” be the feature vectors of the first two images respectively\n## Secondly, let’s build the vocabulary for the first two (train) captions by adding the two tokens “startseq” and “endseq” in both of them: (Assume we have already performed the basic cleaning steps)\n## \n## Caption_1 -> “startseq the black cat sat on grass endseq”\n## Caption_2 -> “startseq the white cat is walking on road endseq”\n## \n## vocab = {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n## \n## Let’s give an index to each word in the vocabulary:\n## black -1, cat -2, endseq -3, grass -4, is -5, on -6, road -7, sat -8, startseq -9, the -10, walking -11, white -12\n## \n## Now let’s try to frame it as a supervised learning problem where we have a set of data points D = {Xi, Yi}, where Xi is the feature vector of data point ‘i’ and Yi is the corresponding target variable.\n## \n## Let’s take the first image vector Image_1 and its corresponding caption “startseq the black cat sat on grass endseq”. Recall that, Image vector is the input and the caption is what we need to predict. But the way we predict the caption is as follows:\n## For the first time, we provide the image vector and the first word as input and try to predict the second word, i.e.:\n## Input = Image_1 + ‘startseq’; Output = ‘the’\n## Then we provide image vector and the first two words as input and try to predict the third word, i.e.:\n## Input = Image_1 + ‘startseq the’; Output = ‘cat’\n## And so on . . .\n## \n## Thus, we can summarize the data matrix for one image and its corresponding caption as follows:\n## Step 1 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq\" :: Target Word = \"the\"\n## Step 2 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the\" :: Target Word = \"black\"\n## Step 3 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black\" :: Target Word = \"cat\"\n## Step 4 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat\" :: Target Word = \"sat\"\n## Step 5 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat\" :: Target Word = \"on\"\n## Step 6 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on\" :: Target Word = \"grass\"\n## Step 7 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on grass\" :: Target Word = \"endseq\"\n## \n## It must be noted that, one image+caption is not a single data point but are multiple data points depending on the length of the caption.\n## All the above 7 data points together constitute the full data for one image and its caption!!!!\n## Similarly for second images, there will be multiple steps together that consitute its full data point.\n## \n## We must now understand that in every data point, it’s not just the image which goes as input to the system, but also, a partial caption which helps to predict the next word in the sequence.\n## Since we are processing sequences, we will employ a Recurrent Neural Network to read these partial captions (more on this later).\n## However, we have already discussed that we are not going to pass the actual English text of the caption, rather we are going to pass the sequence of indices where each index represents a unique word.\n## \n## Since we have already created an index for each word, let’s now replace the words with their indices and understand how the data matrix will look like:\n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6 4\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"9\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"9 10\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6 3\" :: Target Word = \"3\"\n## \n## Since we would be doing batch processing (explained later), we need to make sure that each sequence is of equal length. Hence we need to append 0’s (zero padding) at the end of each sequence. But how many zeros should we append in each sequence?\n## Well, this is the reason we had calculated the maximum length of a caption. So we will append those many number of zeros which will lead to every sequence having a length = maximum length of caption.\n## \n## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"1\"\n## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 0 0 ...]\" :: Target Word = \"2\"\n## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 0 0 ...]\" :: Target Word = \"8\"\n## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 0 0 ...]\" :: Target Word = \"6\"\n## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 0 0 ...]\" :: Target Word = \"4\"\n## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 4 0 0 ...]\" :: Target Word = \"3\"\n## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"12\"\n## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 0 0 ...]\" :: Target Word = \"2\"\n## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 0 0 ...]\" :: Target Word = \"5\"\n## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 0 0 ...]\" :: Target Word = \"11\"\n## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 0 0 ...]\" :: Target Word = \"6\"\n## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 0 0 ...]\" :: Target Word = \"7\"\n## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 3 0 0 ...]\" :: Target Word = \"3\"\n## Appended adequete 0's to each partial caption to make its length = max length\n## \n## \n## \n## Need for a Data Generator:\n## In the above example, I have only considered 2 images and captions which have lead to 15 data points.\n## However, in our actual training dataset we have 6000 images, each having 5 captions. This makes a total of 30000 images and captions.\n## Even if we assume that each caption on an average is just 7 words long, it will lead to a total of 30000*7 i.e. 210000 data points.\n## \n## Compute the size of the data matrix:\n## \n## \n##    img_vector = 2048        partial caption (length = max caption length)\n## ---------------------------------------------------------------------------\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## -                       -                                                 -\n## ---------------------------------------------------------------------------\n## Say above matrix has n rows, m columns, so Size of the data matrix = n*m\n## n-> number of data points (assumed as 210000)  (6000 images * 5 captions per image * 7 words average legnth of each caption)\n## m-> length of each data point\n##     = 2048 + length of partial caption (say something)\n##     = 2048 + something\n## \n## Now this \"something\"  NOTE EQUAL to the max length of caption!\n## Every word (or index) will be mapped (embedded) to higher dimensional space through one of the word embedding techniques.\n## As we using GloVe-200, each embedding has 200 floats representing each number.\n## \n## So with each caption sentence consisting of max length caption size word-indexes, each word represented by 200 dimensional value:\n##    Assuming max lenght of caption = 45\n##    means \"something\" = 2048 + ( 45 * 200 ) = 2048 + 9000 = 11048 float values to represent each sentence\n## \n## Therefore, size of data matrix = m*n = 210000 * 11048 = xxx float values!!!\n## Assuming float takes 2 bytes (very conservative), that still means xxx * 2 = xxx GB\n## \n## This is pretty huge requirement and even if we are able to manage to load this much data into the RAM, it will make the system very slow.\n## For this reason we use data generators a lot in Deep Learning. Data Generators are a functionality which is natively implemented in Python. The ImageDataGenerator class provided by the Keras API is nothing but an implementation of generator function in Python.\n## \n## So how does using a generator function solve this problem?\n## If you know the basics of Deep Learning, then you must know that to train a model on a particular dataset, we use some version of Stochastic Gradient Descent (SGD) like Adam, Rmsprop, Adagrad, etc.\n## With SGD, we do not calculate the loss on the entire data set to update the gradients. Rather in every iteration, we calculate the loss on a batch of data points (typically 64, 128, 256, etc.) to update the gradients.\n## \n## This means that we do not require to store the entire dataset in the memory at once. Even if we have the current batch of points in the memory, it is sufficient for our purpose.\n## A generator function in Python is used exactly for this purpose. It’s like an iterator which resumes the functionality from the point it left the last time it was called.\n## To understand more about Generators, please read here (https://wiki.python.org/moin/Generators).\n## ","execution_count":null,"outputs":[]},{"metadata":{"id":"dQkTM-yDjHkU","trusted":true},"cell_type":"code","source":"# data generator, use during the call to model.fit_generator() to create batchwise data\ndef data_generator_1(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in _descriptions.items():\n            n+=1\n            # retrieve the encoded features of image\n            img_feat = _imgs_features_arr[ key ] ## keys in the image encodings dict and descriptions dict use image_filename without the .jpg\n            for desc in desc_list:\n                # encode the sequence\n                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n                    # encode output sequence\n                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n                    # store\n                    X1.append(img_feat)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            # yield the batch data\n            if n == _images_per_batch:\n                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n                yield [np.array(X1), np.array(X2)], np.array(y)\n                X1, X2, y = list(), list(), list()\n                n=0","execution_count":11,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reload everything needed from pickled files created based on a certain split of Train and Validation datasets 97k:3k.\n## Run necessary code again to check all good to proceed with Training phase:\n## Outcome expected:\n## 1) unique words in original vocabulary = 24323\n## 2) unique words in culled vocab vocab_threshold = 6757\n##    Thus, VOCAB_SIZE = 6757 + 1 = 6758\n## 3) Embedding matrix shape should be (6758,200)"},{"metadata":{"trusted":true},"cell_type":"code","source":"IPDIR_DETERMINISTIC","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"'../input/thesis-imgcap-run2-deterministic-info/'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(IPDIR_DETERMINISTIC)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"['ixtoword_train_97000.pkl',\n 'descriptions_test.pkl',\n 'img_encodings_val_3000.pkl',\n 'img_encodings_train_97000.pkl',\n 'descriptions_train_97000.pkl',\n 'wordtoix_train_97000.pkl',\n 'embedding_matrix_6758_200.pkl',\n 'descriptions_train_97000_startend.pkl',\n 'descriptions_val_3000.pkl',\n 'descriptions_train_and_val.pkl',\n 'val_imgs_keys_3000_randomSplit444.pkl',\n 'train_imgs_keys_97000_randomSplit444.pkl']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reload all the files to make it deterministic\n\n\n# variable = img_encodings_train\nwith open(IPDIR_DETERMINISTIC + 'img_encodings_train_97000.pkl', 'rb') as handle:\n    img_encodings_train = pickle.load(handle)\n\n# variable = img_encodings_val\nwith open(IPDIR_DETERMINISTIC + 'img_encodings_val_3000.pkl', 'rb') as handle:\n    img_encodings_val = pickle.load(handle)\n\n# variable = descriptions_train\nwith open(IPDIR_DETERMINISTIC + 'descriptions_train_97000_startend.pkl', 'rb') as handle:\n    descriptions_train = pickle.load(handle)\n\n# variable = descriptions_val\nwith open(IPDIR_DETERMINISTIC + 'descriptions_val_3000.pkl', 'rb') as handle:\n    descriptions_val = pickle.load(handle)\n\n# variable = train_imgs_keys\nwith open(IPDIR_DETERMINISTIC + 'train_imgs_keys_97000_randomSplit444.pkl', 'rb') as handle:\n    train_imgs_keys = pickle.load(handle)\n\n# variable = val_imgs_keys\nwith open(IPDIR_DETERMINISTIC + 'val_imgs_keys_3000_randomSplit444.pkl', 'rb') as handle:\n    val_imgs_keys = pickle.load(handle)\n\n# variable = embedding_matrix\nwith open(IPDIR_DETERMINISTIC + 'embedding_matrix_6758_200.pkl', 'rb') as handle:\n    embedding_matrix = pickle.load(handle)\n\n# variable = ixtoword\nwith open(IPDIR_DETERMINISTIC + 'ixtoword_train_97000.pkl', 'rb') as handle:\n    ixtoword = pickle.load(handle)\n\n# variable = wordtoix\nwith open(IPDIR_DETERMINISTIC + 'wordtoix_train_97000.pkl', 'rb') as handle:\n    wordtoix = pickle.load(handle)\n\nprint(f\"Reloaded it all\")","execution_count":14,"outputs":[{"output_type":"stream","text":"Reloaded it all\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check the reloaded files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\t\\tlen(img_encodings_val) = {len(img_encodings_val)}\")\nprint(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\t\\tlen(descriptions_val) = {len(descriptions_val)}\")\nprint(f\"\\nCHECK : reloaded values = 97k for Train , 3k for Validation\")","execution_count":15,"outputs":[{"output_type":"stream","text":"Encodings data:\nlen(img_encodings_train) = 97000\t\tlen(img_encodings_val) = 3000\nDescriptions data:\nlen(descriptions_train) = 97000\t\tlen(descriptions_val) = 3000\n\nCHECK : reloaded values = 97k for Train , 3k for Validation\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## at this stage the descriptions_train already has the start and end tokens added to it\nvocabulary = set()\nfor key in descriptions_train.keys():\n    [vocabulary.update(d.split()) for d in descriptions_train[key]]\nprint(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")\nprint(f\"\\nCHECK : reloaded value = 24323\")","execution_count":16,"outputs":[{"output_type":"stream","text":"Original Vocabulary Size with all words = 24323\n\nCHECK : reloaded value = 24323\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n\nall_desc_in_training_samples = []\nfor key, val in descriptions_train.items():\n    for cap in val:\n        all_desc_in_training_samples.append(cap)\n\nMIN_WORD_COUNT_THRESHOLD = 10\nword_counts = {}\nnsents = 0\nfor each_desc in all_desc_in_training_samples:\n    nsents += 1\n    for w in each_desc.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n\nprint(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")\nprint(f\"\\nCHECK : reloaded value = 6757\")","execution_count":17,"outputs":[{"output_type":"stream","text":"Culled vocabulary to only retain words occurring more than threshold = 10 times.\nNew vocab size , len(vocab_threshold) = 6757\n\nCHECK : reloaded value = 6757\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n\n## convert a dictionary of clean descriptions to a list of descriptions\ndef extract_each_desc(_descriptions):\n    all_desc = list()\n    for key in _descriptions.keys():\n        [all_desc.append(d) for d in _descriptions[key]]\n    return all_desc\n\n## find the longest description length\ndef find_max_length_desc(_descriptions):\n    desc_sentences = extract_each_desc(_descriptions)\n    return max(len(d.split()) for d in desc_sentences)\n\nMAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\nprint(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")\nprint(f\"\\nCHECK : reloaded value = 49\")","execution_count":18,"outputs":[{"output_type":"stream","text":"Max Description Length: 49\n\nCHECK : reloaded value = 49\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\nprint(f\"len(wordtoix) = {len(wordtoix)}\")\nprint(f\"\\nCHECK : reloaded value = 6757\")\n\nVOCAB_SIZE = len(wordtoix) + 1\nprint(f\"\\n\\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = {VOCAB_SIZE}\")\n\nEMBEDDING_DIMS = 200\nprint(f\"\\n\\nSet the   EMBEDDING_DIMS = {EMBEDDING_DIMS}\")","execution_count":19,"outputs":[{"output_type":"stream","text":"len(wordtoix) = 6757\n\nCHECK : reloaded value = 6757\n\n\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = 6758\n\n\nSet the   EMBEDDING_DIMS = 200\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\nprint( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )","execution_count":20,"outputs":[{"output_type":"stream","text":"1 9 526\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")\nprint(f\"\\nCHECK : reloaded shape = 6758,200\")","execution_count":21,"outputs":[{"output_type":"stream","text":"Shape of embedding_matrix = (6758, 200)\n\nCHECK : reloaded shape = 6758,200\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Recheck these parameters that will define the Decoder architecture ::: EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\nprint(f\"SHOULD be:\\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\")\nprint(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")","execution_count":22,"outputs":[{"output_type":"stream","text":"SHOULD be:\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n\nThe values to be used in Decoder setup:\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n","name":"stdout"}]},{"metadata":{"id":"MTxczXFBRp85","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"CGzQr4tiiWDc"},"cell_type":"markdown","source":"## ALWAYS - Define the RNN Decoder model\n## Reload ALL weights from previous training point. Then freeze ONLY the embedding layer as before. Thus, the embedding layer will continue to use the GloVe-200 embeddings matrix weights that were first set during Training Phase 1 setup.\n\n\n### About Keras Emebedding layer\n### \n#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n######tf.keras.layers.Embedding(\n######    input_dim, output_dim,\n######    embeddings_initializer=\"uniform\",\n######    embeddings_regularizer=None,\n######    activity_regularizer=None,\n######    embeddings_constraint=None,\n######    mask_zero=False,\n######    input_length=None,\n######    **kwargs\n######)"},{"metadata":{"trusted":true},"cell_type":"code","source":"IPDIR_WEIGHTS_IN","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"'../input/imgcap-kagg-run2-weights-in/'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(IPDIR_WEIGHTS_IN)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"['Decoder_Run_2_Wt_ep_7.h5',\n 'Decoder_Run_2_Wt_ep_5.h5',\n 'Decoder_Run_2_Wt_ep_1.h5',\n 'Decoder_Run_2_Wt_ep_4.h5',\n 'Decoder_Run_2_Wt_ep_2.h5',\n 'Decoder_Run_2_Wt_ep_3.h5',\n 'Decoder_Run_2_Wt_ep_6.h5']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## IMPORTANT - SPECIFY CORRECT WEIGHTS FILE TO LOAD FROM"},{"metadata":{"trusted":true},"cell_type":"code","source":"RELOAD_WEIGHTS_FILE_NAME = r'Decoder_Run_2_Wt_ep_7.h5'","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RELOAD_WEIGHTS_FILE_PATH = IPDIR_WEIGHTS_IN + RELOAD_WEIGHTS_FILE_NAME\nprint(f\"Will reload this file = {RELOAD_WEIGHTS_FILE_PATH}\")\nprint(f\"Verfiy the weights in file exists : Check value is True = {os.path.exists(RELOAD_WEIGHTS_FILE_PATH) and os.path.isfile(RELOAD_WEIGHTS_FILE_PATH)}\")","execution_count":26,"outputs":[{"output_type":"stream","text":"Will reload this file = ../input/imgcap-kagg-run2-weights-in/Decoder_Run_2_Wt_ep_7.h5\nVerfiy the weights in file exists : Check value is True = True\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_parameter_counts(_model):\n    total_params = _model.count_params()\n    total_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.trainable_variables]))\n    total_non_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.non_trainable_variables]))\n    #print(f\"Total params = {total_decoder_params}\\t Trainable total = {total_decoder_trainable_params}\\t Non-trainable total = {total_decoder_non_trainable_params}\")\n    return total_params, total_trainable_params, total_non_trainable_params\n\ndef reload_rnn_encoder_for_more_training(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n    if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n        ## Decoder Model defining\n        \n        ## parameters to define model - sent during function call\n        #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n        #VOCAB_SIZE is initialised earlier\n        #MAX_LENGTH_CAPTION is initialised earlier\n        \n        inputs1 = keras.Input(shape=(2048,))\n        fe1 = keras.layers.Dropout(0.5)(inputs1)\n        fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n        \n        # partial caption sequence model\n        inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n        se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n        se2 = keras.layers.Dropout(0.5)(se1)\n        se3 = keras.layers.LSTM(256)(se2)\n        \n        # decoder (feed forward) model\n        decoder1 = keras.layers.add([fe2, se3])\n        decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n        outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n        \n        # merge the two input models\n        reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n        \n        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n        print(f\"\\nCreated RNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\")\n        print(f\"\\nBEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n        \n        \n        print(f\"\\nAttempting to load weights...\\n\")\n        \n        ## load the weights\n        reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n        print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n        \n        ## freeze the embeddings layer weights so they are non-trainable\n        reloaded_rnn_decoder_model.layers[2].trainable = False\n        print(f\"\\nFrozen embeddings layer.\")\n        \n        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n        print(f\"\\nAFTER FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n        \n        return reloaded_rnn_decoder_model\n    else:\n        print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n        return None","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\nprint(f\"Weights to be reloaded from here:\\n{RELOAD_WEIGHTS_FILE_PATH}\")","execution_count":28,"outputs":[{"output_type":"stream","text":"The values to be used in Decoder setup:\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\nWeights to be reloaded from here:\n../input/imgcap-kagg-run2-weights-in/Decoder_Run_2_Wt_ep_7.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reloaded_RNN_decoder = reload_rnn_encoder_for_more_training(RELOAD_WEIGHTS_FILE_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\nif reloaded_RNN_decoder is None:\n    print(f\"FATAL ERROR setting up Decoder\")\nelse:\n    print(f\"\\n{type(reloaded_RNN_decoder)}\")","execution_count":29,"outputs":[{"output_type":"stream","text":"\nCreated RNN Decoder model defined with these paramenters:\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n\nBEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\nTotal parameters = 4146710 , Trainable parameter = 4146710 , Non-trainable parameters = 0\n\nAttempting to load weights...\n\nSUCCESS - Reloaded weights from :: ../input/imgcap-kagg-run2-weights-in/Decoder_Run_2_Wt_ep_7.h5\n\nFrozen embeddings layer.\n\nAFTER FREEZING, parameter counts:\nTotal parameters = 4146710 , Trainable parameter = 2795110 , Non-trainable parameters = 1351600\n\n<class 'tensorflow.python.keras.engine.functional.Functional'>\n","name":"stdout"}]},{"metadata":{"id":"K_mLYJjsRqMO","outputId":"809ebcdd-f972-4f47-f60f-a207404054a5","trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params has REDUCED\nreloaded_RNN_decoder.summary()","execution_count":30,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 49)]         0                                            \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, 2048)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 49, 200)      1351600     input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nadd (Add)                       (None, 256)          0           dense[0][0]                      \n                                                                 lstm[0][0]                       \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 6758)         1736806     dense_1[0][0]                    \n==================================================================================================\nTotal params: 4,146,710\nTrainable params: 2,795,110\nNon-trainable params: 1,351,600\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n## NOTE the count of Trainable params has REDUCED\n\n#Model: \"functional_3\"\n#__________________________________________________________________________________________________\n#Layer (type)                    Output Shape         Param #     Connected to                     \n#==================================================================================================\n#input_4 (InputLayer)            [(None, 49)]         0                                            \n#__________________________________________________________________________________________________\n#input_3 (InputLayer)            [(None, 2048)]       0                                            \n#__________________________________________________________________________________________________\n#embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n#__________________________________________________________________________________________________\n#dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n#__________________________________________________________________________________________________\n#dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n#__________________________________________________________________________________________________\n#lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n#__________________________________________________________________________________________________\n#add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n#                                                                 lstm_1[0][0]                     \n#__________________________________________________________________________________________________\n#dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n#__________________________________________________________________________________________________\n#dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n#==================================================================================================\n#Total params: 4,146,710\n#Trainable params: 2,795,110\n#Non-trainable params: 1,351,600\n#__________________________________________________________________________________________________","execution_count":null,"outputs":[]},{"metadata":{"id":"iNo79p0Az37z","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"XB81Ohfoz4lL"},"cell_type":"markdown","source":"### ALWAYS - Compile RNN model"},{"metadata":{"id":"CF5Dw9brxxwh","trusted":true},"cell_type":"code","source":"## setup the optimizer and compile\n\n## see the default LR used, set some dummy to check it works\noptimizer_adam = tf.keras.optimizers.Adam()  ## if nothing specified the default LR = 0.001\nreloaded_RNN_decoder.compile(loss='categorical_crossentropy', optimizer=optimizer_adam)\nprint(f\"Default LR without explicity setting so far = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\")\n\noptimizer_adam.learning_rate.assign(0.5)\nprint(f\"After setting LR as 0.5 = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\")","execution_count":31,"outputs":[{"output_type":"stream","text":"Default LR without explicity setting so far = 0.0010000000474974513\nAfter setting LR as 0.5 = 0.5\n","name":"stdout"}]},{"metadata":{"id":"hIqp5fxB0ldP","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"xdJW1J2YmrxO"},"cell_type":"markdown","source":"## ALWAYS - Train the RNN Decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"OPDIR_WEIGHTS","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"'../working/weights_out/'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir( OPDIR_WEIGHTS )","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"[]"},"metadata":{}}]},{"metadata":{"id":"EXJ2Yjuz05wG","outputId":"c0ab13d8-b8d2-4450-ef87-589ebd49b46c","trusted":true},"cell_type":"code","source":"## values setup earlier - for reference\n\nprint(f\"\\tEncodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\nlen(img_encodings_val) = {len(img_encodings_val)}\")\nprint(f\"\\tDescriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\nlen(descriptions_val) = {len(descriptions_val)}\")\n\nprint(f\"\\nNO Validation Set being used for now.\\n\")\n\nprint(f\"len(wordtoix) = {len(wordtoix)}\")\nprint(f\"VOCAB_SIZE = {VOCAB_SIZE}\")\nprint(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}\")\nprint(f\"embedding_matrix Shape = {embedding_matrix.shape}\")\nprint(f\"MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\n\nprint(f\"\\nInitialed with this input weights file:\\n{RELOAD_WEIGHTS_FILE_PATH}\")","execution_count":34,"outputs":[{"output_type":"stream","text":"\tEncodings data:\nlen(img_encodings_train) = 97000\nlen(img_encodings_val) = 3000\n\tDescriptions data:\nlen(descriptions_train) = 97000\nlen(descriptions_val) = 3000\n\nNO Validation Set being used for now.\n\nlen(wordtoix) = 6757\nVOCAB_SIZE = 6758\nEMBEDDING_DIMS = 200\nembedding_matrix Shape = (6758, 200)\nMAX_LENGTH_CAPTION = 49\n\nInitialed with this input weights file:\n../input/imgcap-kagg-run2-weights-in/Decoder_Run_2_Wt_ep_7.h5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_RUN_NUMBER = 2 ## large training data - 97k train\nMODEL_RUN_NUMBER","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"2"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Specify the last epoch number (index from 1, NOT 0) so that the output weights files will have the correct epoch number for tracking\n## E.g. if earlier phase ran up to Ep 7, then specify below as 7\nEARLIER_TRAINING_EPOCH_END = 7\nEARLIER_TRAINING_EPOCH_END","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"7"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"########### Phase 4 #########\n\nprint(f\"\\n\\nMODEL {MODEL_RUN_NUMBER} :: Training Phase 4 started at :: {datetime.datetime.now().strftime('%H:%M:%S')}\\n\")\n\nLR_4 = 0.0001\nBATCH_SIZE_4 = 64   ## how many images per batch\nN_EPOCHS_4 = 3\nSTEPS_PER_EPOCH_4 = len(descriptions_train) // BATCH_SIZE_4\nprint(f\"\\nPhase 4 parameters:\")\nprint(f\"STEPS_PER_EPOCH_4 = {STEPS_PER_EPOCH_4}\")\nprint(f\"BATCH_SIZE_4 = {BATCH_SIZE_4}\")\nprint(f\"N_EPOCHS_4 = {N_EPOCHS_4}\")\n\noptimizer_adam.learning_rate.assign(LR_4)\n\nfor i in range( EARLIER_TRAINING_EPOCH_END , EARLIER_TRAINING_EPOCH_END + N_EPOCHS_4 ):\n    print(f\"\\nEpoch {i+1} started at {datetime.datetime.now().strftime('%H:%M:%S')}\\nLR used = {reloaded_RNN_decoder.optimizer.learning_rate.numpy()}\\n\")\n    generator_1 = data_generator_1(descriptions_train, img_encodings_train, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_4, VOCAB_SIZE)\n    reloaded_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_4, verbose=1)\n    \n    reloaded_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'Decoder_Run_' + str(MODEL_RUN_NUMBER) + '_Wt_ep_' + str(i+1) + '.h5' )","execution_count":null,"outputs":[{"output_type":"stream","text":"\n\nMODEL 2 :: Training Phase 4 started at :: 16:56:07\n\n\nPhase 4 parameters:\nSTEPS_PER_EPOCH_4 = 1515\nBATCH_SIZE_4 = 64\nN_EPOCHS_4 = 3\n\nEpoch 8 started at 16:56:07\nLR used = 9.999999747378752e-05\n\n1515/1515 [==============================] - 7329s 5s/step - loss: 3.0858\n\nEpoch 9 started at 18:58:27\nLR used = 9.999999747378752e-05\n\n1515/1515 [==============================] - 7264s 5s/step - loss: 3.0619\n\nEpoch 10 started at 20:59:36\nLR used = 9.999999747378752e-05\n\n1144/1515 [=====================>........] - ETA: 29:58 - loss: 3.0474","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"let this fail - console output at end of the Phase 3 training run\n\nMODEL 2 :: Training Phase 4 started at :: 16:56:07\n\n\nPhase 4 parameters:\nSTEPS_PER_EPOCH_4 = 1515\nBATCH_SIZE_4 = 64\nN_EPOCHS_4 = 3\n\nEpoch 8 started at 16:56:07\nLR used = 9.999999747378752e-05\n\n1515/1515 [==============================] - 7329s 5s/step - loss: 3.0858\n\nEpoch 9 started at 18:58:27\nLR used = 9.999999747378752e-05\n\n1515/1515 [==============================] - 7264s 5s/step - loss: 3.0619\n\nEpoch 10 started at 20:59:36\nLR used = 9.999999747378752e-05\n\n1143/1515 [=====================>........] - ETA: 30:03 - loss: 3.0474","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KEEP TRACK OF EARLIER PHASE TRAINING"},{"metadata":{"id":"J5Dx5QnMmrJ1","outputId":"ec46884f-bb34-4244-8b6f-ff525a537581","trusted":false},"cell_type":"code","source":"LET THIS CELL FAIL IF RUN ACCIDENTALLY - KEPT TO CAPTURE HOW THE PHASE 1 TRAINING WAS DONE AND THE OUTPUT AT THE END\n\n##########################################################\n##########################################################\n##########################################################\n\n########### Phase 1 #########\n\nprint(f\"\\n\\nMODEL {MODEL_RUN_NUMBER} :: Training Phase 1 started at :: {datetime.datetime.now().strftime('%H:%M:%S')}\\n\")\n\nLR_1 = 0.0005\nBATCH_SIZE_1 = 128   ## how many images per batch\nN_EPOCHS_1 = 2\nSTEPS_PER_EPOCH_1 = len(descriptions_train) // BATCH_SIZE_1\nprint(f\"\\nPhase 1 parameters:\")\nprint(f\"STEPS_PER_EPOCH_1 = {STEPS_PER_EPOCH_1}\")\nprint(f\"BATCH_SIZE_1 = {BATCH_SIZE_1}\")\nprint(f\"N_EPOCHS_1 = {N_EPOCHS_1}\")\n\noptimizer_adam.learning_rate.assign(LR_1)\n\nfor i in range(N_EPOCHS_1):\n    print(f\"\\nEpoch {i+1} started at {datetime.datetime.now().strftime('%H:%M:%S')}\\nLR used = {model_RNN_decoder.optimizer.learning_rate.numpy()} \\n\")\n    generator_1 = data_generator_1(descriptions_train, img_encodings_train, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_1, VOCAB_SIZE)\n    model_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_1, verbose=1)\n    \n    model_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'Decoder_Run_' + str(MODEL_RUN_NUMBER) + '_Wt_ep_' + str(i+1) + '.h5' )\n\n##########################################################\n\nThe console output for Phase 1 training at the end:\n\nMODEL 2 :: Training Phase 1 started at :: 17:15:11\n\n\nPhase 1 parameters:\nSTEPS_PER_EPOCH_1 = 757\nBATCH_SIZE_1 = 128\nN_EPOCHS_1 = 2\n\nEpoch 1 started at 17:15:11\nLR used = 0.0005000000237487257 \n\n757/757 [==============================] - 7705s 10s/step - loss: 4.3657\n\nEpoch 2 started at 19:23:55\nLR used = 0.0005000000237487257 \n\n757/757 [==============================] - 7793s 10s/step - loss: 3.4819\n\n##########################################################\n##########################################################\n##########################################################\n\n???????????????????????\nphase 2 is missing **************\n??????????????????????\n\n\n##########################################################\n##########################################################\n##########################################################\n\nMODEL 2 :: Training Phase 3 started at :: 08:28:14\n\n\nPhase 3 parameters:\nSTEPS_PER_EPOCH_3 = 757\nBATCH_SIZE_3 = 128\nN_EPOCHS_3 = 3\n\nEpoch 5 started at 08:28:14\nLR used = 0.00019999999494757503\n\n757/757 [==============================] - 7752s 10s/step - loss: 3.1868\n\nEpoch 6 started at 10:37:46\nLR used = 0.00019999999494757503\n\n757/757 [==============================] - 7795s 10s/step - loss: 3.1432\n\nEpoch 7 started at 12:47:52\nLR used = 0.00019999999494757503\n\n757/757 [==============================] - 7784s 10s/step - loss: 3.1107","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"4Xc08wQoRqZx","trusted":false},"cell_type":"code","source":"#model_reset_states\n#lr optimizer\n#tuner","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"0D3b7izSIqUb","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}