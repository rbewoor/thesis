{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "23dZtOGmFhQB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  lib  working\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "RuFZIchrFhSp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco-2017-dataset    thesis-imgcap-imgencodings-1\r\n",
      "embeddings-glove200  thesis-imgcap-run2-deterministic-info\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco-2017-dataset    thesis-imgcap-imgencodings-1\r\n",
      "embeddings-glove200  thesis-imgcap-run2-deterministic-info\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptions_test.pkl\t\t       img_encodings_train_97000.pkl\r\n",
      "descriptions_train_97000.pkl\t       img_encodings_val_3000.pkl\r\n",
      "descriptions_train_97000_startend.pkl  ixtoword_train_97000.pkl\r\n",
      "descriptions_train_and_val.pkl\t       train_imgs_keys_97000_randomSplit444.pkl\r\n",
      "descriptions_val_3000.pkl\t       val_imgs_keys_3000_randomSplit444.pkl\r\n",
      "embedding_matrix_6758_200.pkl\t       wordtoix_train_97000.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/thesis-imgcap-run2-deterministic-info/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118287\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/coco-2017-dataset/coco2017/train2017/' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/coco-2017-dataset/coco2017/val2017/' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions_train2017.json   instances_val2017.json\r\n",
      "captions_val2017.json\t  person_keypoints_train2017.json\r\n",
      "instances_train2017.json  person_keypoints_val2017.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/coco-2017-dataset/coco2017/annotations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl\r\n",
      "train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl\r\n",
      "val2017_all_5k_images_encoded_features_pickled_2.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/thesis-imgcap-imgencodings-1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/thesis-imgcap-imgencodings-1/' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove200_embeddings_dict_1.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../input/embeddings-glove200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir '../working/weights_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook_source__.ipynb  weights_out\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!ls '../working/weights_out/' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -r '../working/weights_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-rBS0Z-E4p8"
   },
   "source": [
    "# Training model on 97k of some 100k images of Coco_Train2017 dataset. Using the balance 3k as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n",
    "\n",
    "## Created subsets of the total data and used them to make the image feature encodings during preprocessing to avoid bottlenecking during training.\n",
    "\n",
    "### Had already uploaded all 118287 images to Google drive.\n",
    "### Made folders for subsets of 5k images.\n",
    "### Split done using the list from os.listdir(gdrive folder)[ slice_start : slice_end ].\n",
    "### Ran those subsets of 5k images through the CNN-Encoder to get one file of encodings of the image feature arrays.\n",
    "### The encoder is pre-trained Google Inception-v3 trained on Imagenet.\n",
    "\n",
    "## AVAILABLE data :\n",
    "### Coco_Train2017     = has 118287 images\n",
    "### Coco_Val2017       = has 5000   images\n",
    "### Combined total     = 123287     images\n",
    "\n",
    "## USED data :\n",
    "### Coco_Train2017     = 100000  images\n",
    "### Coco_Val2017       = 5000    images\n",
    "### Combined total     = 105000  images\n",
    "\n",
    "### Using only the first 100k images of Train2017 + all 5k images of Val2017\n",
    "### Thus total data available for training = 100k + 5k = 105k\n",
    "### Details of split of data:\n",
    "### Training   data = 97000 images from Coco_Train2017\n",
    "### Validation data = 3000  images from Coco_Train2017\n",
    "### Test       data = 5000  images from Coco_Val2017\n",
    "\n",
    "### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n",
    "###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n",
    "\n",
    "## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "## Image captioning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "buTVBtveEpqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "#import itertools\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "## import numpy as np\n",
    "## from numpy import array\n",
    "## import pandas as pd\n",
    "## import matplotlib.pyplot as plt\n",
    "## %matplotlib inline\n",
    "## import string\n",
    "## import os\n",
    "## from PIL import Image\n",
    "## import glob\n",
    "## from pickle import dump, load\n",
    "## from time import time\n",
    "## from keras.preprocessing import sequence\n",
    "## from keras.models import Sequential\n",
    "## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "## from keras.optimizers import Adam, RMSprop\n",
    "## from keras.layers.wrappers import Bidirectional\n",
    "## from keras.layers.merge import add\n",
    "## from keras.applications.inception_v3 import InceptionV3\n",
    "## from keras.preprocessing import image\n",
    "## from keras.models import Model\n",
    "## from keras import Input, layers\n",
    "## from keras import optimizers\n",
    "## from keras.applications.inception_v3 import preprocess_input\n",
    "## from keras.preprocessing.text import Tokenizer\n",
    "## from keras.preprocessing.sequence import pad_sequences\n",
    "## from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebw9HVoxEptM",
    "outputId": "84656bc7-27ad-46cd-e2fe-bab33c5c0032"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "0b0aXNyCEd3p"
   },
   "outputs": [],
   "source": [
    "## Kaggle versions\n",
    "\n",
    "## Weights from training till now\n",
    "OPDIR = r'../working/'\n",
    "\n",
    "## New weights to save here\n",
    "OPDIR_WEIGHTS = r'../working/weights_out/'\n",
    "\n",
    "## Images locations\n",
    "IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "IPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n",
    "\n",
    "## Annotations json file location from where to pick up the captions\n",
    "IPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\n",
    "\n",
    "## Bottleneck CNN Encoder output for all the images to be used in training\n",
    "IPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\n",
    "\n",
    "## using GloVe - 200 file, already created the embedding matrix and pickled\n",
    "IPDIR_EMBED_DICT = r'../input/embeddings-glove200/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on earlier split\n",
    "IPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\n",
    "\n",
    "\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n",
    "\n",
    "## Google drive versions\n",
    "#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
    "#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
    "#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGBalAoSEjmf"
   },
   "source": [
    "## ALWAYS - Data Preprocessing — Images - RELOAD FROM PICKLE FILE: Image encodings info obtained from encoder\n",
    "\n",
    "### Create two encodings dicts from the already picked files of encodings:\n",
    "###   1) img_encodings_train_and_val from the Train2017 dataset - used for the training and validation data DURING model training.\n",
    "###      Will contain 100k image feature vectors\n",
    "###   2) img_encodings_test from the Val2017   dataset - used only for the Testing phase AFTER model training is completed.\n",
    "###      Will contain 5k image feature vectors\n",
    "\n",
    "### e.g. a val2017 dataset image entry is: img_encodings['000000179765'] should be as below:\n",
    "#### array([0.14290808, 0.14481388, 0.30199888, ..., 0.20583029, 0.1378399 ,\n",
    "####        0.05842396], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl',\n",
       " 'val2017_all_5k_images_encoded_features_pickled_2.pkl',\n",
       " 'train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl',\n",
       " 'train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_IMG_ENCODINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading encodings for the train+val images\n",
      "Loaded file num 1 :: train2017_subset_5k_55000_60000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 5000\n",
      "Loaded file num 2 :: train2017_subset_5k_70000_75000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 10000\n",
      "Loaded file num 3 :: train2017_subset_5k_95000_100000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 15000\n",
      "Loaded file num 4 :: train2017_subset_5k_40000_45000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 20000\n",
      "Loaded file num 5 :: train2017_subset_5k_60000_65000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 25000\n",
      "Loaded file num 6 :: train2017_subset_5k_35000_40000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 30000\n",
      "Loaded file num 7 :: train2017_subset_5k_30000_35000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 35000\n",
      "Loaded file num 8 :: train2017_subset_5k_25000_30000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 40000\n",
      "Loaded file num 9 :: train2017_subset_5k_45000_50000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 45000\n",
      "Loaded file num 10 :: train2017_subset_5k_5000_10000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 50000\n",
      "Loaded file num 11 :: train2017_subset_5k_85000_90000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 55000\n",
      "Loaded file num 12 :: train2017_subset_5k_65000_70000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 60000\n",
      "Loaded file num 13 :: train2017_subset_5k_15000_20000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 65000\n",
      "Loaded file num 14 :: train2017_subset_5k_75000_80000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 70000\n",
      "Loaded file num 15 :: train2017_subset_5k_10000_15000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 75000\n",
      "Loaded file num 16 :: train2017_subset_5k_80000_85000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 80000\n",
      "Loaded file num 17 :: train2017_subset_5k_50000_55000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 85000\n",
      "Loaded file num 18 :: train2017_subset_5k_20000_25000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 90000\n",
      "Loaded file num 19 :: train2017_subset_5k_0_5000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 95000\n",
      "Loaded file num 20 :: train2017_subset_5k_90000_95000_images_encoded_features_pickled_1.pkl\n",
      "Number of entries in img_encodings_train2017 = 100000\n",
      "\n",
      "Final count img_encodings_train_and_val = 100000. Should be = 100k.\n",
      "\n",
      "\n",
      "Loading encodings for the test images\n",
      "Loaded file num 1 :: val2017_all_5k_images_encoded_features_pickled_2.pkl\n",
      "Number of entries in img_encodings_test = 5000\n",
      "\n",
      "Final count img_encodings_test = 5000. Should be = 5k.\n"
     ]
    }
   ],
   "source": [
    "## From Train2017 images pickled files\n",
    "print(f\"\\n\\nLoading encodings for the train+val images\")\n",
    "pickled_encodings_files_train2017 = os.listdir(IPDIR_IMG_ENCODINGS)\n",
    "pickled_encodings_files_train2017.remove('val2017_all_5k_images_encoded_features_pickled_2.pkl')  ## remove the val2017 pickle file\n",
    "img_encodings_train_and_val = {}\n",
    "for idx, subset_encodings_file_train2017 in enumerate(pickled_encodings_files_train2017):\n",
    "    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_train2017, 'rb') as handle:\n",
    "        img_encodings_train_and_val.update(pickle.load(handle))\n",
    "        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_train2017}\")\n",
    "    print(f\"Number of entries in img_encodings_train2017 = {len(img_encodings_train_and_val)}\")\n",
    "print(f\"\\nFinal count img_encodings_train_and_val = {len(img_encodings_train_and_val)}. Should be = 100k.\")\n",
    "\n",
    "\n",
    "\n",
    "## From Val2017 images pickled files\n",
    "print(f\"\\n\\nLoading encodings for the test images\")\n",
    "pickled_encodings_files_val2017 = ['val2017_all_5k_images_encoded_features_pickled_2.pkl']\n",
    "img_encodings_test = {}\n",
    "for idx, subset_encodings_file_val2017 in enumerate(pickled_encodings_files_val2017):\n",
    "    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_val2017, 'rb') as handle:\n",
    "        img_encodings_test.update(pickle.load(handle))\n",
    "        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_val2017}\")\n",
    "    print(f\"Number of entries in img_encodings_test = {len(img_encodings_test)}\")\n",
    "print(f\"\\nFinal count img_encodings_test = {len(img_encodings_test)}. Should be = 5k.\")\n",
    "\n",
    "del pickled_encodings_files_train2017, pickled_encodings_files_val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWNaS_12FJts"
   },
   "source": [
    "## ALWAYS - create \"descriptions\" dictionaries mapping an image to its list of captions\n",
    "## A descriptions dict has the key   = image name without the .jpg extension\n",
    "##                         the value = [list of 5 captions for that image]\n",
    "\n",
    "### Uses the annotations json files for COCO_Val2017 and COCO_Train2017\n",
    "\n",
    "### Create two dicts:\n",
    "###   1) \"descriptions_train_and_val_dataset\" dict from the Train2017 captions file - used for training and validation data DURING model training.\n",
    "###      Will contain 118k images info when first loaded.\n",
    "###      As only 100k of this is being used, will cull it to the correct 100k images and their descriptions later.\n",
    "###   2) \"descriptions_test_dataset\" from the Val2017 captions file - used only for the Testing phase AFTER model training is completed.\n",
    "###      Will contain 5k images info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMN9aeYBFaJa"
   },
   "source": [
    "### process the captions_train2017.json file - will be used to create our TRAIN AND VALIDATION DATASET of 97k and 3k images respectively.\n",
    "### For now, it will have 100k values, but later it will be split using the train_test_split function of scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "bO75f_hAEpx8",
    "outputId": "2d1a878e-ae55-4e91-cb4f-e9c9e5a00f4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person_keypoints_train2017.json',\n",
       " 'instances_val2017.json',\n",
       " 'instances_train2017.json',\n",
       " 'person_keypoints_val2017.json',\n",
       " 'captions_val2017.json',\n",
       " 'captions_train2017.json']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_ANNO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "14VHYPxjEp0f",
    "outputId": "4039688e-0732-4743-a69f-d29ea933aa1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203564</td>\n",
       "      <td>37</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>322141</td>\n",
       "      <td>49</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16977</td>\n",
       "      <td>89</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id  id                                            caption\n",
       "0    203564  37  A bicycle replica with a clock as the front wh...\n",
       "1    322141  49  A room with blue walls and a white sink and door.\n",
       "2     16977  89  A car that seems to be parked illegally behind..."
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build the descriptions_train_and_val_dataset dict\n",
    "\n",
    "with open(IPDIR_ANNO+'captions_train2017.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
    "  #type(data['annotations']) # is a list\n",
    "  #type(data['images'])      # also is a list\n",
    "\n",
    "dfanno = pd.DataFrame(data=data['annotations'])\n",
    "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
    "# dfanno.dtypes =\n",
    "#   image_id     int64\n",
    "#   id           int64\n",
    "#   caption     object\n",
    "#   dtype: object\n",
    "\n",
    "dfimages = pd.DataFrame(data=data['images'])\n",
    "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
    "# dfimages.dtypes =\n",
    "#   license           int64\n",
    "#   file_name        object\n",
    "#   coco_url         object\n",
    "#   height            int64\n",
    "#   width             int64\n",
    "#   date_captured    object\n",
    "#   flickr_url       object\n",
    "#   id                int64\n",
    "#   dtype: object\n",
    "## of above, am dropping useless columns\n",
    "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
    "\n",
    "## columns remaining in the dfs are:\n",
    "# dfanno columns are      image_id , id , caption\n",
    "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
    "# dfimages columns are    file_name        , height ,  width , id\n",
    "#                         000000397133.jpg , 427    ,  640   , 397133\n",
    "\n",
    "## the captions are not ordered for each image and seem to be randomly placed\n",
    "\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "bh35WWkGEp3A",
    "outputId": "5bd89a4b-84b5-4d4f-ce98-9a6db170c57d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203564</td>\n",
       "      <td>37</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "      <td>000000203564.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203564</td>\n",
       "      <td>181</td>\n",
       "      <td>The bike has a clock as a tire.</td>\n",
       "      <td>000000203564.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203564</td>\n",
       "      <td>478</td>\n",
       "      <td>A black metal bicycle with a clock inside the ...</td>\n",
       "      <td>000000203564.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption  \\\n",
       "0    203564   37  A bicycle replica with a clock as the front wh...   \n",
       "1    203564  181                    The bike has a clock as a tire.   \n",
       "2    203564  478  A black metal bicycle with a clock inside the ...   \n",
       "\n",
       "          file_name  \n",
       "0  000000203564.jpg  \n",
       "1  000000203564.jpg  \n",
       "2  000000203564.jpg  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n",
    "## the file_name column has the actual image name from the \"image\" key section\n",
    "dfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\n",
    "dfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\n",
    "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "GljG2CRREp5s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(descriptions_train_and_val) = 118287\n"
     ]
    }
   ],
   "source": [
    "## Mapping image with captions using dictionary\n",
    "\n",
    "def create_descriptions_dictionary(_dfin):\n",
    "    descriptions = {}\n",
    "    for row in _dfin.itertuples():\n",
    "      rowdict = row._asdict()\n",
    "      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n",
    "      img_caption = rowdict['caption']\n",
    "      if(img_filename not in descriptions):\n",
    "        descriptions[img_filename] = [img_caption]\n",
    "      else:\n",
    "        descriptions[img_filename].append(img_caption)\n",
    "    return descriptions\n",
    "\n",
    "descriptions_train_and_val = create_descriptions_dictionary(dfanno)\n",
    "print(f\"len(descriptions_train_and_val) = {len(descriptions_train_and_val)}\")\n",
    "\n",
    "del dfanno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process the captions_val2017.json file - will be used to create our TEST DATASET of 5k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190236</td>\n",
       "      <td>401</td>\n",
       "      <td>An office cubicle with four different types of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...\n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway\n",
       "2    190236  401  An office cubicle with four different types of..."
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build the descriptions_test_dataset dict\n",
    "\n",
    "with open(IPDIR_ANNO+'captions_val2017.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
    "  #type(data['annotations']) # is a list\n",
    "  #type(data['images'])      # also is a list\n",
    "\n",
    "dfanno = pd.DataFrame(data=data['annotations'])\n",
    "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
    "# dfanno.dtypes =\n",
    "#   image_id     int64\n",
    "#   id           int64\n",
    "#   caption     object\n",
    "#   dtype: object\n",
    "\n",
    "dfimages = pd.DataFrame(data=data['images'])\n",
    "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
    "# dfimages.dtypes =\n",
    "#   license           int64\n",
    "#   file_name        object\n",
    "#   coco_url         object\n",
    "#   height            int64\n",
    "#   width             int64\n",
    "#   date_captured    object\n",
    "#   flickr_url       object\n",
    "#   id                int64\n",
    "#   dtype: object\n",
    "## of above, am dropping useless columns\n",
    "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
    "\n",
    "## columns remaining in the dfs are:\n",
    "# dfanno columns are      image_id , id , caption\n",
    "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
    "# dfimages columns are    file_name        , height ,  width , id\n",
    "#                         000000397133.jpg , 427    ,  640   , 397133\n",
    "\n",
    "## the captions are not ordered for each image and seem to be randomly placed\n",
    "\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179765</td>\n",
       "      <td>479</td>\n",
       "      <td>A black Honda motorcycle with a dark burgundy ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption  \\\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...   \n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway   \n",
       "2    179765  479  A black Honda motorcycle with a dark burgundy ...   \n",
       "\n",
       "          file_name  \n",
       "0  000000179765.jpg  \n",
       "1  000000179765.jpg  \n",
       "2  000000179765.jpg  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n",
    "## the file_name column has the actual image name from the \"image\" key section\n",
    "dfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\n",
    "dfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\n",
    "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(descriptions_test) = 5000\n"
     ]
    }
   ],
   "source": [
    "## Mapping image with captions using dictionary\n",
    "\n",
    "def create_descriptions_dictionary(_dfin):\n",
    "    descriptions = {}\n",
    "    for row in _dfin.itertuples():\n",
    "      rowdict = row._asdict()\n",
    "      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n",
    "      img_caption = rowdict['caption']\n",
    "      if(img_filename not in descriptions):\n",
    "        descriptions[img_filename] = [img_caption]\n",
    "      else:\n",
    "        descriptions[img_filename].append(img_caption)\n",
    "    return descriptions\n",
    "\n",
    "descriptions_test = create_descriptions_dictionary(dfanno)\n",
    "print(f\"len(descriptions_test) = {len(descriptions_test)}\")\n",
    "\n",
    "del dfanno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this stage the \"descriptions_train_and_val\"  dict has info for ALL Coco_Train2017 images = 118287 images. Since, for model training only using 100k images, the extra images info needs to be removed.\n",
    "### The image encodings contain the 100k image to be retained. So use its keys to retain the correct info in descriptions_train_and_val as they have same key i.e. image file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in descriptions_train_and_val_dataset BEFORE culling = 118287\n",
      "Retained entries in descriptions_train_and_val = 100000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entries in descriptions_train_and_val_dataset BEFORE culling = {len(descriptions_train_and_val)}\")\n",
    "retain_dict = {}\n",
    "for key in img_encodings_train_and_val.keys():\n",
    "    retain_dict.update({key:descriptions_train_and_val[key]})\n",
    "descriptions_train_and_val = retain_dict.copy() ## replace with the just identifed 100k images for Train+Validation model training\n",
    "del retain_dict\n",
    "print(f\"Retained entries in descriptions_train_and_val = {len(descriptions_train_and_val)}\")  ## will be for all 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOVUkimpI819"
   },
   "source": [
    "## Data cleaning on the descriptions dicts for descriptions_train_and_val\n",
    "## First on the descriptions_train_and_val_dataset\n",
    "##       Later same cleanup on descriptions_test\n",
    "\n",
    "### Clean up actions done:\n",
    "### 1. lower casing\n",
    "### 2. punctuation removal i.e. string.punctuation which coevers '!\" HASH DOLLAR_SIGN %&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' are replaced with a space\n",
    "### 3. remove any words with length = 1\n",
    "### 4. remove any words now left as non-alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two men wearing aprons working in a commercial-style kitchen.',\n",
       " 'Chefs preparing food in a professional metallic style kitchen.',\n",
       " 'Two people standing around in a large kitchen.',\n",
       " 'A commercial kitchen with two men working to prepare several plates.',\n",
       " 'two men in white shirts in a large steel kitchen']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with a hyphen - in the caption\n",
    "descriptions_train_and_val['000000005802']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "vluW2RFnEqD4"
   },
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\n",
    "## commented table made \"commercial-style kitchen\" into \"commercialstyle kitchen\"\n",
    "#tran_table = str.maketrans('', '', string.punctuation)\n",
    "## but this table makes \"commercial-style kitchen\" into \"commercial style kitchen\"\n",
    "tran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "for key, desc_list in descriptions_train_and_val.items():\n",
    "    for idx in range(len(desc_list)):\n",
    "        desc = desc_list[idx]\n",
    "        # replace all punctuation with space in description before tokenizing\n",
    "        desc = desc.translate(tran_table)\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove any non-alphabetic tokens\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # overwrite with cleaned description\n",
    "        desc_list[idx] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "lY0tQci4Ip_C",
    "outputId": "57238993-d160-4007-b7a1-10c04942bc5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two men wearing aprons working in commercial style kitchen',\n",
       " 'chefs preparing food in professional metallic style kitchen',\n",
       " 'two people standing around in large kitchen',\n",
       " 'commercial kitchen with two men working to prepare several plates',\n",
       " 'two men in white shirts in large steel kitchen']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption which had the hyphen - in the caption  -- POST CLEANUP\n",
    "descriptions_train_and_val['000000005802']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning on the descriptions dicts for descriptions_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': ['A black Honda motorcycle parked in front of a garage.',\n",
       "  'A Honda motorcycle parked in a grass driveway',\n",
       "  'A black Honda motorcycle with a dark burgundy seat.',\n",
       "  'Ma motorcycle parked on the gravel in front of a garage',\n",
       "  'A motorcycle with its brake extended standing outside'],\n",
       " '000000190236': ['An office cubicle with four different types of computers.',\n",
       "  'The home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'Office setting with a lot of computer screens.',\n",
       "  'A desk and chair in an office cubicle.']}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(descriptions_test.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A dog sitting between its masters feet on a footstool watching tv\\n',\n",
       " 'A dog between the feet of a person looking at a TV.',\n",
       " 'A dog and a person are watching television together.',\n",
       " 'A person is sitting with their dog watching tv.',\n",
       " 'A man relaxing at home, watching television with his dog.']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\n",
    "tran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "for key, desc_list in descriptions_test.items():\n",
    "    for idx in range(len(desc_list)):\n",
    "        desc = desc_list[idx]\n",
    "        # replace all punctuation with space in description before tokenizing\n",
    "        desc = desc.translate(tran_table)\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove any non-alphabetic tokens\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # overwrite with cleaned description\n",
    "        desc_list[idx] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog sitting between its masters feet on footstool watching tv',\n",
       " 'dog between the feet of person looking at tv',\n",
       " 'dog and person are watching television together',\n",
       " 'person is sitting with their dog watching tv',\n",
       " 'man relaxing at home watching television with his dog']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption  -- POST CLEANUP\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../working/'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open( OPDIR + \\'descriptions_train_and_val.pkl\\' , \\'wb\\') as handle:\\n  pickle.dump(descriptions_train_and_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for descriptions_train_and_val pickled to :: {\\'descriptions_train_and_val.pkl\\'} :: len(descriptions_train_and_val) = {len(descriptions_train_and_val)}\")\\n\\n\\nwith open( OPDIR + \\'descriptions_test.pkl\\' , \\'wb\\') as handle:\\n  pickle.dump(descriptions_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for descriptions_test pickled to :: {\\'descriptions_test.pkl\\'} :: len(descriptions_test) = {len(descriptions_test)}\")\\n'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pickle the cleaned up descriptions_train_and_val    and    descriptions_test\n",
    "\n",
    "\"\"\"\n",
    "with open( OPDIR + 'descriptions_train_and_val.pkl' , 'wb') as handle:\n",
    "  pickle.dump(descriptions_train_and_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for descriptions_train_and_val pickled to :: {'descriptions_train_and_val.pkl'} :: len(descriptions_train_and_val) = {len(descriptions_train_and_val)}\")\n",
    "\n",
    "\n",
    "with open( OPDIR + 'descriptions_test.pkl' , 'wb') as handle:\n",
    "  pickle.dump(descriptions_test, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for descriptions_test pickled to :: {'descriptions_test.pkl'} :: len(descriptions_test) = {len(descriptions_test)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfE6Q0FSQJbM"
   },
   "source": [
    "## ALWAYS - create the Train and Validation datasets using the 100k images and pickle the files for reload later\n",
    "### TRAIN_SIZE = 97k, balance 3k for VALIDATION_SIZE\n",
    "### Split descriptions data i.e. \"descriptions_train_and_val\"   into \"descriptions_train\"  and \"descriptions_val\"\n",
    "### Split encodings data    i.e. \"img_encodings_train_and_val\"  into \"img_encodings_train\" and \"img_encodings_val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys to be used for the split = 100000\n",
      "Sizes of: Train dataset = 97000,  Validation dataset = 3000\n"
     ]
    }
   ],
   "source": [
    "all_train_and_val_keys = [key for key in img_encodings_train_and_val.keys()]\n",
    "print(f\"Number of keys to be used for the split = {len(all_train_and_val_keys)}\")\n",
    "\n",
    "TRAIN_SIZE_N = 97000\n",
    "train_imgs_keys, val_imgs_keys = train_test_split(all_train_and_val_keys, train_size = TRAIN_SIZE_N, random_state=444)\n",
    "\n",
    "print(f\"Sizes of: Train dataset = {len(train_imgs_keys)},  Validation dataset = {len(val_imgs_keys)}\")\n",
    "del all_train_and_val_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of: Train description = 97000,  Validation description = 3000\n",
      "Sizes of: Train encodings   = 97000, Validation encodings = 3000\n"
     ]
    }
   ],
   "source": [
    "## use the keys of the just created split files containing the keys to makes the splits for the descriptions and the encodings dicts\n",
    "\n",
    "descriptions_train = dict()\n",
    "descriptions_val = dict()\n",
    "img_encodings_train = dict()\n",
    "img_encodings_val = dict()\n",
    "\n",
    "for key in train_imgs_keys:\n",
    "    descriptions_train.update({key:descriptions_train_and_val.get(key)})\n",
    "    img_encodings_train.update({key:img_encodings_train_and_val.get(key)})\n",
    "\n",
    "for key in val_imgs_keys:\n",
    "    descriptions_val.update({key:descriptions_train_and_val.get(key)})\n",
    "    img_encodings_val.update({key:img_encodings_train_and_val.get(key)})\n",
    "\n",
    "print(f\"Sizes of: Train description = {len(descriptions_train)},  Validation description = {len(descriptions_val)}\")\n",
    "print(f\"Sizes of: Train encodings   = {len(img_encodings_train)}, Validation encodings = {len(img_encodings_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../working/'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPICKLE_FILENAME_descriptions_train  = r\\'descriptions_train_97000.pkl\\'\\nPICKLE_FILENAME_descriptions_val    = r\\'descriptions_val_3000.pkl\\'\\nPICKLE_FILENAME_img_encodings_train = r\\'img_encodings_train_97000.pkl\\'\\nPICKLE_FILENAME_img_encodings_val   = r\\'img_encodings_val_3000.pkl\\'\\n\\nwith open( OPDIR + PICKLE_FILENAME_descriptions_train , \\'wb\\') as handle:\\n  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for descriptions_train pickled to :: {PICKLE_FILENAME_descriptions_train}\")\\n\\nwith open( OPDIR + PICKLE_FILENAME_descriptions_val , \\'wb\\') as handle:\\n  pickle.dump(descriptions_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for descriptions_val pickled to :: {PICKLE_FILENAME_descriptions_val}\")\\n\\nwith open( OPDIR + PICKLE_FILENAME_img_encodings_train , \\'wb\\') as handle:\\n  pickle.dump(img_encodings_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for img_encodings_train pickled to :: {PICKLE_FILENAME_img_encodings_train}\")\\n\\nwith open( OPDIR + PICKLE_FILENAME_img_encodings_val , \\'wb\\') as handle:\\n  pickle.dump(img_encodings_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for img_encodings_val pickled to :: {PICKLE_FILENAME_img_encodings_val}\")\\n\\n\\nPICKLE_FILENAME_train_imgs_keys   = r\\'train_imgs_keys_97000_randomSplit444.pkl\\'\\nPICKLE_FILENAME_val_imgs_keys     = r\\'val_imgs_keys_3000_randomSplit444.pkl\\'\\n\\nwith open( OPDIR + PICKLE_FILENAME_train_imgs_keys , \\'wb\\') as handle:\\n  pickle.dump(train_imgs_keys, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for train_imgs_keys pickled to :: {PICKLE_FILENAME_train_imgs_keys}\")\\n\\nwith open( OPDIR + PICKLE_FILENAME_val_imgs_keys , \\'wb\\') as handle:\\n  pickle.dump(val_imgs_keys, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for val_imgs_keys pickled to :: {PICKLE_FILENAME_val_imgs_keys}\")\\n'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pickle the descriptions , encodings data for train and validation - split is 97k Train 3k for Validation\n",
    "## pickle also the train_img_keys and val_img_keys \n",
    "\n",
    "\"\"\"\n",
    "PICKLE_FILENAME_descriptions_train  = r'descriptions_train_97000.pkl'\n",
    "PICKLE_FILENAME_descriptions_val    = r'descriptions_val_3000.pkl'\n",
    "PICKLE_FILENAME_img_encodings_train = r'img_encodings_train_97000.pkl'\n",
    "PICKLE_FILENAME_img_encodings_val   = r'img_encodings_val_3000.pkl'\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_descriptions_train , 'wb') as handle:\n",
    "  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for descriptions_train pickled to :: {PICKLE_FILENAME_descriptions_train}\")\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_descriptions_val , 'wb') as handle:\n",
    "  pickle.dump(descriptions_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for descriptions_val pickled to :: {PICKLE_FILENAME_descriptions_val}\")\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_img_encodings_train , 'wb') as handle:\n",
    "  pickle.dump(img_encodings_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for img_encodings_train pickled to :: {PICKLE_FILENAME_img_encodings_train}\")\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_img_encodings_val , 'wb') as handle:\n",
    "  pickle.dump(img_encodings_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for img_encodings_val pickled to :: {PICKLE_FILENAME_img_encodings_val}\")\n",
    "\n",
    "\n",
    "PICKLE_FILENAME_train_imgs_keys   = r'train_imgs_keys_97000_randomSplit444.pkl'\n",
    "PICKLE_FILENAME_val_imgs_keys     = r'val_imgs_keys_3000_randomSplit444.pkl'\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_train_imgs_keys , 'wb') as handle:\n",
    "  pickle.dump(train_imgs_keys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for train_imgs_keys pickled to :: {PICKLE_FILENAME_train_imgs_keys}\")\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_val_imgs_keys , 'wb') as handle:\n",
    "  pickle.dump(val_imgs_keys, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for val_imgs_keys pickled to :: {PICKLE_FILENAME_val_imgs_keys}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weights_out', '__notebook_source__.ipynb']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(OPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aixmm4GbVf8n"
   },
   "source": [
    "## ALWAYS - Add the    \"startseq\"    and    \"endseq\"    tokens to all the training example descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "9ZOSv26-VfLT",
    "outputId": "6fbe0073-c3e7-4cf5-a850-c737171c69ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000044704 ['jar of liquid next to blender full of liquid', 'jar full of red blended good next to blender filled with the same', 'red drink that was made in blender', 'blender and clear glass next to each other with red liquid in them', 'blender and jar of red liquid on table']\n",
      "000000357865 ['bathroom with toilet and sink with cup on it', 'white toilet sitting next to white sink in bathroom', 'small bathroom has towels rugs and gray floor', 'an image of bathroom setting with toilet and counter', 'bathroom with toilet sink trash can towel rack and mirror']\n",
      "000000131172 ['group of people are sitting on bench outside', 'man sitting by woman and girl who are on their cell phones', 'black and white photograph of people on bench table', 'lot of people are sitting on the bench', 'man and his daughters sit on bench']\n",
      "000000466024 ['two bears play in water beside cement wall', 'two bears wrestle in pool of water', 'two bears are holding on to each other in the middle of water', 'two bears holding each other in water by wall', 'two big brown bears are wrestling in their swimming pool']\n",
      "000000380636 ['young person going down hill on snowboard', 'person in red jacket doing snowboard trick', 'person that is snow boading down snowy hill', 'person jumps in mid air on snowboard', 'person riding snowboard at high speed']\n"
     ]
    }
   ],
   "source": [
    "## see the unchanged descriptions - note no startseq and endseq present now\n",
    "i=0\n",
    "for k, v in descriptions_train.items():\n",
    "  i+=1\n",
    "  if i>5:\n",
    "    break\n",
    "  print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add startseq and endseq to the captions of all entries in descriptions_train\n",
    "\n",
    "for k, v in descriptions_train.items():\n",
    "    updated_values = [''.join(['startseq ', each_desc, ' endseq']) for each_desc in v]\n",
    "    descriptions_train.update( { k : updated_values } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "vGWuZDqKfTjp",
    "outputId": "239b4fc9-7863-4223-9167-5a6339156720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000044704 ['startseq jar of liquid next to blender full of liquid endseq', 'startseq jar full of red blended good next to blender filled with the same endseq', 'startseq red drink that was made in blender endseq', 'startseq blender and clear glass next to each other with red liquid in them endseq', 'startseq blender and jar of red liquid on table endseq']\n",
      "000000357865 ['startseq bathroom with toilet and sink with cup on it endseq', 'startseq white toilet sitting next to white sink in bathroom endseq', 'startseq small bathroom has towels rugs and gray floor endseq', 'startseq an image of bathroom setting with toilet and counter endseq', 'startseq bathroom with toilet sink trash can towel rack and mirror endseq']\n",
      "000000131172 ['startseq group of people are sitting on bench outside endseq', 'startseq man sitting by woman and girl who are on their cell phones endseq', 'startseq black and white photograph of people on bench table endseq', 'startseq lot of people are sitting on the bench endseq', 'startseq man and his daughters sit on bench endseq']\n",
      "000000466024 ['startseq two bears play in water beside cement wall endseq', 'startseq two bears wrestle in pool of water endseq', 'startseq two bears are holding on to each other in the middle of water endseq', 'startseq two bears holding each other in water by wall endseq', 'startseq two big brown bears are wrestling in their swimming pool endseq']\n",
      "000000380636 ['startseq young person going down hill on snowboard endseq', 'startseq person in red jacket doing snowboard trick endseq', 'startseq person that is snow boading down snowy hill endseq', 'startseq person jumps in mid air on snowboard endseq', 'startseq person riding snowboard at high speed endseq']\n"
     ]
    }
   ],
   "source": [
    "## see the updated descriptions - note startseq and endseq IS PRESENT at both ends for all the descriptions\n",
    "i=0\n",
    "for k, v in descriptions_train.items():\n",
    "  i+=1\n",
    "  if i>5:\n",
    "    break\n",
    "  print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So now following data is created:\n",
    "\n",
    "### Image ecnodings from Inception-v3 pretrained on Imagenet\n",
    "### Train      data = 97000 images = img_encodings_train variable\n",
    "### Validation data = 3000  images = img_encodings_val   variable\n",
    "### Test       data = 5000  images = img_encodings_test  variable\n",
    "\n",
    "### Image names with their descriptions - after cleaning up\n",
    "### Train      data = 97000 images = descriptions_train variable : only this has the startseq and endseq tokens inserted at either end\n",
    "### Validation data = 3000  images = descriptions_val   variable\n",
    "### Test       data = 5000  images = descriptions_test  variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings data:\n",
      "len(img_encodings_train) = 97000\tlen(img_encodings_val) = 3000\tlen(img_encodings_test) = 5000\n",
      "Descriptions data:\n",
      "len(descriptions_train) = 97000\tlen(descriptions_val) = 3000\tlen(descriptions_test) = 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\tlen(img_encodings_val) = {len(img_encodings_val)}\\tlen(img_encodings_test) = {len(img_encodings_test)}\")\n",
    "print(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\tlen(descriptions_val) = {len(descriptions_val)}\\tlen(descriptions_test) = {len(descriptions_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPICKLE_FILENAME_descriptions_train_startend  = r\\'descriptions_train_97000_startend.pkl\\'\\n\\nwith open( OPDIR + PICKLE_FILENAME_descriptions_train_startend , \\'wb\\') as handle:\\n  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"Data for descriptions_train with start end seq pickled to :: {PICKLE_FILENAME_descriptions_train_startend}\")\\n'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pickle the descriptions_train which now has the start and end sequences inserted\n",
    "\"\"\"\n",
    "PICKLE_FILENAME_descriptions_train_startend  = r'descriptions_train_97000_startend.pkl'\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_descriptions_train_startend , 'wb') as handle:\n",
    "  pickle.dump(descriptions_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Data for descriptions_train with start end seq pickled to :: {PICKLE_FILENAME_descriptions_train_startend}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DURpFw2hpSX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfk-y5SjLKlo"
   },
   "source": [
    "## ALWAYS - Vocabulary building\n",
    "## Create the vocabulary based on the captions in the training descriptions with the start and end sequence tokens added at either end\n",
    "\n",
    "### First for all the words i.e. variable \"vocabulary\"\n",
    "### Then culled based on frequency > threshold i.e. variable \"vocab_threshold\"\n",
    "#### Later used the size of the wordtoix list (built using vocab_threshold), added 1 to make that equal to VOCAB_LEN for the model. The additional of one is to account for the 0 padding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "CRAYM1REIqEk",
    "outputId": "ce647ba0-2357-4fc4-e59f-75129a2ac8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size with all words = 24299\n"
     ]
    }
   ],
   "source": [
    "## at this stage the descriptions_train already has the start and end tokens added to it\n",
    "vocabulary = set()\n",
    "for key in descriptions_train.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions_train[key]]\n",
    "print(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "5lBBO5n0IqHG"
   },
   "outputs": [],
   "source": [
    "## This means we have some large number of unique words in the corpus - across all the descriptions for the 97000 train set images and including the start and end sequence words\n",
    "## \n",
    "## However, if we think about it, many of these words will occur very few times, \n",
    "## say 1, 2 or 3 times. Since we are creating a predictive model, we would not like \n",
    "## to have all the words present in our vocabulary but the words which are more \n",
    "## likely to occur or which are common. This helps the model become more robust \n",
    "## to outliers and make less mistakes.\n",
    "##\n",
    "## Hence we consider only those words which more often in the corpus than some chosen threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5aOjK8aqRgj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcfUg_3NqUT1"
   },
   "source": [
    "### Cull vocabulary based on frequency > threshold\n",
    "### Capturing new data into variable   vocab_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "gFP5GJaVIqJ-",
    "outputId": "cf5f68dd-e2da-48ca-827a-5fe7cb98a168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culled vocabulary to only retain words occurring more than threshold = 10 times.\n",
      "New vocab size , len(vocab_threshold) = 6759\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n",
    "\n",
    "all_desc_in_training_samples = []\n",
    "for key, val in descriptions_train.items():\n",
    "    for cap in val:\n",
    "        all_desc_in_training_samples.append(cap)\n",
    "\n",
    "MIN_WORD_COUNT_THRESHOLD = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for each_desc in all_desc_in_training_samples:\n",
    "    nsents += 1\n",
    "    for w in each_desc.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n",
    "\n",
    "print(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "-KdRyQzEK4Ke"
   },
   "outputs": [],
   "source": [
    "## Thus from the large number of unique words in original vocabulary,\n",
    "## the count of unique words in culled vocab vocab_threshold is substantially reduced.\n",
    "## This reduces the set of possible words for Decoder less, so model is robust, trains wells.\n",
    "## \n",
    "\n",
    "## So we now have two vocabulary variables:\n",
    "#### 1) vocabulary\n",
    "####    list of ALL unique words in our 97k training captions\n",
    "#### 2) vocab_threshold\n",
    "####    list of unique words occuring MORE than the threshold limit\n",
    "\n",
    "## NOTE: Eventually though we will be zero-padding, so the total words = len(vocab_threshold) + 1\n",
    "##       Additional one index for the 0-pad value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SIaSsAMK4QA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrKPGXxBlqk3"
   },
   "source": [
    "## ALWAYS - Data Preprocessing — Captions\n",
    "### 1) Build the word and index mapping dictionaries based on the reduced vocabulary.\n",
    "### 2) Find the maximum length of caption description\n",
    "###    NOTE: This is using the train images descriptions WITH the startseq and endseq word insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "zal9N9qbhQli"
   },
   "outputs": [],
   "source": [
    "### We want to predict the captions. So while training, the captions are the target variables (Y)\n",
    "### But the prediction of caption happens word by word, not in one go.\n",
    "### So encode each word into a fixed sized vector later (GloVe).\n",
    "### Here create the word-to-index and index-to-word dictionaries.\n",
    "###\n",
    "### We represent every unique word in the vocabulary by an integer (index).\n",
    "### From the culled vocab (high freq words including the zero), we have 6760 unique words\n",
    "###      in our corpus and thus each word will be represented by an integer index between 1 to 6760.\n",
    "###\n",
    "### Create two dictionaries: using the final vocabulary that model should output including the start and end sequence special tokens\n",
    "###        wordtoix[‘walking’] -> returns index of the word ‘walking’ e.g. it is 32\n",
    "###        ixtoword[i] -> returns the word whose index is ‘i’ e.g. ixtoword[32] will be 'walking'\n",
    "### Later, while building the RNN-Decoder model, its parameter VOCAB_SIZE is calculated as = len(wordtoix) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "wfpa8FOVhQof"
   },
   "outputs": [],
   "source": [
    "## NOTE: using the vocab_threhold\n",
    "ixtoword = dict()\n",
    "wordtoix = dict()\n",
    "idx = 1 ## index 1 is starting value as the 0 will be mapped to the 0-pad value\n",
    "for w in vocab_threshold:\n",
    "    wordtoix[w] = idx\n",
    "    ixtoword[idx] = w\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As wordtoix and ixtoword lists are required for the inference later - pickle them too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPICKLE_FILENAME_wordtoix  = r\\'wordtoix_train_97000.pkl\\'\\nPICKLE_FILENAME_ixtoword  = r\\'ixtoword_train_97000.pkl\\'\\n\\nwith open( OPDIR + PICKLE_FILENAME_wordtoix , \\'wb\\') as handle:\\n  pickle.dump(wordtoix, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"wordtoix pickled to :: {PICKLE_FILENAME_wordtoix}\")\\n\\nwith open( OPDIR + PICKLE_FILENAME_ixtoword , \\'wb\\') as handle:\\n  pickle.dump(ixtoword, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"ixtoword pickled to :: {PICKLE_FILENAME_ixtoword}\")\\n'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PICKLE_FILENAME_wordtoix  = r'wordtoix_train_97000.pkl'\n",
    "PICKLE_FILENAME_ixtoword  = r'ixtoword_train_97000.pkl'\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_wordtoix , 'wb') as handle:\n",
    "  pickle.dump(wordtoix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"wordtoix pickled to :: {PICKLE_FILENAME_wordtoix}\")\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_ixtoword , 'wb') as handle:\n",
    "  pickle.dump(ixtoword, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"ixtoword pickled to :: {PICKLE_FILENAME_ixtoword}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000044704': ['startseq jar of liquid next to blender full of liquid endseq',\n",
       "  'startseq jar full of red blended good next to blender filled with the same endseq',\n",
       "  'startseq red drink that was made in blender endseq',\n",
       "  'startseq blender and clear glass next to each other with red liquid in them endseq',\n",
       "  'startseq blender and jar of red liquid on table endseq'],\n",
       " '000000357865': ['startseq bathroom with toilet and sink with cup on it endseq',\n",
       "  'startseq white toilet sitting next to white sink in bathroom endseq',\n",
       "  'startseq small bathroom has towels rugs and gray floor endseq',\n",
       "  'startseq an image of bathroom setting with toilet and counter endseq',\n",
       "  'startseq bathroom with toilet sink trash can towel rack and mirror endseq']}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(descriptions_train.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 49\n"
     ]
    }
   ],
   "source": [
    "## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n",
    "\n",
    "## convert a dictionary of clean descriptions to a list of descriptions\n",
    "def extract_each_desc(_descriptions):\n",
    "    all_desc = list()\n",
    "    for key in _descriptions.keys():\n",
    "        [all_desc.append(d) for d in _descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "## find the longest description length\n",
    "def find_max_length_desc(_descriptions):\n",
    "    desc_sentences = extract_each_desc(_descriptions)\n",
    "    return max(len(d.split()) for d in desc_sentences)\n",
    "\n",
    "MAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\n",
    "print(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking what sentences had the max length and get an idea of how many sentences had what lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq jar of liquid next to blender full of liquid endseq',\n",
       " 'startseq jar full of red blended good next to blender filled with the same endseq',\n",
       " 'startseq red drink that was made in blender endseq',\n",
       " 'startseq blender and clear glass next to each other with red liquid in them endseq',\n",
       " 'startseq blender and jar of red liquid on table endseq',\n",
       " 'startseq bathroom with toilet and sink with cup on it endseq',\n",
       " 'startseq white toilet sitting next to white sink in bathroom endseq',\n",
       " 'startseq small bathroom has towels rugs and gray floor endseq']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cap for val in descriptions_train.values() for cap in val][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "doPdv9RAK4im"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101875    startseq two men holding tennis rackets up in ...\n",
       "Name: sent, dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=[cap for val in descriptions_train.values() for cap in val], columns=['sent'])\n",
    "df['caplen'] = df['sent'].str.split().apply(len)\n",
    "df['sent'][df['caplen'] == MAX_LENGTH_CAPTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     94555\n",
       "8     37858\n",
       "7      3533\n",
       "6         4\n",
       "5         1\n",
       "49        1\n",
       "46        4\n",
       "45        1\n",
       "44        7\n",
       "43        9\n",
       "42       13\n",
       "41       15\n",
       "40       14\n",
       "4         2\n",
       "39       16\n",
       "Name: sent, dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=[cap for val in descriptions_train.values() for cap in val], columns=['sent'])\n",
    "count = df['sent'].str.split().apply(len).value_counts()\n",
    "count.index = count.index.astype(str)\n",
    "count.sort_index(inplace=True, ascending=False)\n",
    "count.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ6W0IYDJVHZ"
   },
   "source": [
    "## ALWAYS - RELOAD FROM PICKLE FILE:  Reload the GloVe vectors related data structures\n",
    "\n",
    "### Sizes of reloaded pickled data should be =\n",
    "#### Size of embeddings_dict_glove200 = 20.971616 MB\n",
    "\n",
    "### Citing GloVe: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf] [bib]GloVe: Global Vectors for Word Representation. [pdf] [bib]\n",
    "### Link: More details on this here- https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "pSnmzft5JROf"
   },
   "outputs": [],
   "source": [
    "## Already downloaded the glove file and used it to create the embeddings dict and pickled it. Reloading now.\n",
    "###    This data is made available under the Public Domain Dedication and License v1.0 whose full text can be found at: http://www.opendatacommons.org/licenses/pddl/1.0/.\n",
    "###    Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n",
    "###    Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n",
    "###    Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n",
    "###    Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
    "\n",
    "### Am using glove.6B.zip on 14.09.2020\n",
    "### Then its glove.6B.200d.txt file for the 200 dimensional word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "mYWwBYpgJRRI",
    "outputId": "b849b063-ee52-47f4-c2fc-9790897c9cdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/embeddings-glove200/'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_EMBED_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "vfblkwZHJRTw",
    "outputId": "cbba178e-2e77-4121-a7fb-0eba5661b7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in this embeddings file = 400000\n",
      "Shape of random word = (200,)\n",
      "Shape of another random word = (200,)\n",
      "Size of embeddings_dict_glove200 = 20.971624 MB\n"
     ]
    }
   ],
   "source": [
    "## reload from picked file\n",
    "\n",
    "\n",
    "with open( IPDIR_EMBED_DICT + 'glove200_embeddings_dict_1.pkl', 'rb') as handle:\n",
    "    embeddings_dict_glove200 = pickle.load(handle)\n",
    "\n",
    "print(f\"Number of words in this embeddings file = {len(embeddings_dict_glove200)}\")\n",
    "print(f\"Shape of random word = {embeddings_dict_glove200['1917'].shape}\")\n",
    "print(f\"Shape of another random word = {embeddings_dict_glove200['sculpture'].shape}\")\n",
    "print(f\"Size of embeddings_dict_glove200 = {sys.getsizeof(embeddings_dict_glove200)/1000000} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POFL697yPHMV"
   },
   "source": [
    "### Now create the EMBEDDINGS MATRIX using the embeddings dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "Zi6FyTCPjHsd"
   },
   "outputs": [],
   "source": [
    "## Now, for all the unique words the model should possibly output as its vocabulary, we create an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "VORQIdfbh0G4",
    "outputId": "3a9fc04f-19a9-4b3b-c648-53cea7b636a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6759"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\n",
    "len(wordtoix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 247\n"
     ]
    }
   ],
   "source": [
    "## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\n",
    "print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "Q7o8RlR4PFkJ",
    "outputId": "d8b92bb4-cbe9-4955-f684-30356744f8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix = (6760, 200)\n"
     ]
    }
   ],
   "source": [
    "## since using GloVe-200, each word is represented as a 200 dimensional vector\n",
    "EMBEDDING_DIMS = 200\n",
    "## VOCAB_SIZE = the parameter to be used while defining the decoder model\n",
    "VOCAB_SIZE = len(wordtoix) + 1 # number of unique high freq words in the vocabulary + 1 (one index for the 0)\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIMS))\n",
    "for word, i in wordtoix.items():\n",
    "    #if i < max_words:\n",
    "    ## using .get(word) method and not [word] as second method throws key error, while first returns none.\n",
    "    ##        E.g.    print(embeddings_dict_glove200.get('sculture')) = None\n",
    "    ##            But print(embeddings_dict_glove200['sculture'}) will break with key error\n",
    "    embedding_vector = embeddings_dict_glove200.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "1zczwbWePFmn",
    "outputId": "b5c9c135-2849-45e3-d47b-4881d766a1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index position of word sculpture in the vocabulary = 2027\n",
      "\n",
      "Embedding vector for word 'sculpture' from GloVe directly = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n",
      "  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n",
      "  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n",
      "  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n",
      "  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n",
      "  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n",
      " -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n",
      "  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n",
      "  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n",
      "  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n",
      " -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n",
      " -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n",
      " -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n",
      " -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n",
      "  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n",
      " -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n",
      "  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n",
      " -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n",
      " -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n",
      " -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n",
      " -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n",
      " -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n",
      " -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n",
      " -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n",
      " -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n",
      "  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n",
      " -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n",
      " -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n",
      " -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n",
      " -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n",
      "  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n",
      "  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n",
      "  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n",
      " -0.1479    -0.62678  ]\n",
      "\n",
      "embedding_matrix entry for 'sculpture' = [ 0.24496   -0.43193   -0.45643   -0.19918    0.51172   -1.0377\n",
      "  0.25122    0.12462    0.10547   -0.97614    0.53084   -0.1065\n",
      "  0.37079    0.68693    0.87499   -0.20523   -0.3771     0.2687\n",
      "  0.83217   -0.31284    0.014147   1.2787     0.68525   -0.11444\n",
      "  0.16005    0.72932    0.39781   -0.69046   -0.04684   -0.16001\n",
      "  0.16502   -0.25699    0.13922   -0.12093   -0.048757   0.0030296\n",
      " -0.51352   -0.79619    0.78581   -0.42436   -0.47878   -0.28729\n",
      "  0.36649    0.27928    0.32741    0.37925    0.59473   -0.28681\n",
      "  0.066106   0.29147    0.059296   0.71001   -0.14208   -0.063589\n",
      "  0.076285  -0.027859   0.092648   0.16012    0.36895   -0.37574\n",
      " -0.27742    0.2595     0.060266   0.0081959 -0.4189     0.46089\n",
      " -0.65505   -0.13595   -0.47514   -0.5632    -0.2457    -0.85826\n",
      " -0.25973    0.66389    0.20883   -0.34182   -0.29798    0.10557\n",
      " -0.87879   -0.51638    0.7632    -0.4451     0.25784   -0.072907\n",
      "  0.32852   -0.084657   0.54618   -0.054893   1.1879    -0.82279\n",
      " -0.44927   -0.42007   -0.19797    0.28214   -0.22301   -0.64102\n",
      "  0.74922   -0.33223   -0.091449  -0.41277   -1.226     -0.16385\n",
      " -0.21405   -0.22783    0.40781   -0.80598   -0.16981   -0.03796\n",
      " -0.21637    0.071835  -0.10196   -0.32416   -0.34524   -0.44249\n",
      " -0.20595   -0.23764   -0.35326    0.15692   -0.63537    0.44373\n",
      " -0.49891   -0.33051   -0.11767   -0.017186   0.18475   -0.25121\n",
      " -0.029881   0.68022   -0.65274    0.15769    0.12651   -0.29099\n",
      " -0.13823   -0.26539   -0.73749    0.018799   0.25111   -0.0077853\n",
      " -0.63739   -0.11757    0.52995   -1.112     -0.45759   -0.65803\n",
      " -0.23451    0.35737    0.46083    0.028114   0.36177    0.1401\n",
      "  0.2457     0.43242   -0.076973  -0.4477     0.72404   -0.50541\n",
      " -0.17743    0.36071   -0.087228   0.31374    0.098276   0.1711\n",
      " -0.27123   -0.45765    0.40342    0.1337     0.089537  -0.1742\n",
      " -0.30914    0.47521    0.090243   0.16875    0.26361    0.45878\n",
      " -0.68736   -0.29789    0.66853   -0.20317    0.16407    0.085847\n",
      "  0.50354   -0.11787    0.0096102 -0.93856    0.22984   -0.40761\n",
      "  0.17154    0.16755    0.46072   -0.047126  -0.20012    0.32569\n",
      "  0.8945    -0.62629    0.19179   -0.53753   -0.65715    0.13078\n",
      " -0.1479    -0.62678  ]\n"
     ]
    }
   ],
   "source": [
    "## e.g. of a word PRESENT the GoVe200 data file\n",
    "mytestword = 'sculpture'\n",
    "print(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\n",
    "print(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\n",
    "print(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")\n",
    "## it should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "pnQW2VwOPFo_",
    "outputId": "4d9bcb11-17c4-47d9-daf5-e0610ed6d8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index position of word sculture in the vocabulary = None\n",
      "\n",
      "Embedding vector for word 'sculpture' from GloVe directly = None\n",
      "\n",
      "embedding_matrix entry for 'sculture' = None\n"
     ]
    }
   ],
   "source": [
    "## e.g. of a word NOT PRESENT the GoVe200 data file\n",
    "## answer None in all cases since the word is not in the vocabulary itself the other two will throw key error\n",
    "mytestword = 'sculture'\n",
    "print(f\"Index position of word {mytestword} in the vocabulary = {wordtoix.get(mytestword)}\")\n",
    "print(f\"\\nEmbedding vector for word 'sculpture' from GloVe directly = {embeddings_dict_glove200.get(mytestword)}\")\n",
    "print(f\"\\nembedding_matrix entry for '{mytestword}' = {embeddings_dict_glove200.get(mytestword)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_dict_glove200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6760, 200)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPICKLE_FILENAME_embedding_matrix  = r\\'embedding_matrix_6758_200.pkl\\'\\n\\nwith open( OPDIR + PICKLE_FILENAME_embedding_matrix , \\'wb\\') as handle:\\n  pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(f\"embedding_matrix with shape {embedding_matrix.shape} pickled to :: {PICKLE_FILENAME_embedding_matrix}\")\\n'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pickle the embedding matrix just created\n",
    "\"\"\"\n",
    "PICKLE_FILENAME_embedding_matrix  = r'embedding_matrix_6758_200.pkl'\n",
    "\n",
    "with open( OPDIR + PICKLE_FILENAME_embedding_matrix , 'wb') as handle:\n",
    "  pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"embedding_matrix with shape {embedding_matrix.shape} pickled to :: {PICKLE_FILENAME_embedding_matrix}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJgRpN5MjJD3"
   },
   "source": [
    "## Data Preparation using Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "7v8nlMCHYock"
   },
   "outputs": [],
   "source": [
    "## Hereafter, I will try to explain the remaining steps by taking a sample example as follows:\n",
    "## Consider we have 3 images and their 3 corresponding captions as follows:\n",
    "## \n",
    "## (Train image 1) Caption -> The black cat sat on grass\n",
    "## (Train image 2) Caption -> The white cat is walking on road\n",
    "## (Test image) Caption -> The black cat is walking on grass\n",
    "## \n",
    "## Now, let’s say we use the first two images and their captions to train the model and the third image to test our model.\n",
    "## Now the questions that will be answered are: how do we frame this as a supervised learning problem?, what does the data matrix look like? how many data points do we have?, etc.\n",
    "## First we need to convert both the images to their corresponding 2048 length feature vector as discussed above. Let “Image_1” and “Image_2” be the feature vectors of the first two images respectively\n",
    "## Secondly, let’s build the vocabulary for the first two (train) captions by adding the two tokens “startseq” and “endseq” in both of them: (Assume we have already performed the basic cleaning steps)\n",
    "## \n",
    "## Caption_1 -> “startseq the black cat sat on grass endseq”\n",
    "## Caption_2 -> “startseq the white cat is walking on road endseq”\n",
    "## \n",
    "## vocab = {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n",
    "## \n",
    "## Let’s give an index to each word in the vocabulary:\n",
    "## black -1, cat -2, endseq -3, grass -4, is -5, on -6, road -7, sat -8, startseq -9, the -10, walking -11, white -12\n",
    "## \n",
    "## Now let’s try to frame it as a supervised learning problem where we have a set of data points D = {Xi, Yi}, where Xi is the feature vector of data point ‘i’ and Yi is the corresponding target variable.\n",
    "## \n",
    "## Let’s take the first image vector Image_1 and its corresponding caption “startseq the black cat sat on grass endseq”. Recall that, Image vector is the input and the caption is what we need to predict. But the way we predict the caption is as follows:\n",
    "## For the first time, we provide the image vector and the first word as input and try to predict the second word, i.e.:\n",
    "## Input = Image_1 + ‘startseq’; Output = ‘the’\n",
    "## Then we provide image vector and the first two words as input and try to predict the third word, i.e.:\n",
    "## Input = Image_1 + ‘startseq the’; Output = ‘cat’\n",
    "## And so on . . .\n",
    "## \n",
    "## Thus, we can summarize the data matrix for one image and its corresponding caption as follows:\n",
    "## Step 1 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq\" :: Target Word = \"the\"\n",
    "## Step 2 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the\" :: Target Word = \"black\"\n",
    "## Step 3 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black\" :: Target Word = \"cat\"\n",
    "## Step 4 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat\" :: Target Word = \"sat\"\n",
    "## Step 5 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat\" :: Target Word = \"on\"\n",
    "## Step 6 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on\" :: Target Word = \"grass\"\n",
    "## Step 7 :: image feature = FeatVecImg1 :: Partial Caption = \"startseq the black cat sat on grass\" :: Target Word = \"endseq\"\n",
    "## \n",
    "## It must be noted that, one image+caption is not a single data point but are multiple data points depending on the length of the caption.\n",
    "## All the above 7 data points together constitute the full data for one image and its caption!!!!\n",
    "## Similarly for second images, there will be multiple steps together that consitute its full data point.\n",
    "## \n",
    "## We must now understand that in every data point, it’s not just the image which goes as input to the system, but also, a partial caption which helps to predict the next word in the sequence.\n",
    "## Since we are processing sequences, we will employ a Recurrent Neural Network to read these partial captions (more on this later).\n",
    "## However, we have already discussed that we are not going to pass the actual English text of the caption, rather we are going to pass the sequence of indices where each index represents a unique word.\n",
    "## \n",
    "## Since we have already created an index for each word, let’s now replace the words with their indices and understand how the data matrix will look like:\n",
    "## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"9\" :: Target Word = \"10\"\n",
    "## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10\" :: Target Word = \"1\"\n",
    "## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1\" :: Target Word = \"2\"\n",
    "## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2\" :: Target Word = \"8\"\n",
    "## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8\" :: Target Word = \"6\"\n",
    "## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6\" :: Target Word = \"4\"\n",
    "## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"9 10 1 2 8 6 4\" :: Target Word = \"3\"\n",
    "## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"9\" :: Target Word = \"10\"\n",
    "## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"9 10\" :: Target Word = \"12\"\n",
    "## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12\" :: Target Word = \"2\"\n",
    "## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2\" :: Target Word = \"5\"\n",
    "## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5\" :: Target Word = \"11\"\n",
    "## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11\" :: Target Word = \"6\"\n",
    "## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6\" :: Target Word = \"7\"\n",
    "## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"9 10 12 2 5 11 6 3\" :: Target Word = \"3\"\n",
    "## \n",
    "## Since we would be doing batch processing (explained later), we need to make sure that each sequence is of equal length. Hence we need to append 0’s (zero padding) at the end of each sequence. But how many zeros should we append in each sequence?\n",
    "## Well, this is the reason we had calculated the maximum length of a caption. So we will append those many number of zeros which will lead to every sequence having a length = maximum length of caption.\n",
    "## \n",
    "## Step 1  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n",
    "## Step 2  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"1\"\n",
    "## Step 3  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 0 0 ...]\" :: Target Word = \"2\"\n",
    "## Step 4  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 0 0 ...]\" :: Target Word = \"8\"\n",
    "## Step 5  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 0 0 ...]\" :: Target Word = \"6\"\n",
    "## Step 6  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 0 0 ...]\" :: Target Word = \"4\"\n",
    "## Step 7  :: image feature = FeatVecImg1 :: Partial Caption = \"[9 10 1 2 8 6 4 0 0 ...]\" :: Target Word = \"3\"\n",
    "## Step 8  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 0 0 ...]\" :: Target Word = \"10\"\n",
    "## Step 9  :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 0 0 ...]\" :: Target Word = \"12\"\n",
    "## Step 10 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 0 0 ...]\" :: Target Word = \"2\"\n",
    "## Step 11 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 0 0 ...]\" :: Target Word = \"5\"\n",
    "## Step 12 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 0 0 ...]\" :: Target Word = \"11\"\n",
    "## Step 13 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 0 0 ...]\" :: Target Word = \"6\"\n",
    "## Step 14 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 0 0 ...]\" :: Target Word = \"7\"\n",
    "## Step 15 :: image feature = FeatVecImg2 :: Partial Caption = \"[9 10 12 2 5 11 6 3 0 0 ...]\" :: Target Word = \"3\"\n",
    "## Appended adequete 0's to each partial caption to make its length = max length\n",
    "## \n",
    "## \n",
    "## \n",
    "## Need for a Data Generator:\n",
    "## In the above example, I have only considered 2 images and captions which have lead to 15 data points.\n",
    "## However, in our actual training dataset we have 6000 images, each having 5 captions. This makes a total of 30000 images and captions.\n",
    "## Even if we assume that each caption on an average is just 7 words long, it will lead to a total of 30000*7 i.e. 210000 data points.\n",
    "## \n",
    "## Compute the size of the data matrix:\n",
    "## \n",
    "## \n",
    "##    img_vector = 2048        partial caption (length = max caption length)\n",
    "## ---------------------------------------------------------------------------\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## -                       -                                                 -\n",
    "## ---------------------------------------------------------------------------\n",
    "## Say above matrix has n rows, m columns, so Size of the data matrix = n*m\n",
    "## n-> number of data points (assumed as 210000)  (6000 images * 5 captions per image * 7 words average legnth of each caption)\n",
    "## m-> length of each data point\n",
    "##     = 2048 + length of partial caption (say something)\n",
    "##     = 2048 + something\n",
    "## \n",
    "## Now this \"something\"  NOTE EQUAL to the max length of caption!\n",
    "## Every word (or index) will be mapped (embedded) to higher dimensional space through one of the word embedding techniques.\n",
    "## As we using GloVe-200, each embedding has 200 floats representing each number.\n",
    "## \n",
    "## So with each caption sentence consisting of max length caption size word-indexes, each word represented by 200 dimensional value:\n",
    "##    Assuming max lenght of caption = 45\n",
    "##    means \"something\" = 2048 + ( 45 * 200 ) = 2048 + 9000 = 11048 float values to represent each sentence\n",
    "## \n",
    "## Therefore, size of data matrix = m*n = 210000 * 11048 = xxx float values!!!\n",
    "## Assuming float takes 2 bytes (very conservative), that still means xxx * 2 = xxx GB\n",
    "## \n",
    "## This is pretty huge requirement and even if we are able to manage to load this much data into the RAM, it will make the system very slow.\n",
    "## For this reason we use data generators a lot in Deep Learning. Data Generators are a functionality which is natively implemented in Python. The ImageDataGenerator class provided by the Keras API is nothing but an implementation of generator function in Python.\n",
    "## \n",
    "## So how does using a generator function solve this problem?\n",
    "## If you know the basics of Deep Learning, then you must know that to train a model on a particular dataset, we use some version of Stochastic Gradient Descent (SGD) like Adam, Rmsprop, Adagrad, etc.\n",
    "## With SGD, we do not calculate the loss on the entire data set to update the gradients. Rather in every iteration, we calculate the loss on a batch of data points (typically 64, 128, 256, etc.) to update the gradients.\n",
    "## \n",
    "## This means that we do not require to store the entire dataset in the memory at once. Even if we have the current batch of points in the memory, it is sufficient for our purpose.\n",
    "## A generator function in Python is used exactly for this purpose. It’s like an iterator which resumes the functionality from the point it left the last time it was called.\n",
    "## To understand more about Generators, please read here (https://wiki.python.org/moin/Generators).\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "dQkTM-yDjHkU"
   },
   "outputs": [],
   "source": [
    "# data generator, use during the call to model.fit_generator() to create batchwise data\n",
    "def data_generator_1(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch, _vocab_size):\n",
    "    X1, X2, y = [] , [] , []  ## empty lists to populate the input and target data for a bath\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in _descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the encoded features of image\n",
    "            img_feat = _imgs_features_arr[ key ] ## keys in the image encodings dict and descriptions dict use image_filename without the .jpg\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [_wordtoix[word] for word in desc.split(' ') if word in _wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=_max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = keras.utils.to_categorical([out_seq], num_classes=_vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(img_feat)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n == _images_per_batch:\n",
    "                ## ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'lstm_1/lstm_cell_1/kernel:0', 'lstm_1/lstm_cell_1/recurrent_kernel:0', 'lstm_1/lstm_cell_1/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0'].\n",
    "                #yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "                yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGg7LIE5bHfV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Already ran the above steps earlier and pickled all the variables created based on a certain split of Train and Validation datasets 97k:3k.\n",
    "## Overwriting all the variables by reloading from the saved pickled files.\n",
    "## Run necessary code again to check all good to proceed with Training phase:\n",
    "## Outcome expected:\n",
    "## 1) unique words in original vocabulary = 24323\n",
    "## 2) unique words in culled vocab vocab_threshold = 6757\n",
    "##    Thus, VOCAB_SIZE = 6757 + 1 = 6758\n",
    "## 3) Embedding matrix shape should be (6758,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/thesis-imgcap-run2-deterministic-info/'"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_DETERMINISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_matrix_6758_200.pkl',\n",
       " 'val_imgs_keys_3000_randomSplit444.pkl',\n",
       " 'ixtoword_train_97000.pkl',\n",
       " 'img_encodings_val_3000.pkl',\n",
       " 'descriptions_train_97000_startend.pkl',\n",
       " 'train_imgs_keys_97000_randomSplit444.pkl',\n",
       " 'descriptions_val_3000.pkl',\n",
       " 'descriptions_train_and_val.pkl',\n",
       " 'descriptions_train_97000.pkl',\n",
       " 'descriptions_test.pkl',\n",
       " 'img_encodings_train_97000.pkl',\n",
       " 'wordtoix_train_97000.pkl']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_DETERMINISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload all the files to make it deterministic\n",
    "\n",
    "\n",
    "# variable = img_encodings_train\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_train_97000.pkl', 'rb') as handle:\n",
    "    img_encodings_train = pickle.load(handle)\n",
    "\n",
    "# variable = img_encodings_val\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_val_3000.pkl', 'rb') as handle:\n",
    "    img_encodings_val = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_train\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_train_97000_startend.pkl', 'rb') as handle:\n",
    "    descriptions_train = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_val\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_val_3000.pkl', 'rb') as handle:\n",
    "    descriptions_val = pickle.load(handle)\n",
    "\n",
    "# variable = train_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'train_imgs_keys_97000_randomSplit444.pkl', 'rb') as handle:\n",
    "    train_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = val_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'val_imgs_keys_3000_randomSplit444.pkl', 'rb') as handle:\n",
    "    val_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = embedding_matrix\n",
    "with open(IPDIR_DETERMINISTIC + 'embedding_matrix_6758_200.pkl', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "# variable = ixtoword\n",
    "with open(IPDIR_DETERMINISTIC + 'ixtoword_train_97000.pkl', 'rb') as handle:\n",
    "    ixtoword = pickle.load(handle)\n",
    "\n",
    "# variable = wordtoix\n",
    "with open(IPDIR_DETERMINISTIC + 'wordtoix_train_97000.pkl', 'rb') as handle:\n",
    "    wordtoix = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the reloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings data:\n",
      "len(img_encodings_train) = 97000\t\tlen(img_encodings_val) = 3000\n",
      "Descriptions data:\n",
      "len(descriptions_train) = 97000\t\tlen(descriptions_val) = 3000\n",
      "\n",
      "CHECK : reloaded values = 97k for Train , 3k for Validation\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\t\\tlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\t\\tlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "print(f\"\\nCHECK : reloaded values = 97k for Train , 3k for Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size with all words = 24323\n",
      "\n",
      "CHECK : reloaded value = 24323\n"
     ]
    }
   ],
   "source": [
    "## at this stage the descriptions_train already has the start and end tokens added to it\n",
    "vocabulary = set()\n",
    "for key in descriptions_train.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions_train[key]]\n",
    "print(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 24323\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culled vocabulary to only retain words occurring more than threshold = 10 times.\n",
      "New vocab size , len(vocab_threshold) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n",
    "\n",
    "all_desc_in_training_samples = []\n",
    "for key, val in descriptions_train.items():\n",
    "    for cap in val:\n",
    "        all_desc_in_training_samples.append(cap)\n",
    "\n",
    "MIN_WORD_COUNT_THRESHOLD = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for each_desc in all_desc_in_training_samples:\n",
    "    nsents += 1\n",
    "    for w in each_desc.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n",
    "\n",
    "print(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 49\n",
      "\n",
      "CHECK : reloaded value = 49\n"
     ]
    }
   ],
   "source": [
    "## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n",
    "\n",
    "## convert a dictionary of clean descriptions to a list of descriptions\n",
    "def extract_each_desc(_descriptions):\n",
    "    all_desc = list()\n",
    "    for key in _descriptions.keys():\n",
    "        [all_desc.append(d) for d in _descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "## find the longest description length\n",
    "def find_max_length_desc(_descriptions):\n",
    "    desc_sentences = extract_each_desc(_descriptions)\n",
    "    return max(len(d.split()) for d in desc_sentences)\n",
    "\n",
    "MAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\n",
    "print(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"\\nCHECK : reloaded value = 49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(wordtoix) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n",
      "\n",
      "\n",
      "Set the   VOCAB_SIZE = len(wordtoix) + 1 = 6758\n"
     ]
    }
   ],
   "source": [
    "## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")\n",
    "\n",
    "VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"\\n\\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 526\n"
     ]
    }
   ],
   "source": [
    "## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\n",
    "print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix = (6758, 200)\n",
      "\n",
      "CHECK : reloaded shape = 6758,200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")\n",
    "print(f\"\\nCHECK : reloaded shape = 6758,200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD be:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "## Recheck these parameters that will define the Decoder architecture ::: EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n",
    "print(f\"SHOULD be:\\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\")\n",
    "print(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTxczXFBRp85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGzQr4tiiWDc"
   },
   "source": [
    "## ALWAYS - Define the RNN Decoder model\n",
    "## Use the GloVe embeddings matrix values to update layer weights and freeze ONLY those weights\n",
    "\n",
    "\n",
    "### About Keras Emebedding layer\n",
    "### \n",
    "#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "######tf.keras.layers.Embedding(\n",
    "######    input_dim, output_dim,\n",
    "######    embeddings_initializer=\"uniform\",\n",
    "######    embeddings_regularizer=None,\n",
    "######    activity_regularizer=None,\n",
    "######    embeddings_constraint=None,\n",
    "######    mask_zero=False,\n",
    "######    input_length=None,\n",
    "######    **kwargs\n",
    "######)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "_EVJLMeGRp_Y"
   },
   "outputs": [],
   "source": [
    "## Decoder Model defining\n",
    "\n",
    "## parameters to define model - all these three populated earlier\n",
    "## EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n",
    "\n",
    "inputs1 = keras.Input(shape=(2048,))\n",
    "fe1 = keras.layers.Dropout(0.5)(inputs1)\n",
    "fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model\n",
    "inputs2 = keras.Input(shape=(MAX_LENGTH_CAPTION,))\n",
    "se1 = keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIMS, mask_zero=True)(inputs2)\n",
    "se2 = keras.layers.Dropout(0.5)(se1)\n",
    "se3 = keras.layers.LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = keras.layers.add([fe2, se3])\n",
    "decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "outputs = keras.layers.Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model_RNN_decoder = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "OHWDqfjiRqB4",
    "outputId": "e55a7148-8854-431d-c667-785f81063188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,146,710\n",
      "Trainable params: 4,146,710\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() BEFORE FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params\n",
    "model_RNN_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() BEFORE FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params\n",
    "\n",
    "#Model: \"functional_3\"\n",
    "#__________________________________________________________________________________________________\n",
    "#Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "#==================================================================================================\n",
    "#input_4 (InputLayer)            [(None, 49)]         0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
    "#                                                                 lstm_1[0][0]                     \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n",
    "#==================================================================================================\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 4,146,710\n",
    "#Non-trainable params: 0\n",
    "#__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../working/'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rFiBoJyRqHg",
    "outputId": "cf620edb-6810-45b9-fec1-5d9d637c9442"
   },
   "outputs": [],
   "source": [
    "#RNN_decoder_model_plot_filename = r'RNN_decoder_model_plot_97kTrain_1.jpg'\n",
    "#tf.keras.utils.plot_model(model_RNN_decoder, to_file= OPDIR + RNN_decoder_model_plot_filename, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETH9PxyGxpt-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMKYtXCQxexx"
   },
   "source": [
    "### ALWAYS - set weights using the GloVe embedding matrix and freeze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "6c0oR60fRqJZ",
    "outputId": "9dbf8c22-edfb-428b-e172-7c7b03bc60c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6758, 200)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## contain the 200 dim representation of our vocab_threshold based output words and the 0-pad value which was left as 0's\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "naYfB0FjwocE"
   },
   "outputs": [],
   "source": [
    "model_RNN_decoder.layers[2].set_weights([embedding_matrix])\n",
    "model_RNN_decoder.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4HreAB0woha"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMFFw_g5xy7r"
   },
   "source": [
    "### DO ONCE - RNN Decoder summary and plot\n",
    "#### AFTER loading GloVe embeddings and Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "K_mLYJjsRqMO",
    "outputId": "809ebcdd-f972-4f47-f60f-a207404054a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,146,710\n",
      "Trainable params: 2,795,110\n",
      "Non-trainable params: 1,351,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "model_RNN_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "\n",
    "#Model: \"functional_3\"\n",
    "#__________________________________________________________________________________________________\n",
    "#Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "#==================================================================================================\n",
    "#input_4 (InputLayer)            [(None, 49)]         0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#input_3 (InputLayer)            [(None, 2048)]       0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#embedding_1 (Embedding)         (None, 49, 200)      1351600     input_4[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_2 (Dropout)             (None, 2048)         0           input_3[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_3 (Dropout)             (None, 49, 200)      0           embedding_1[0][0]                \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_3 (Dense)                 (None, 256)          524544      dropout_2[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#lstm_1 (LSTM)                   (None, 256)          467968      dropout_3[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
    "#                                                                 lstm_1[0][0]                     \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_5 (Dense)                 (None, 6758)         1736806     dense_4[0][0]                    \n",
    "#==================================================================================================\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 2,795,110\n",
    "#Non-trainable params: 1,351,600\n",
    "#__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lX2xaz1xxrF",
    "outputId": "12854ec9-d37b-4a85-b364-ed191b6613ee"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model_RNN_decoder, to_file=OPDIR+'RNN_decoder_model_plot_AFTER_GLOVE_1.jpg', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxU_TgOGxxt6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNo79p0Az37z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB81Ohfoz4lL"
   },
   "source": [
    "### ALWAYS - Compile RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "id": "CF5Dw9brxxwh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default LR without explicity setting so far = 0.0010000000474974513\n",
      "After setting LR as 0.5 = 0.5\n"
     ]
    }
   ],
   "source": [
    "## setup the optimizer and compile\n",
    "\n",
    "## see the default LR used, set some dummy to check it works\n",
    "optimizer_adam = tf.keras.optimizers.Adam()  ## if nothing specified the default LR = 0.001\n",
    "model_RNN_decoder.compile(loss='categorical_crossentropy', optimizer=optimizer_adam)\n",
    "print(f\"Default LR without explicity setting so far = {model_RNN_decoder.optimizer.learning_rate.numpy()}\")\n",
    "\n",
    "optimizer_adam.learning_rate.assign(0.5)\n",
    "print(f\"After setting LR as 0.5 = {model_RNN_decoder.optimizer.learning_rate.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIqp5fxB0ldP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdJW1J2YmrxO"
   },
   "source": [
    "## ALWAYS - Train the RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../working/weights_out/'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR_WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir( OPDIR_WEIGHTS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "EXJ2Yjuz05wG",
    "outputId": "c0ab13d8-b8d2-4450-ef87-589ebd49b46c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEncodings data:\n",
      "len(img_encodings_train) = 97000\tlen(img_encodings_val) = 3000\n",
      "\tDescriptions data:\n",
      "len(descriptions_train) = 97000\tlen(descriptions_val) = 3000\n",
      "\n",
      "No Validation Set being used for now.\n",
      "\n",
      "len(wordtoix) = 6757\n",
      "VOCAB_SIZE = 6758\n",
      "EMBEDDING_DIMS = 200\n",
      "embedding_matrix Shape = (6758, 200)\n",
      "MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "## values setup earlier - for reference\n",
    "\n",
    "print(f\"\\tEncodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\nlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"\\tDescriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\nlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "\n",
    "print(f\"\\nNo Validation Set being used for now.\\n\")\n",
    "\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"VOCAB_SIZE = {VOCAB_SIZE}\")\n",
    "print(f\"EMBEDDING_DIMS = {EMBEDDING_DIMS}\")\n",
    "print(f\"embedding_matrix Shape = {embedding_matrix.shape}\")\n",
    "print(f\"MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RUN_NUMBER = 2 ## large training data - 97k train\n",
    "MODEL_RUN_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5Dx5QnMmrJ1",
    "outputId": "ec46884f-bb34-4244-8b6f-ff525a537581"
   },
   "outputs": [],
   "source": [
    "## For reference: def data_generator(_descriptions, _imgs_features_arr, _wordtoix, _max_length, _images_per_batch):\n",
    "## Notes:\n",
    "##      1) The data generator function needs exactly the CORRECT descriptions for the training (or validation data). It takes the keys from the descriptions and then\n",
    "##         searches the image encodings array. So it is fine even if we use an image encodings with more images.\n",
    "##         But we already made the dedicated arrays for Train and Validation datasets, so use them explicity!!!\n",
    "\n",
    "########### Phase 1 #########\n",
    "\n",
    "print(f\"\\n\\nMODEL {MODEL_RUN_NUMBER} :: Training Phase 1 started at :: {datetime.datetime.now().strftime('%H:%M:%S')}\\n\")\n",
    "\n",
    "LR_1 = 0.0005\n",
    "BATCH_SIZE_1 = 128   ## how many images per batch\n",
    "N_EPOCHS_1 = 2\n",
    "STEPS_PER_EPOCH_1 = len(descriptions_train) // BATCH_SIZE_1\n",
    "print(f\"\\nPhase 1 parameters:\")\n",
    "print(f\"STEPS_PER_EPOCH_1 = {STEPS_PER_EPOCH_1}\")\n",
    "print(f\"BATCH_SIZE_1 = {BATCH_SIZE_1}\")\n",
    "print(f\"N_EPOCHS_1 = {N_EPOCHS_1}\")\n",
    "\n",
    "optimizer_adam.learning_rate.assign(LR_1)\n",
    "\n",
    "for i in range(N_EPOCHS_1):\n",
    "    print(f\"\\nEpoch {i+1} started at {datetime.datetime.now().strftime('%H:%M:%S')}\\nLR used = {model_RNN_decoder.optimizer.learning_rate.numpy()} \\n\")\n",
    "    generator_1 = data_generator_1(descriptions_train, img_encodings_train, wordtoix, MAX_LENGTH_CAPTION, BATCH_SIZE_1, VOCAB_SIZE)\n",
    "    model_RNN_decoder.fit_generator(generator_1, epochs=1, steps_per_epoch=STEPS_PER_EPOCH_1, verbose=1)\n",
    "    \n",
    "    model_RNN_decoder.save_weights( OPDIR_WEIGHTS + 'Decoder_Run_' + str(MODEL_RUN_NUMBER) + '_Wt_ep_' + str(i+1) + '.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Console output after training:\n",
    "\n",
    "MODEL 2 :: Training Phase 1 started at :: 17:15:11\n",
    "\n",
    "\n",
    "Phase 1 parameters:\n",
    "STEPS_PER_EPOCH_1 = 757\n",
    "BATCH_SIZE_1 = 128\n",
    "N_EPOCHS_1 = 2\n",
    "\n",
    "Epoch 1 started at 17:15:11\n",
    "LR used = 0.0005000000237487257 \n",
    "\n",
    "757/757 [==============================] - 7705s 10s/step - loss: 4.3657\n",
    "\n",
    "Epoch 2 started at 19:23:55\n",
    "LR used = 0.0005000000237487257 \n",
    "\n",
    "757/757 [==============================] - 7793s 10s/step - loss: 3.4819\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xc08wQoRqZx"
   },
   "outputs": [],
   "source": [
    "#model_reset_states\n",
    "#lr optimizer\n",
    "#tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D3b7izSIqUb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
