{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-rBS0Z-E4p8"
   },
   "source": [
    "# Compare the model caption inference against the ground truth captions from annotations file.\n",
    "# Trained model on 97k of some 100k images of Coco_Train2017 dataset. Balance 3k kept aside as the Validation Set. Using all the 5k images of Coco_Val2017 dataset as the Test dataset.\n",
    "\n",
    "## Encoder: Pre-trained Google Inception-v3 trained on Imagenet imported directly from Keras\n",
    "## Decoder: Trained with the 97k data and their captions from annotations file.\n",
    "\n",
    "## AVAILABLE data :\n",
    "### Coco_Train2017     = has 118287 images\n",
    "### Coco_Val2017       = has 5000   images\n",
    "### Combined total     = 123287     images\n",
    "\n",
    "## USED data :\n",
    "### Coco_Train2017     = 100000  images\n",
    "### Coco_Val2017       = 5000    images\n",
    "### Combined total     = 105000  images\n",
    "\n",
    "### Using only the first 100k images of Train2017 + all 5k images of Val2017\n",
    "### Thus total data available for training = 100k + 5k = 105k\n",
    "### Details of split of data:\n",
    "### Training   data = 97000 images from Coco_Train2017\n",
    "### Validation data = 3000  images from Coco_Train2017\n",
    "### Test       data = 5000  images from Coco_Val2017\n",
    "\n",
    "### Note: 1) Each image will have multiple captions (up to 5 as some may be discarded)\n",
    "###       2) Not using the Coco_Test2017 dataset at all as it has no annotations json file which has the captions.\n",
    "\n",
    "## Using this LINK: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "## Image captioning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "buTVBtveEpqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "#import itertools\n",
    "import PIL\n",
    "import PIL.Image\n",
    "\n",
    "## import numpy as np\n",
    "## from numpy import array\n",
    "## import pandas as pd\n",
    "## import matplotlib.pyplot as plt\n",
    "## %matplotlib inline\n",
    "## import string\n",
    "## import os\n",
    "## from PIL import Image\n",
    "## import glob\n",
    "## from pickle import dump, load\n",
    "## from time import time\n",
    "## from keras.preprocessing import sequence\n",
    "## from keras.models import Sequential\n",
    "## from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "## from keras.optimizers import Adam, RMSprop\n",
    "## from keras.layers.wrappers import Bidirectional\n",
    "## from keras.layers.merge import add\n",
    "## from keras.applications.inception_v3 import InceptionV3\n",
    "## from keras.preprocessing import image\n",
    "## from keras.models import Model\n",
    "## from keras import Input, layers\n",
    "## from keras import optimizers\n",
    "## from keras.applications.inception_v3 import preprocess_input\n",
    "## from keras.preprocessing.text import Tokenizer\n",
    "## from keras.preprocessing.sequence import pad_sequences\n",
    "## from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ebw9HVoxEptM",
    "outputId": "84656bc7-27ad-46cd-e2fe-bab33c5c0032"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0b0aXNyCEd3p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## Kaggle versions\\n\\n## Weights from training till now\\nOPDIR = r'../working/'\\n\\n## Images locations\\nIPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\\nIPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\\n\\n## Annotations json file location from where to pick up the captions\\nIPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\\n\\n## Bottleneck CNN Encoder output for all the images to be used in training\\nIPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\\n\\n## to make deterministic - saved all the variables based on the data used to train during phase 1\\n## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\\nIPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\\n\\n## Weights from previous stage of training\\nIPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run2-weights-in/'\\n\\n\\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\\n\\n## Google drive versions\\n#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\\n#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\\n#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\\n#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LAPTOP versions\n",
    "\n",
    "## Weights from training till now\n",
    "#OPDIR = r'../working/'\n",
    "\n",
    "## Images locations\n",
    "#IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "IPDIR_IMGS_COCO_VAL = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Data/COCO_val2017_5k/val2017/'\n",
    "\n",
    "## Annotations json file location from where to pick up the captions\n",
    "IPDIR_ANNO = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/coco_2017_annotations/'\n",
    "\n",
    "## Bottleneck CNN Encoder output for all the images of TRAIN , VALIDATION and TEST DATASETS\n",
    "#IPDIR_IMG_ENCODINGS = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_ImgEncodings_1/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on the data used to train during phase 1\n",
    "## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\n",
    "IPDIR_DETERMINISTIC = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Run2_Deterministic_Info/'\n",
    "\n",
    "## Weights from previous stage of training\n",
    "IPDIR_WEIGHTS_IN = r'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/'\n",
    "\n",
    "\"\"\"\n",
    "## Kaggle versions\n",
    "\n",
    "## Weights from training till now\n",
    "OPDIR = r'../working/'\n",
    "\n",
    "## Images locations\n",
    "IPDIR_IMGS_COCO_TRAIN = r'../input/coco-2017-dataset/coco2017/train2017/'\n",
    "IPDIR_IMGS_COCO_VAL = r'../input/coco-2017-dataset/coco2017/val2017/'\n",
    "\n",
    "## Annotations json file location from where to pick up the captions\n",
    "IPDIR_ANNO = r'../input/coco-2017-dataset/coco2017/annotations/'\n",
    "\n",
    "## Bottleneck CNN Encoder output for all the images to be used in training\n",
    "IPDIR_IMG_ENCODINGS = r'../input/thesis-imgcap-imgencodings-1/'\n",
    "\n",
    "## to make deterministic - saved all the variables based on the data used to train during phase 1\n",
    "## ALL SUBSEQUENT PHASES WILL RELOAD FROM THE DATA IN THIS LOCATION\n",
    "IPDIR_DETERMINISTIC = '../input/thesis-imgcap-run2-deterministic-info/'\n",
    "\n",
    "## Weights from previous stage of training\n",
    "IPDIR_WEIGHTS_IN = r'../input/imgcap-kagg-run2-weights-in/'\n",
    "\n",
    "\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/' #not needed as already created the embedding matrix and pickled\n",
    "\n",
    "## Google drive versions\n",
    "#OPDIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/ImgCapTraining/CocoVal2017_1/'\n",
    "#IPDIRIMGS = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_val2017_wget_1/val2017/'\n",
    "#IPDIRANNO = '/content/gdrive/My Drive/ThesisStoryGen/Data/coco_annotations_trainval2017/'\n",
    "#WORD_EMBEDDINGS_DIR = '/content/gdrive/My Drive/ThesisStoryGen/Data/WordEmbeddings/'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload everything needed from pickled files created based on a certain split of Train and Validation datasets 97k:3k.\n",
    "## Run necessary code again to check all good to proceed with Training phase:\n",
    "## Outcome expected:\n",
    "## 1) unique words in original vocabulary = 24323\n",
    "## 2) unique words in culled vocab vocab_threshold = 6757\n",
    "##    Thus, VOCAB_SIZE = 6757 + 1 = 6758\n",
    "## 3) Embedding matrix shape should be (6758,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Run2_Deterministic_Info/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_DETERMINISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['descriptions_test.pkl',\n",
       " 'descriptions_train_97000.pkl',\n",
       " 'descriptions_train_97000_startend.pkl',\n",
       " 'descriptions_train_and_val.pkl',\n",
       " 'descriptions_val_3000.pkl',\n",
       " 'embedding_matrix_6758_200.pkl',\n",
       " 'img_encodings_train_97000.pkl',\n",
       " 'img_encodings_val_3000.pkl',\n",
       " 'ixtoword_train_97000.pkl',\n",
       " 'train_imgs_keys_97000_randomSplit444.pkl',\n",
       " 'val_imgs_keys_3000_randomSplit444.pkl',\n",
       " 'wordtoix_train_97000.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_DETERMINISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded it all\n"
     ]
    }
   ],
   "source": [
    "## Reload all the files to make it deterministic\n",
    "\n",
    "\n",
    "# variable = img_encodings_train\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_train_97000.pkl', 'rb') as handle:\n",
    "    img_encodings_train = pickle.load(handle)\n",
    "\n",
    "# variable = img_encodings_val\n",
    "with open(IPDIR_DETERMINISTIC + 'img_encodings_val_3000.pkl', 'rb') as handle:\n",
    "    img_encodings_val = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_train\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_train_97000_startend.pkl', 'rb') as handle:\n",
    "    descriptions_train = pickle.load(handle)\n",
    "\n",
    "# variable = descriptions_val\n",
    "with open(IPDIR_DETERMINISTIC + 'descriptions_val_3000.pkl', 'rb') as handle:\n",
    "    descriptions_val = pickle.load(handle)\n",
    "\n",
    "# variable = train_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'train_imgs_keys_97000_randomSplit444.pkl', 'rb') as handle:\n",
    "    train_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = val_imgs_keys\n",
    "with open(IPDIR_DETERMINISTIC + 'val_imgs_keys_3000_randomSplit444.pkl', 'rb') as handle:\n",
    "    val_imgs_keys = pickle.load(handle)\n",
    "\n",
    "# variable = embedding_matrix\n",
    "with open(IPDIR_DETERMINISTIC + 'embedding_matrix_6758_200.pkl', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "# variable = ixtoword\n",
    "with open(IPDIR_DETERMINISTIC + 'ixtoword_train_97000.pkl', 'rb') as handle:\n",
    "    ixtoword = pickle.load(handle)\n",
    "\n",
    "# variable = wordtoix\n",
    "with open(IPDIR_DETERMINISTIC + 'wordtoix_train_97000.pkl', 'rb') as handle:\n",
    "    wordtoix = pickle.load(handle)\n",
    "\n",
    "print(f\"Reloaded it all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the reloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings data:\n",
      "len(img_encodings_train) = 97000\t\tlen(img_encodings_val) = 3000\n",
      "Descriptions data:\n",
      "len(descriptions_train) = 97000\t\tlen(descriptions_val) = 3000\n",
      "\n",
      "CHECK : reloaded values = 97k for Train , 3k for Validation\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encodings data:\\nlen(img_encodings_train) = {len(img_encodings_train)}\\t\\tlen(img_encodings_val) = {len(img_encodings_val)}\")\n",
    "print(f\"Descriptions data:\\nlen(descriptions_train) = {len(descriptions_train)}\\t\\tlen(descriptions_val) = {len(descriptions_val)}\")\n",
    "print(f\"\\nCHECK : reloaded values = 97k for Train , 3k for Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocabulary Size with all words = 24323\n",
      "\n",
      "CHECK : reloaded value = 24323\n"
     ]
    }
   ],
   "source": [
    "## at this stage the descriptions_train already has the start and end tokens added to it\n",
    "vocabulary = set()\n",
    "for key in descriptions_train.keys():\n",
    "    [vocabulary.update(d.split()) for d in descriptions_train[key]]\n",
    "print(f\"Original Vocabulary Size with all words = {len(vocabulary)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 24323\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culled vocabulary to only retain words occurring more than threshold = 10 times.\n",
      "New vocab size , len(vocab_threshold) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions, find the freq and retain words where the freq > threshold chosen\n",
    "\n",
    "all_desc_in_training_samples = []\n",
    "for key, val in descriptions_train.items():\n",
    "    for cap in val:\n",
    "        all_desc_in_training_samples.append(cap)\n",
    "\n",
    "MIN_WORD_COUNT_THRESHOLD = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for each_desc in all_desc_in_training_samples:\n",
    "    nsents += 1\n",
    "    for w in each_desc.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab_threshold = [w for w in word_counts if word_counts[w] >= MIN_WORD_COUNT_THRESHOLD]\n",
    "\n",
    "print(f\"Culled vocabulary to only retain words occurring more than threshold = {MIN_WORD_COUNT_THRESHOLD} times.\\nNew vocab size , len(vocab_threshold) = {len(vocab_threshold)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 49\n",
      "\n",
      "CHECK : reloaded value = 49\n"
     ]
    }
   ],
   "source": [
    "## determine the maximum sequence length - parameter MAX_LENGTH_CAPTION used during the RNN deocder model setup\n",
    "\n",
    "## convert a dictionary of clean descriptions to a list of descriptions\n",
    "def extract_each_desc(_descriptions):\n",
    "    all_desc = list()\n",
    "    for key in _descriptions.keys():\n",
    "        [all_desc.append(d) for d in _descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "## find the longest description length\n",
    "def find_max_length_desc(_descriptions):\n",
    "    desc_sentences = extract_each_desc(_descriptions)\n",
    "    return max(len(d.split()) for d in desc_sentences)\n",
    "\n",
    "MAX_LENGTH_CAPTION = find_max_length_desc(descriptions_train)  ## will be used directly later while defining Decoder model\n",
    "print(f\"Max Description Length: {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"\\nCHECK : reloaded value = 49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(wordtoix) = 6757\n",
      "\n",
      "CHECK : reloaded value = 6757\n",
      "\n",
      "\n",
      "Set the   VOCAB_SIZE = len(wordtoix) + 1 = 6758\n",
      "\n",
      "\n",
      "Set the   EMBEDDING_DIMS = 200\n"
     ]
    }
   ],
   "source": [
    "## the value now, as it will be used as:: VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"len(wordtoix) = {len(wordtoix)}\")\n",
    "print(f\"\\nCHECK : reloaded value = 6757\")\n",
    "\n",
    "VOCAB_SIZE = len(wordtoix) + 1\n",
    "print(f\"\\n\\nSet the   VOCAB_SIZE = len(wordtoix) + 1 = {VOCAB_SIZE}\")\n",
    "\n",
    "EMBEDDING_DIMS = 200\n",
    "print(f\"\\n\\nSet the   EMBEDDING_DIMS = {EMBEDDING_DIMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9 526\n"
     ]
    }
   ],
   "source": [
    "## see the index output by wordtoix for the start and end sequence tokens as well as some random one word\n",
    "print( wordtoix.get('startseq') , wordtoix.get('endseq') , wordtoix.get('cat') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding_matrix = (6758, 200)\n",
      "\n",
      "CHECK : reloaded shape = 6758,200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of embedding_matrix = {embedding_matrix.shape}\")\n",
    "print(f\"\\nCHECK : reloaded shape = 6758,200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD be:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n"
     ]
    }
   ],
   "source": [
    "## Recheck these parameters that will define the Decoder architecture ::: EMBEDDING_DIMS , VOCAB_SIZE , MAX_LENGTH_CAPTION\n",
    "print(f\"SHOULD be:\\nEMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\")\n",
    "print(f\"\\nThe values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete all the unused variable as not important for this inference and comparison processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del img_encodings_train, img_encodings_val, descriptions_train, descriptions_val, train_imgs_keys, val_imgs_keys, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT USING ENCODINGS - WILL PASS IMAGE THRU ENCODER TO EXTRACT ENCODINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the Coco_Val_2017 5k image encodings from pickled file.\n",
    "### NOTE: This is the TEST dataset I am using which has the ground truth captions for comparison against my models inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IPDIR_IMG_ENCODINGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aad6753d8a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIPDIR_IMG_ENCODINGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'IPDIR_IMG_ENCODINGS' is not defined"
     ]
    }
   ],
   "source": [
    "IPDIR_IMG_ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(IPDIR_IMG_ENCODINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Val2017 images pickled files\n",
    "print(f\"\\n\\nLoading encodings for the test images\")\n",
    "pickled_encodings_files_val2017 = ['val2017_all_5k_images_encoded_features_pickled_2.pkl']\n",
    "img_encodings_test = {}\n",
    "for idx, subset_encodings_file_val2017 in enumerate(pickled_encodings_files_val2017):\n",
    "    with open(IPDIR_IMG_ENCODINGS + subset_encodings_file_val2017, 'rb') as handle:\n",
    "        img_encodings_test.update(pickle.load(handle))\n",
    "        print(f\"Loaded file num {idx+1} :: {subset_encodings_file_val2017}\")\n",
    "    print(f\"Number of entries in img_encodings_test = {len(img_encodings_test)}\")\n",
    "print(f\"\\nFinal count img_encodings_test = {len(img_encodings_test)}. Should be = 5k.\")\n",
    "\n",
    "del pickled_encodings_files_val2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thus \"img_encodings_test\" has the CNN encoder output feature encodings for all 5k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the annotations file to load the descriptions and clean as per standard processing followed overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/coco_2017_annotations/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_ANNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captions_train2017.json', 'captions_val2017.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_ANNO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190236</td>\n",
       "      <td>401</td>\n",
       "      <td>An office cubicle with four different types of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...\n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway\n",
       "2    190236  401  An office cubicle with four different types of..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the descriptions_test_dataset dict\n",
    "## BUT NOTE: it is actually from the captions_val2017 dataset - which I treat as my TEST DATASET\n",
    "\n",
    "with open(IPDIR_ANNO+'captions_val2017.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "  #data.keys() # dict_keys(['info', 'licenses', 'images', 'annotations'])\n",
    "  #type(data['annotations']) # is a list\n",
    "  #type(data['images'])      # also is a list\n",
    "\n",
    "dfanno = pd.DataFrame(data=data['annotations'])\n",
    "# dfanno.columns = Index(['image_id', 'id', 'caption'], dtype='object') \n",
    "# dfanno.dtypes =\n",
    "#   image_id     int64\n",
    "#   id           int64\n",
    "#   caption     object\n",
    "#   dtype: object\n",
    "\n",
    "dfimages = pd.DataFrame(data=data['images'])\n",
    "# dfimages.columns = Index(['license', 'file_name', 'coco_url', 'height', 'width', 'date_captured', 'flickr_url', 'id'], dtype='object')\n",
    "# dfimages.dtypes =\n",
    "#   license           int64\n",
    "#   file_name        object\n",
    "#   coco_url         object\n",
    "#   height            int64\n",
    "#   width             int64\n",
    "#   date_captured    object\n",
    "#   flickr_url       object\n",
    "#   id                int64\n",
    "#   dtype: object\n",
    "## of above, am dropping useless columns\n",
    "dfimages.drop(['license', 'coco_url', 'date_captured', 'flickr_url'], axis = 1, inplace=True)\n",
    "\n",
    "## columns remaining in the dfs are:\n",
    "# dfanno columns are      image_id , id , caption\n",
    "#                         179765   , 38 ,\tA black Honda motorcycle parked in front of a ...\n",
    "# dfimages columns are    file_name        , height ,  width , id\n",
    "#                         000000397133.jpg , 427    ,  640   , 397133\n",
    "\n",
    "## the captions are not ordered for each image and seem to be randomly placed\n",
    "\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179765</td>\n",
       "      <td>38</td>\n",
       "      <td>A black Honda motorcycle parked in front of a ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179765</td>\n",
       "      <td>182</td>\n",
       "      <td>A Honda motorcycle parked in a grass driveway</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179765</td>\n",
       "      <td>479</td>\n",
       "      <td>A black Honda motorcycle with a dark burgundy ...</td>\n",
       "      <td>000000179765.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id   id                                            caption  \\\n",
       "0    179765   38  A black Honda motorcycle parked in front of a ...   \n",
       "1    179765  182      A Honda motorcycle parked in a grass driveway   \n",
       "2    179765  479  A black Honda motorcycle with a dark burgundy ...   \n",
       "\n",
       "          file_name  \n",
       "0  000000179765.jpg  \n",
       "1  000000179765.jpg  \n",
       "2  000000179765.jpg  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## marrying the \"annotations\" and the \"images\" keys info together into dfanno\n",
    "## the file_name column has the actual image name from the \"image\" key section\n",
    "dfanno = dfanno.merge(dfimages, how=\"inner\", left_on='image_id', right_on='id')\n",
    "dfanno.drop(['height', 'width', 'id_y'], axis = 1, inplace=True)\n",
    "dfanno.rename(columns={'id_x':'id'}, inplace=True)\n",
    "dfanno.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(descriptions_test) = 5000\n"
     ]
    }
   ],
   "source": [
    "## Mapping image with captions using dictionary\n",
    "\n",
    "def create_descriptions_dictionary(_dfin):\n",
    "    descriptions = {}\n",
    "    for row in _dfin.itertuples():\n",
    "      rowdict = row._asdict()\n",
    "      img_filename = rowdict['file_name'].split('.')[0] # drop the .jpg part\n",
    "      img_caption = rowdict['caption']\n",
    "      if(img_filename not in descriptions):\n",
    "        descriptions[img_filename] = [img_caption]\n",
    "      else:\n",
    "        descriptions[img_filename].append(img_caption)\n",
    "    return descriptions\n",
    "\n",
    "descriptions_test = create_descriptions_dictionary(dfanno)\n",
    "print(f\"len(descriptions_test) = {len(descriptions_test)}\")\n",
    "\n",
    "del dfanno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the description for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000179765': ['A black Honda motorcycle parked in front of a garage.',\n",
       "  'A Honda motorcycle parked in a grass driveway',\n",
       "  'A black Honda motorcycle with a dark burgundy seat.',\n",
       "  'Ma motorcycle parked on the gravel in front of a garage',\n",
       "  'A motorcycle with its brake extended standing outside'],\n",
       " '000000190236': ['An office cubicle with four different types of computers.',\n",
       "  'The home office space seems to be very cluttered.',\n",
       "  'an office with desk computer and chair and laptop.',\n",
       "  'Office setting with a lot of computer screens.',\n",
       "  'A desk and chair in an office cubicle.']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(descriptions_test.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A dog sitting between its masters feet on a footstool watching tv\\n',\n",
       " 'A dog between the feet of a person looking at a TV.',\n",
       " 'A dog and a person are watching television together.',\n",
       " 'A person is sitting with their dog watching tv.',\n",
       " 'A man relaxing at home, watching television with his dog.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "## string.punctuation  gives   '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' and will take care of all these characters being made into a space\n",
    "tran_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "for key, desc_list in descriptions_test.items():\n",
    "    for idx in range(len(desc_list)):\n",
    "        desc = desc_list[idx]\n",
    "        # replace all punctuation with space in description before tokenizing\n",
    "        desc = desc.translate(tran_table)\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # remove any non-alphabetic tokens\n",
    "        desc = [word for word in desc if word.isalpha()]\n",
    "        # overwrite with cleaned description\n",
    "        desc_list[idx] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog sitting between its masters feet on footstool watching tv',\n",
       " 'dog between the feet of person looking at tv',\n",
       " 'dog and person are watching television together',\n",
       " 'person is sitting with their dog watching tv',\n",
       " 'man relaxing at home watching television with his dog']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## example of caption with accidental newline \\n in the caption  -- POST CLEANUP\n",
    "descriptions_test['000000482917']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now the cleaned descriptions are ready in \"descriptions_test\" file.\n",
    "## Note: No need to add the startseq and endseq tokens as I will remove that from the inference before comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get an idea of the length of GT captions in cleaned descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black honda motorcycle parked in front of garage',\n",
       " 'honda motorcycle parked in grass driveway',\n",
       " 'black honda motorcycle with dark burgundy seat',\n",
       " 'ma motorcycle parked on the gravel in front of garage',\n",
       " 'motorcycle with its brake extended standing outside',\n",
       " 'an office cubicle with four different types of computers',\n",
       " 'the home office space seems to be very cluttered',\n",
       " 'an office with desk computer and chair and laptop']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gt_cap for desc in descriptions_test.values() for gt_cap in desc][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>caplen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>black honda motorcycle parked in front of garage</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>honda motorcycle parked in grass driveway</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>black honda motorcycle with dark burgundy seat</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ma motorcycle parked on the gravel in front of...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>motorcycle with its brake extended standing ou...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an office cubicle with four different types of...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the home office space seems to be very cluttered</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>an office with desk computer and chair and laptop</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  caplen\n",
       "0   black honda motorcycle parked in front of garage       8\n",
       "1          honda motorcycle parked in grass driveway       6\n",
       "2     black honda motorcycle with dark burgundy seat       7\n",
       "3  ma motorcycle parked on the gravel in front of...      10\n",
       "4  motorcycle with its brake extended standing ou...       7\n",
       "5  an office cubicle with four different types of...       9\n",
       "6   the home office space seems to be very cluttered       9\n",
       "7  an office with desk computer and chair and laptop       9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=[gt_cap for desc in descriptions_test.values() for gt_cap in desc], columns=['sent'])\n",
    "df['caplen'] = df['sent'].str.split().apply(len)\n",
    "#df['sent'][df['caplen'] == MAX_LENGTH_CAPTION]\n",
    "df[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45     1\n",
       "42     1\n",
       "35     1\n",
       "34     1\n",
       "33     1\n",
       "32     2\n",
       "30     1\n",
       "29     1\n",
       "28     2\n",
       "27     3\n",
       "26     1\n",
       "25     6\n",
       "24     6\n",
       "23     7\n",
       "22    11\n",
       "Name: caplen, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see counts of descriptions with the LONGEST lengths\n",
    "## we see only one caption has the maximum length\n",
    "## NOTE: no startseq and endseq tokens present\n",
    "countdf = df['caplen'].value_counts()\n",
    "countdf.sort_index(inplace=True, ascending=False)\n",
    "countdf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      160\n",
       "6     1971\n",
       "7     4873\n",
       "8     6005\n",
       "9     4864\n",
       "10    3118\n",
       "11    1764\n",
       "12     958\n",
       "13     512\n",
       "14     306\n",
       "15     151\n",
       "16     117\n",
       "17      61\n",
       "18      44\n",
       "19      26\n",
       "Name: caplen, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see counts of descriptions with the SHORTEST lengths\n",
    "## NOTE: no startseq and endseq tokens present\n",
    "countdf = df['caplen'].value_counts()\n",
    "countdf.sort_index(inplace=True, ascending=True)\n",
    "countdf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## maximum lenght of description in Test data is 45 without extra tokens, else 47 with startseq and endseq inserted\n",
    "## But my trained model has max caption length of 49 including the extra tokens for startseq and endseq\n",
    "df['caplen'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>caplen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>large square concrete wall which shows people ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    sent  caplen\n",
       "15996  large square concrete wall which shows people ...      45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see the description with longest length = 45\n",
    "df.loc[df['caplen'] == df['caplen'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, countdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGzQr4tiiWDc"
   },
   "source": [
    "## ALWAYS - Define the RNN Decoder model\n",
    "## Reload ALL weights from previous training point. Then freeze ONLY the embedding layer as before. Thus, the embedding layer will continue to use the GloVe-200 embeddings matrix weights that were first set during Training Phase 1 setup.\n",
    "\n",
    "\n",
    "### About Keras Emebedding layer\n",
    "### \n",
    "#### Default initializing of this layer: https://keras.io/api/layers/core_layers/embedding/\n",
    "######tf.keras.layers.Embedding(\n",
    "######    input_dim, output_dim,\n",
    "######    embeddings_initializer=\"uniform\",\n",
    "######    embeddings_regularizer=None,\n",
    "######    activity_regularizer=None,\n",
    "######    embeddings_constraint=None,\n",
    "######    mask_zero=False,\n",
    "######    input_length=None,\n",
    "######    **kwargs\n",
    "######)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_WEIGHTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decoder_Run_2_Wt_ep_1.h5',\n",
       " 'Decoder_Run_2_Wt_ep_10.h5',\n",
       " 'Decoder_Run_2_Wt_ep_2.h5',\n",
       " 'Decoder_Run_2_Wt_ep_3.h5',\n",
       " 'Decoder_Run_2_Wt_ep_4.h5',\n",
       " 'Decoder_Run_2_Wt_ep_5.h5',\n",
       " 'Decoder_Run_2_Wt_ep_6.h5',\n",
       " 'Decoder_Run_2_Wt_ep_7.h5',\n",
       " 'Decoder_Run_2_Wt_ep_8.h5',\n",
       " 'Decoder_Run_2_Wt_ep_9.h5']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_WEIGHTS_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT - SPECIFY CORRECT WEIGHTS FILE TO LOAD FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_WEIGHTS_FILE_NAME = r'Decoder_Run_2_Wt_ep_10.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will reload this file = /media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/Decoder_Run_2_Wt_ep_10.h5\n",
      "Verfiy the weights in file exists : Check value is True = True\n"
     ]
    }
   ],
   "source": [
    "RELOAD_WEIGHTS_FILE_PATH = IPDIR_WEIGHTS_IN + RELOAD_WEIGHTS_FILE_NAME\n",
    "print(f\"Will reload this file = {RELOAD_WEIGHTS_FILE_PATH}\")\n",
    "print(f\"Verfiy the weights in file exists : Check value is True = {os.path.exists(RELOAD_WEIGHTS_FILE_PATH) and os.path.isfile(RELOAD_WEIGHTS_FILE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_parameter_counts(_model):\n",
    "    total_params = _model.count_params()\n",
    "    total_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.trainable_variables]))\n",
    "    total_non_trainable_params = int(np.sum([np.prod(v.get_shape().as_list()) for v in _model.non_trainable_variables]))\n",
    "    #print(f\"Total params = {total_decoder_params}\\t Trainable total = {total_decoder_trainable_params}\\t Non-trainable total = {total_decoder_non_trainable_params}\")\n",
    "    return total_params, total_trainable_params, total_non_trainable_params\n",
    "\n",
    "def reload_rnn_decoder_for_inference(_saved_weights_file, _EMBEDDING_DIMS, _VOCAB_SIZE, _MAX_LENGTH_CAPTION):\n",
    "    if os.path.exists(_saved_weights_file) and os.path.isfile(_saved_weights_file):\n",
    "        ## Decoder Model defining\n",
    "        \n",
    "        ## parameters to define model - sent during function call\n",
    "        #EMBEDDING_DIMS is initialised earlier while creating embedding matrix\n",
    "        #VOCAB_SIZE is initialised earlier\n",
    "        #MAX_LENGTH_CAPTION is initialised earlier\n",
    "        \n",
    "        inputs1 = keras.Input(shape=(2048,))\n",
    "        fe1 = keras.layers.Dropout(0.5)(inputs1)\n",
    "        fe2 = keras.layers.Dense(256, activation='relu')(fe1)\n",
    "        \n",
    "        # partial caption sequence model\n",
    "        inputs2 = keras.Input(shape=(_MAX_LENGTH_CAPTION,))\n",
    "        se1 = keras.layers.Embedding(_VOCAB_SIZE, _EMBEDDING_DIMS, mask_zero=True)(inputs2)\n",
    "        se2 = keras.layers.Dropout(0.5)(se1)\n",
    "        se3 = keras.layers.LSTM(256)(se2)\n",
    "        \n",
    "        # decoder (feed forward) model\n",
    "        decoder1 = keras.layers.add([fe2, se3])\n",
    "        decoder2 = keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "        outputs = keras.layers.Dense(_VOCAB_SIZE, activation='softmax')(decoder2)\n",
    "        \n",
    "        # merge the two input models\n",
    "        reloaded_rnn_decoder_model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nCreated RNN Decoder model defined with these paramenters:\\nEMBEDDING_DIMS = {_EMBEDDING_DIMS} , VOCAB_SIZE = {_VOCAB_SIZE} , MAX_LENGTH_CAPTION = {_MAX_LENGTH_CAPTION}\")\n",
    "        print(f\"\\nBEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\nAttempting to load weights...\\n\")\n",
    "        \n",
    "        ## load the weights\n",
    "        reloaded_rnn_decoder_model.load_weights(_saved_weights_file)\n",
    "        print(f\"SUCCESS - Reloaded weights from :: {_saved_weights_file}\")\n",
    "        \n",
    "        ## freeze the embeddings layer weights so they are non-trainable\n",
    "        reloaded_rnn_decoder_model.trainable = False\n",
    "        print(f\"\\nFrozen all layers as this is only for inference and not for training.\")\n",
    "        \n",
    "        total_p, trainable_p, non_trainable_p = get_model_parameter_counts(reloaded_rnn_decoder_model)\n",
    "        print(f\"\\nAFTER FREEZING, parameter counts:\\nTotal parameters = {total_p} , Trainable parameter = {trainable_p} , Non-trainable parameters = {non_trainable_p}\")\n",
    "        \n",
    "        return reloaded_rnn_decoder_model\n",
    "    else:\n",
    "        print(f\"\\nERROR reloading weights. Check weights file exists here = {_saved_weights_file} ;\\nOR model setup parameters incompatible with the saved weights file given.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values to be used in Decoder setup:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "Weights to be reloaded from here:\n",
      "/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/Decoder_Run_2_Wt_ep_10.h5\n"
     ]
    }
   ],
   "source": [
    "print(f\"The values to be used in Decoder setup:\\nEMBEDDING_DIMS = {EMBEDDING_DIMS} , VOCAB_SIZE = {VOCAB_SIZE} , MAX_LENGTH_CAPTION = {MAX_LENGTH_CAPTION}\")\n",
    "print(f\"Weights to be reloaded from here:\\n{RELOAD_WEIGHTS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created RNN Decoder model defined with these paramenters:\n",
      "EMBEDDING_DIMS = 200 , VOCAB_SIZE = 6758 , MAX_LENGTH_CAPTION = 49\n",
      "\n",
      "BEFORE LOADING WEIGHTS AND FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 4146710 , Non-trainable parameters = 0\n",
      "\n",
      "Attempting to load weights...\n",
      "\n",
      "SUCCESS - Reloaded weights from :: /media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCap/SavedData/Thesis_ImgCap_Weights_In_Run2/Decoder_Run_2_Wt_ep_10.h5\n",
      "\n",
      "Frozen all layers as this is only for inference and not for training.\n",
      "\n",
      "AFTER FREEZING, parameter counts:\n",
      "Total parameters = 4146710 , Trainable parameter = 0 , Non-trainable parameters = 4146710\n",
      "\n",
      "<class 'tensorflow.python.keras.engine.training.Model'>\n"
     ]
    }
   ],
   "source": [
    "reloaded_RNN_decoder = reload_rnn_decoder_for_inference(RELOAD_WEIGHTS_FILE_PATH, EMBEDDING_DIMS, VOCAB_SIZE, MAX_LENGTH_CAPTION)\n",
    "if reloaded_RNN_decoder is None:\n",
    "    print(f\"FATAL ERROR setting up Decoder\")\n",
    "else:\n",
    "    print(f\"\\n{type(reloaded_RNN_decoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "K_mLYJjsRqMO",
    "outputId": "809ebcdd-f972-4f47-f60f-a207404054a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 49)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 49, 200)      1351600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 49, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6758)         1736806     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,146,710\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,146,710\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING THE EMBEDDINGS WEIGHTS\n",
    "## NOTE the count of Trainable params has REDUCED\n",
    "reloaded_RNN_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output of command:  model_RNN_decoder.summary() AFTER FREEZING EVERYTHING\n",
    "## NOTE the count of Trainable params is ZERO now\n",
    "\n",
    "#Model: \"functional_5\"\n",
    "#__________________________________________________________________________________________________\n",
    "#Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "#==================================================================================================\n",
    "#input_6 (InputLayer)            [(None, 49)]         0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#input_5 (InputLayer)            [(None, 2048)]       0                                            \n",
    "#__________________________________________________________________________________________________\n",
    "#embedding_2 (Embedding)         (None, 49, 200)      1351600     input_6[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_4 (Dropout)             (None, 2048)         0           input_5[0][0]                    \n",
    "#__________________________________________________________________________________________________\n",
    "#dropout_5 (Dropout)             (None, 49, 200)      0           embedding_2[0][0]                \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_6 (Dense)                 (None, 256)          524544      dropout_4[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#lstm_2 (LSTM)                   (None, 256)          467968      dropout_5[0][0]                  \n",
    "#__________________________________________________________________________________________________\n",
    "#add_2 (Add)                     (None, 256)          0           dense_6[0][0]                    \n",
    "#                                                                 lstm_2[0][0]                     \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_7 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
    "#__________________________________________________________________________________________________\n",
    "#dense_8 (Dense)                 (None, 6758)         1736806     dense_7[0][0]                    \n",
    "#==================================================================================================\n",
    "#Total params: 4,146,710\n",
    "#Trainable params: 0\n",
    "#Non-trainable params: 4,146,710\n",
    "#__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNo79p0Az37z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN Encoder (Keras Google Inception-v3 pretrained on Imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.training.Model"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    ### Load pre-trained model of Inception-v3 pretrained on Imagenet\n",
    "    model_inception_v3_pretrained_imagement = tf.keras.applications.InceptionV3(weights='imagenet')\n",
    "    # Create new model, by removing last layer (output layer) from Inception-V3\n",
    "    model_CNN_encoder = keras.Model(inputs=model_inception_v3_pretrained_imagement.input, outputs=model_inception_v3_pretrained_imagement.layers[-2].output)\n",
    "    \n",
    "    ## should be tensorflow.python.keras.engine.functional.Functional\n",
    "    type(model_CNN_encoder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform inference on all the 5k images of the TEST DATASET (coco_val2017 data)\n",
    "### descriptions_test dict has array of cleaned descriptions with image (without jpg) as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions related to using the cnn encoder to extract image features\n",
    "\n",
    "def preprocess_image_for_Incepv3(_img_path):\n",
    "    \"\"\"\n",
    "    ## Make images suitable for use by Inception-v3 model later\n",
    "    ##\n",
    "    ## Resize to (299, 299)\n",
    "    ## As model needs 4-dim input tensor, add one dimenion to make it (1, 299, 299, 3)\n",
    "    ## Preprocess the image using custom function of Inception-v3 model\n",
    "    \"\"\"\n",
    "    img = tf.keras.preprocessing.image.load_img(_img_path, target_size=(299, 299))\n",
    "    #print(f\"type={type(img)}\") # type(img): type=<class 'PIL.Image.Image'>\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img) # Converts PIL Image instance to numpy array (299,299,3)\n",
    "    img = np.expand_dims(img, axis=0) #Add one more dimension: (1, 299, 299, 3) # Inception-V3 requires 4 dimensions\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img) # preprocess image as per Inception-V3 model\n",
    "    return img  # shape will be (1, 299, 299, 3)\n",
    "\n",
    "def encode_image(_imgpath, _model_CNN_encoder):\n",
    "    \"\"\"\n",
    "    # Function to encode given image into a vector of size (2048, )\n",
    "    \"\"\"\n",
    "    ## preprocess image per Inception-v3 requirements\n",
    "    preproc_img = preprocess_image_for_Incepv3(_imgpath)\n",
    "    ## get encoding vector for image\n",
    "    encoded_features = _model_CNN_encoder.predict(preproc_img) \n",
    "    ## reshape from (1, 2048) to (2048, )\n",
    "    encoded_features = encoded_features.reshape(encoded_features.shape[1], ) \n",
    "    return encoded_features\n",
    "\n",
    "## functions to actually do the inference\n",
    "\n",
    "def greedySearch(_decoder_model, _img_encoding, _max_length, _wordtoix = None, _ixtoword = None):\n",
    "    wordtoix = _wordtoix\n",
    "    ixtoword = _ixtoword\n",
    "    in_text = 'startseq'\n",
    "    for i in range(_max_length):\n",
    "        sequence = [ wordtoix[w] for w in in_text.split() if w in wordtoix ]\n",
    "        sequence = keras.preprocessing.sequence.pad_sequences([sequence], maxlen=_max_length)\n",
    "        yhat = _decoder_model.predict([_img_encoding,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    caption_out = in_text.split()\n",
    "    #caption_out = caption_out[1:-1]  ## drop the startseq and endseq words at either end\n",
    "    caption_out = ' '.join(caption_out)\n",
    "    return caption_out\n",
    "\n",
    "def do_inference_one_image_totally_new(_infer_image_jpg, _img_encoding, _MAX_LENGTH_CAPTION, _model_RNN_decoder, _wordtoix = None, _ixtoword = None, _DEBUG_SWITCH=False):\n",
    "    ## show the original image\n",
    "    image = _img_encoding.reshape((1,2048))\n",
    "    \n",
    "    ## briefly show the image about to be inferred if in debug mode\n",
    "    if _DEBUG_SWITCH:\n",
    "        plt.ion()\n",
    "        x = plt.imread(_infer_image_jpg)\n",
    "        plt.imshow(x)\n",
    "        plt.show()\n",
    "        plt.pause(1.5)  ## display for 1.5 seconds\n",
    "    \n",
    "    predicted_caption = None\n",
    "    ## get the prediction caption using greedy search\n",
    "    predicted_caption = greedySearch(_model_RNN_decoder, image, _MAX_LENGTH_CAPTION, _wordtoix = _wordtoix, _ixtoword = _ixtoword)\n",
    "    \n",
    "    if _DEBUG_SWITCH:\n",
    "        print(f\"\\nFor image :: {_infer_image_jpg}\\n\\nInference caption output:\\n{ predicted_caption }\")\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH_CAPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRH-BDBA Acads/Thesis/StoryGenerator/Data/COCO_val2017_5k/val2017/'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_IMGS_COCO_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_infer_full_path_list = [IPDIR_IMGS_COCO_VAL + f for f in os.listdir(IPDIR_IMGS_COCO_VAL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000139\n",
      "000000000285\n",
      "000000000632\n",
      "000000000724\n",
      "000000000776\n"
     ]
    }
   ],
   "source": [
    "## check the key being made to access the descriptions from its dict\n",
    "for f in imgs_to_infer_full_path_list[:5]:\n",
    "    print(f\"{ os.path.splitext(os.path.basename(f))[0] }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 5000\n",
      "Processing 51 of 5000\n",
      "Processing 101 of 5000\n",
      "Processing 151 of 5000\n",
      "Processing 201 of 5000\n",
      "Processing 251 of 5000\n",
      "Processing 301 of 5000\n",
      "Processing 351 of 5000\n",
      "Processing 401 of 5000\n",
      "Processing 451 of 5000\n",
      "Processing 501 of 5000\n",
      "Processing 551 of 5000\n",
      "Processing 601 of 5000\n",
      "Processing 651 of 5000\n",
      "Processing 701 of 5000\n",
      "Processing 751 of 5000\n",
      "Processing 801 of 5000\n",
      "Processing 851 of 5000\n",
      "Processing 901 of 5000\n",
      "Processing 951 of 5000\n",
      "Processing 1001 of 5000\n",
      "Processing 1051 of 5000\n",
      "Processing 1101 of 5000\n",
      "Processing 1151 of 5000\n",
      "Processing 1201 of 5000\n",
      "Processing 1251 of 5000\n",
      "Processing 1301 of 5000\n",
      "Processing 1351 of 5000\n",
      "Processing 1401 of 5000\n",
      "Processing 1451 of 5000\n",
      "Processing 1501 of 5000\n",
      "Processing 1551 of 5000\n",
      "Processing 1601 of 5000\n",
      "Processing 1651 of 5000\n",
      "Processing 1701 of 5000\n",
      "Processing 1751 of 5000\n",
      "Processing 1801 of 5000\n",
      "Processing 1851 of 5000\n",
      "Processing 1901 of 5000\n",
      "Processing 1951 of 5000\n",
      "Processing 2001 of 5000\n",
      "Processing 2051 of 5000\n",
      "Processing 2101 of 5000\n",
      "Processing 2151 of 5000\n",
      "Processing 2201 of 5000\n",
      "Processing 2251 of 5000\n",
      "Processing 2301 of 5000\n",
      "Processing 2351 of 5000\n",
      "Processing 2401 of 5000\n",
      "Processing 2451 of 5000\n",
      "Processing 2501 of 5000\n",
      "Processing 2551 of 5000\n",
      "Processing 2601 of 5000\n",
      "Processing 2651 of 5000\n",
      "Processing 2701 of 5000\n",
      "Processing 2751 of 5000\n",
      "Processing 2801 of 5000\n",
      "Processing 2851 of 5000\n",
      "Processing 2901 of 5000\n",
      "Processing 2951 of 5000\n",
      "Processing 3001 of 5000\n",
      "Processing 3051 of 5000\n",
      "Processing 3101 of 5000\n",
      "Processing 3151 of 5000\n",
      "Processing 3201 of 5000\n",
      "Processing 3251 of 5000\n",
      "Processing 3301 of 5000\n",
      "Processing 3351 of 5000\n",
      "Processing 3401 of 5000\n",
      "Processing 3451 of 5000\n",
      "Processing 3501 of 5000\n",
      "Processing 3551 of 5000\n",
      "Processing 3601 of 5000\n",
      "Processing 3651 of 5000\n",
      "Processing 3701 of 5000\n",
      "Processing 3751 of 5000\n",
      "Processing 3801 of 5000\n",
      "Processing 3851 of 5000\n",
      "Processing 3901 of 5000\n",
      "Processing 3951 of 5000\n",
      "Processing 4001 of 5000\n",
      "Processing 4051 of 5000\n",
      "Processing 4101 of 5000\n",
      "Processing 4151 of 5000\n",
      "Processing 4201 of 5000\n",
      "Processing 4251 of 5000\n",
      "Processing 4301 of 5000\n",
      "Processing 4351 of 5000\n",
      "Processing 4401 of 5000\n",
      "Processing 4451 of 5000\n",
      "Processing 4501 of 5000\n",
      "Processing 4551 of 5000\n",
      "Processing 4601 of 5000\n",
      "Processing 4651 of 5000\n",
      "Processing 4701 of 5000\n",
      "Processing 4751 of 5000\n",
      "Processing 4801 of 5000\n",
      "Processing 4851 of 5000\n",
      "Processing 4901 of 5000\n",
      "Processing 4951 of 5000\n"
     ]
    }
   ],
   "source": [
    "## use the encodings and get inferences\n",
    "comparison_dict = {}  ## key will be the image name, values will be inference caption and the cleaned descriptions\n",
    "i = 0\n",
    "total_to_infer = len(imgs_to_infer_full_path_list)\n",
    "for img_to_infer in imgs_to_infer_full_path_list:\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processing {i+1} of {total_to_infer}\")\n",
    "    ## extract just the filename without .jpg extension from the full path\n",
    "    key4DescDict = os.path.splitext(os.path.basename(img_to_infer))[0]    \n",
    "    ## Get the encoding by running thru bottleneck\n",
    "    ## Function to encode given image into a vector of size (2048, )\n",
    "    img_encoding_for_inference = encode_image(img_to_infer, model_CNN_encoder)\n",
    "    ## now do the decoder inference using the encoding\n",
    "    predicted_caption = do_inference_one_image_totally_new(img_to_infer, img_encoding_for_inference, MAX_LENGTH_CAPTION, reloaded_RNN_decoder, wordtoix, ixtoword)\n",
    "    ## strip off the startseq and endseq - note that startseq is ALWAYS present as the first word, but the endseq MAY NOT ALWAYS be at the end\n",
    "    predicted_caption = predicted_caption.split(' ')\n",
    "    if predicted_caption[-1] == 'endseq':\n",
    "        predicted_caption = ' '.join(predicted_caption[1:-1]) ## remove both ends\n",
    "    else:\n",
    "        predicted_caption = ' '.join(predicted_caption[1:])   ## remove only startseq\n",
    "    ## update the comparison dict\n",
    "    comparison_dict.update( { key4DescDict : [predicted_caption , descriptions_test[key4DescDict] ] } )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERIFY WHERE TO PICKLE THE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPDIR = r'/home/rohit/PyWDUbuntu/thesis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohit/PyWDUbuntu/thesis/'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_DICT_PICKLE_FILE = r'ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picle destination=\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Picle destination=\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, pickled to:\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl\n"
     ]
    }
   ],
   "source": [
    "## pickle it\n",
    "with open(OPDIR + COMPARISON_DICT_PICKLE_FILE, 'wb') as handle:\n",
    "  pickle.dump(comparison_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"Success, pickled to:\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE BLEU SCORE USING NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER IMPORTANT - CHANGE CONDA ENV FIRST\n",
    "## DO NOT USE               ce7comb1\n",
    "## CHANGE TO                ce6idelements1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as nltk_bleu\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohit/PyWDUbuntu/thesis/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPDIR = r'/home/rohit/PyWDUbuntu/thesis/'\n",
    "OPDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_DICT_PICKLE_FILE = r'ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picle source=\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Picle source=\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload from picled file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded from picle file:\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCap_Run2_TestSet_DescCap_Comparison_WtEp10_1.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(OPDIR + COMPARISON_DICT_PICKLE_FILE, 'rb') as handle:\n",
    "  comparison_dict_reload = pickle.load(handle)\n",
    "print(f\"Reloaded from picle file:\\n{OPDIR + COMPARISON_DICT_PICKLE_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu scores with NLTK\n",
    "\n",
    "### Link: https://ariepratama.github.io/Introduction-to-BLEU-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000000139': ['living room with couch and television',\n",
       "  ['woman stands in the dining area at the table',\n",
       "   'room with chairs table and woman in it',\n",
       "   'woman standing in kitchen by window',\n",
       "   'person standing at table in room',\n",
       "   'living area with television and table']],\n",
       " '000000000285': ['brown bear is standing in the grass',\n",
       "  ['big burly grizzly bear is show with grass in the background',\n",
       "   'the large brown bear has black nose',\n",
       "   'closeup of brown bear sitting in grassy area',\n",
       "   'large bear that is sitting on grass',\n",
       "   'close up picture of brown bear face']],\n",
       " '000000000632': ['bedroom with bed and table with two lamps',\n",
       "  ['bedroom scene with bookcase blue comforter and window',\n",
       "   'bedroom with bookshelf full of books',\n",
       "   'this room has bed with blue sheets and large bookcase',\n",
       "   'bed and mirror in small room',\n",
       "   'bed room with neatly made bed window and book shelf']],\n",
       " '000000000724': ['stop sign with street sign on it',\n",
       "  ['stop sign is mounted upside down on it post',\n",
       "   'stop sign that is hanging upside down',\n",
       "   'an upside down stop sign by the road',\n",
       "   'stop sign put upside down on metal pole',\n",
       "   'stop sign installed upside down on street corner']],\n",
       " '000000000776': ['teddy bear sitting on top of brown teddy bear',\n",
       "  ['three teddy bears each different color snuggling together',\n",
       "   'three stuffed animals are sitting on bed',\n",
       "   'three teddy bears giving each other hug',\n",
       "   'group of three stuffed animal teddy bears',\n",
       "   'three stuffed bears hugging and sitting on blue pillow']],\n",
       " '000000000785': ['man in red jacket and skis on snow covered slope',\n",
       "  ['woman posing for the camera standing on skis',\n",
       "   'woman standing on skiis while posing for the camera',\n",
       "   'woman in red jacket skiing down slope',\n",
       "   'young woman is skiing down the mountain slope',\n",
       "   'person on skis makes her way through the snow']],\n",
       " '000000000802': ['kitchen with refrigerator and refrigerator',\n",
       "  ['kitchen with refrigerator stove and oven with cabinets',\n",
       "   'white oven and white refrigerator are in the kitchen',\n",
       "   'the refrigerator is brand new and was delivered today',\n",
       "   'stark white appliances stand out against brown wooden cabinets',\n",
       "   'kitchen appliances and cabinets as seen through opening']],\n",
       " '000000000872': ['baseball player swinging bat at ball',\n",
       "  ['couple of baseball player standing on field',\n",
       "   'two men playing baseball in field on sunny day',\n",
       "   'two baseball players are playing baseball on field',\n",
       "   'couple of men play baseball and the batter runs for base',\n",
       "   'two guys playing baseball with trees in the back']],\n",
       " '000000000885': ['man in white shirt playing tennis on tennis court',\n",
       "  ['male tennis player in white shorts is playing tennis',\n",
       "   'this woman has just returned volley in tennis',\n",
       "   'man holding tennis racket playing tennis',\n",
       "   'the man balances on one leg after serving tennis ball',\n",
       "   'someone playing in tennis tournament with crowd looking on']],\n",
       " '000000001000': ['group of people standing around tennis court',\n",
       "  ['the people are posing for group photo',\n",
       "   'large family poses for picture on tennis court',\n",
       "   'group of young children standing next to each other',\n",
       "   'group of people that are standing near tennis net',\n",
       "   'group of kids posing for picture on tennis court']],\n",
       " '000000001268': ['woman in red shirt and white shirt and white umbrella',\n",
       "  ['beautiful woman taking picture with her smart phone',\n",
       "   'people underneath an arched bridge near the water',\n",
       "   'girl is taking picture of people fishing',\n",
       "   'the woman is taking photo of the white goose next to the river',\n",
       "   'waterway under bridge with people sitting down and woman taking photo']],\n",
       " '000000001296': ['woman holding cell phone in front of her',\n",
       "  ['woman holding hello kitty phone on her hands',\n",
       "   'woman holds up her phone in front of her face',\n",
       "   'woman in white shirt holding up cellphone',\n",
       "   'woman checking her cell phone with hello kitty case',\n",
       "   'the asian girl is holding her miss kitty phone']],\n",
       " '000000001353': ['group of people sitting around table with luggage',\n",
       "  ['some children are riding on mini orange train',\n",
       "   'several children are riding on toy train ride',\n",
       "   'the children are riding on an indoor train',\n",
       "   'several children on small indoor kiddie train',\n",
       "   'group of children ride on an indoor train']],\n",
       " '000000001425': ['sandwich with sandwich and cup of coffee',\n",
       "  ['meal is lying on plate on table',\n",
       "   'part of sandwich sitting on table',\n",
       "   'plate with burger that is halfway eaten',\n",
       "   'half eaten meal sitting on plate',\n",
       "   'sandwich with bite taken on plate']],\n",
       " '000000001490': ['man riding surfboard on top of wave',\n",
       "  ['man in wet suit stands on surfboard and rows with paddle',\n",
       "   'paddle boarder on large still body of water',\n",
       "   'man is holding long racquet on surfboard in the middle of the sea',\n",
       "   'black and white of person wearing wetsuit standing on surfboard and holding out paddle in large body of water outside',\n",
       "   'man with wet suit on standing on surfboard in the water']]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see a few random entries\n",
    "dict(list(comparison_dict_reload.items())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption 1 :: 000000000139.jpg :: Bleu score = 0.8241129631093145\n",
      "Caption 2 :: 000000000285.jpg :: Bleu score = 0.8346850194310856\n",
      "Caption 3 :: 000000000632.jpg :: Bleu score = 0.6435879088277018\n",
      "Caption 4 :: 000000000724.jpg :: Bleu score = 0.6314181891184061\n"
     ]
    }
   ],
   "source": [
    "## see a few random bleu scores\n",
    "i = 0\n",
    "for k, v in comparison_dict_reload.items():\n",
    "    i += 1\n",
    "    #print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))\n",
    "    print(f\"Caption {i} :: {k + '.jpg'} :: Bleu score = {nltk_bleu.sentence_bleu(v[1], v[0])}\")\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/ce6idelements1/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/rohit/anaconda3/envs/ce6idelements1/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 inputs\n"
     ]
    }
   ],
   "source": [
    "## calculate bleu scores for all the 5k images\n",
    "bscores_all = list()\n",
    "i = 0\n",
    "for k, v in comparison_dict_reload.items():\n",
    "    bscore = nltk_bleu.sentence_bleu(v[1], v[0])\n",
    "    bscores_all.append([k, v[0], bscore])\n",
    "    i += 1\n",
    "print(f\"Processed {i} inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bscores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['000000000139', 'living room with couch and television', 0.8241129631093145],\n",
       " ['000000000285', 'brown bear is standing in the grass', 0.8346850194310856],\n",
       " ['000000000632',\n",
       "  'bedroom with bed and table with two lamps',\n",
       "  0.6435879088277018],\n",
       " ['000000000724', 'stop sign with street sign on it', 0.6314181891184061],\n",
       " ['000000000776',\n",
       "  'teddy bear sitting on top of brown teddy bear',\n",
       "  0.6395565268169059],\n",
       " ['000000000785',\n",
       "  'man in red jacket and skis on snow covered slope',\n",
       "  0.7168290718649489],\n",
       " ['000000000802',\n",
       "  'kitchen with refrigerator and refrigerator',\n",
       "  0.6056679560107702],\n",
       " ['000000000872', 'baseball player swinging bat at ball', 0.6205238582310286],\n",
       " ['000000000885',\n",
       "  'man in white shirt playing tennis on tennis court',\n",
       "  0.7821697863592526],\n",
       " ['000000001000',\n",
       "  'group of people standing around tennis court',\n",
       "  0.8366930113447216]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see a few random entries\n",
    "bscores_all[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbs = pd.DataFrame(bscores_all, columns=['img', 'infcap', 'bsnltk'])\n",
    "dfbs.sort_values(by=['bsnltk'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>infcap</th>\n",
       "      <th>bsnltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>000000484760</td>\n",
       "      <td>clock tower with clock on it</td>\n",
       "      <td>8.580523e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>000000485130</td>\n",
       "      <td>bed with two beds and two beds</td>\n",
       "      <td>3.831503e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>000000197870</td>\n",
       "      <td>bird perched on the ground next to bird</td>\n",
       "      <td>4.351978e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>000000015517</td>\n",
       "      <td>train traveling down bridge next to bridge</td>\n",
       "      <td>4.815777e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>000000439522</td>\n",
       "      <td>man in black jacket and black jacket and black...</td>\n",
       "      <td>5.385075e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               img                                             infcap  \\\n",
       "2294  000000484760                       clock tower with clock on it   \n",
       "2299  000000485130                     bed with two beds and two beds   \n",
       "819   000000197870            bird perched on the ground next to bird   \n",
       "1959  000000015517         train traveling down bridge next to bridge   \n",
       "2461  000000439522  man in black jacket and black jacket and black...   \n",
       "\n",
       "             bsnltk  \n",
       "2294  8.580523e-155  \n",
       "2299   3.831503e-78  \n",
       "819    4.351978e-78  \n",
       "1959   4.815777e-78  \n",
       "2461   5.385075e-02  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0D3b7izSIqUb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>infcap</th>\n",
       "      <th>bsnltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>000000149770</td>\n",
       "      <td>man riding surfboard on top of wave</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>000000199442</td>\n",
       "      <td>man riding wave on top of surfboard</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>000000450303</td>\n",
       "      <td>group of people sitting around table with laptops</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>000000383606</td>\n",
       "      <td>bathroom with sink and mirror</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>000000320696</td>\n",
       "      <td>man riding wave on top of surfboard</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               img                                             infcap  bsnltk\n",
       "1702  000000149770                man riding surfboard on top of wave     1.0\n",
       "3635  000000199442                man riding wave on top of surfboard     1.0\n",
       "1663  000000450303  group of people sitting around table with laptops     1.0\n",
       "611   000000383606                      bathroom with sink and mirror     1.0\n",
       "3581  000000320696                man riding wave on top of surfboard     1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBlue Scores:\n",
      "\n",
      "Max = 1.0\n",
      "Median = 0.6372257075006953\n",
      "Min = 8.580523318321737e-155\n",
      "Average = 0.6327242145911766\n",
      "Std Dev = 0.16428741243707423\n"
     ]
    }
   ],
   "source": [
    "bs_max = float(dfbs[['bsnltk']].max(axis=0))\n",
    "bs_med = float(dfbs[['bsnltk']].median(axis=0))\n",
    "bs_min = float(dfbs[['bsnltk']].min(axis=0))\n",
    "bs_avg = float(dfbs[['bsnltk']].mean(axis=0))\n",
    "bs_std = float(dfbs[['bsnltk']].std(axis=0))\n",
    "print(f\"\\t\\tBlue Scores:\")\n",
    "print(f\"\\nMax = {bs_max}\\nMedian = {bs_med}\\nMin = {bs_min}\\nAverage = {bs_avg}\\nStd Dev = {bs_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
