{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show and Tell - attention based model\n",
    "## Trained - now reload weights and do inference\n",
    "\n",
    "### Based on link: https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "### The model architecture is similar to Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\n",
    "#### Arch paper: https://arxiv.org/abs/1502.03044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-rBS0Z-E4p8"
   },
   "source": [
    "# Trained model already on 100k images of Coco 2017 Train dataset.\n",
    "## Loading trained model and doing inference on new images not used in training\n",
    "\n",
    "## Weights file is useless - MUST restore the checkpoint files and use them to reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "buTVBtveEpqr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import re\n",
    "import pickle\n",
    "from glob import glob\n",
    "import collections\n",
    "import random\n",
    "import PIL\n",
    "import shutil\n",
    "\n",
    "from IPython.display import display as ipy_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0b0aXNyCEd3p"
   },
   "outputs": [],
   "source": [
    "## Local versions\n",
    "\n",
    "## to make deterministic\n",
    "IPDIR_DETERMINISTIC_ATTEND = r'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_Deterministic_Run4/'\n",
    "\n",
    "## new images - unseen during testing - part of Coco 2017 Test dataset\n",
    "TEST_IMGS_DIR = r'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Data/COCO_test2017_41k/test2017/'\n",
    "\n",
    "## checkpoint to restore to trained model\n",
    "CHKPT_DIR = '/home/rohit/PyWDUbuntu/thesis/ImgCapATTEND_opdir_DUMMY_RUN/chkPointDir_DUMMY_RUN/train_DUMMY_RUN/'\n",
    "\n",
    "## weights files from training run\n",
    "IPDIR_WEIGHTS_IN = '/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_Weights_In_Run4/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Fun fact: the decoder below is identical to the one in the example for Neural Machine Translation with Attention (https://www.tensorflow.org/tutorials/sequences/nmt_with_attention).\n",
    "\n",
    "### The model architecture is inspired by the Show, Attend and Tell paper (https://arxiv.org/pdf/1502.03044.pdf).\n",
    "\n",
    "### In this example, you extract the features from the lower convolutional layer of InceptionV3 giving us a vector of shape (8, 8, 2048).\n",
    "### You squash that to a shape of (64, 2048).\n",
    "### This vector is then passed through the CNN Encoder (which consists of a single Fully connected layer).\n",
    "### The RNN (here GRU) attends over the image to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "top_k = 5000\n",
    "vocab_size = top_k + 1\n",
    "max_length = 52\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload Tokenizer - one used during full training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_Deterministic_Run4/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_DETERMINISTIC_ATTEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATTEND_images_run4_5k_for_bleu_scoring.pkl',\n",
       " 'tokenizer_run4_from_training_100k.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_DETERMINISTIC_ATTEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload success\n",
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "## Reload all the files to make it deterministic\n",
    "\n",
    "# variable = tokenizer\n",
    "with open(IPDIR_DETERMINISTIC_ATTEND + 'tokenizer_run4_from_training_100k.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "print(f\"Reload success\")\n",
    "\n",
    "print(f\"{type(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup encoder related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.functional.Functional"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialize InceptionV3 and load the pretrained Imagenet weights\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "type(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint setup the folder from the training saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_dummy_ckhpt_dir(_dest_chk_pt):\n",
    "    src_chk_pt_dir = r'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_ChkPts_In_Run4_Ep19/'\n",
    "    dest_chk_pt = _dest_chk_pt[:]\n",
    "    if dest_chk_pt[-1] != '/':\n",
    "        dest_chk_pt = dest_chk_pt + r'/'\n",
    "        \n",
    "    count = 0\n",
    "    ## empty dest first\n",
    "    for f in os.listdir(_dest_chk_pt):\n",
    "        os.remove(_dest_chk_pt + f)\n",
    "    print(f\"Cleared current contents of the dest dir\\n\")\n",
    "    \n",
    "    for f_src in os.listdir(src_chk_pt_dir):\n",
    "        orig_f = src_chk_pt_dir + f_src\n",
    "        tgt_f   = dest_chk_pt + f_src\n",
    "        shutil.copyfile(orig_f, tgt_f)\n",
    "        count += 1\n",
    "    print(f\"Copied {count} files from\\n{src_chk_pt_dir}\\n\\tto\\n{dest_chk_pt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohit/PyWDUbuntu/thesis/ImgCapATTEND_opdir_DUMMY_RUN/chkPointDir_DUMMY_RUN/train_DUMMY_RUN/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHKPT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ckpt-18.index',\n",
       " 'checkpoint',\n",
       " 'ckpt-15.data-00000-of-00001',\n",
       " 'ckpt-19.data-00000-of-00001',\n",
       " 'ckpt-17.index',\n",
       " 'ckpt-19.index',\n",
       " 'ckpt-16.data-00000-of-00001',\n",
       " 'ckpt-16.index',\n",
       " 'ckpt-15.index',\n",
       " 'ckpt-17.data-00000-of-00001',\n",
       " 'ckpt-18.data-00000-of-00001']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(CHKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared current contents of the dest dir\n",
      "\n",
      "Copied 11 files from\n",
      "/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_ChkPts_In_Run4_Ep19/\n",
      "\tto\n",
      "/home/rohit/PyWDUbuntu/thesis/ImgCapATTEND_opdir_DUMMY_RUN/chkPointDir_DUMMY_RUN/train_DUMMY_RUN/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "populate_dummy_ckhpt_dir(CHKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ckpt-18.index',\n",
       " 'checkpoint',\n",
       " 'ckpt-15.data-00000-of-00001',\n",
       " 'ckpt-19.data-00000-of-00001',\n",
       " 'ckpt-17.index',\n",
       " 'ckpt-19.index',\n",
       " 'ckpt-16.data-00000-of-00001',\n",
       " 'ckpt-16.index',\n",
       " 'ckpt-15.index',\n",
       " 'ckpt-17.data-00000-of-00001',\n",
       " 'ckpt-18.data-00000-of-00001']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(CHKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify following files are present\n",
    "#### ['ckpt-18.index',\n",
    "#### 'checkpoint',\n",
    "#### 'ckpt-15.data-00000-of-00001',\n",
    "#### 'ckpt-19.data-00000-of-00001',\n",
    "#### 'ckpt-17.index',\n",
    "#### 'ckpt-19.index',\n",
    "#### 'ckpt-16.data-00000-of-00001',\n",
    "#### 'ckpt-16.index',\n",
    "#### 'ckpt-15.index',\n",
    "#### 'ckpt-17.data-00000-of-00001',\n",
    "#### 'ckpt-18.data-00000-of-00001']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raise exception if checkpoint files are not as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_chkpt_contents = [\n",
    "    'ckpt-18.index',\n",
    "    'checkpoint',\n",
    "    'ckpt-15.data-00000-of-00001',\n",
    "    'ckpt-19.data-00000-of-00001',\n",
    "    'ckpt-17.index',\n",
    "    'ckpt-19.index',\n",
    "    'ckpt-16.data-00000-of-00001',\n",
    "    'ckpt-16.index',\n",
    "    'ckpt-15.index',\n",
    "    'ckpt-17.data-00000-of-00001',\n",
    "    'ckpt-18.data-00000-of-00001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sorted(saved_chkpt_contents) != sorted(os.listdir(CHKPT_DIR)):\n",
    "    problem_msg = '\\n'.join([\n",
    "        f\"Checkpoint folder is not correct - Unable to restore to trained model\\n\",\n",
    "        f\"Expected checkpoint folder contents =\\n{sorted(saved_chkpt_contents)}\",\n",
    "        f\"Actual   checkpoint folder contents =\\n{sorted(os.listdir(CHKPT_DIR))}\"\n",
    "    ])\n",
    "    raise Exception(problem_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = CHKPT_DIR\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn__decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1280256   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  1575936   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  2565513   \n",
      "_________________________________________________________________\n",
      "bahdanau_attention (Bahdanau multiple                  394753    \n",
      "=================================================================\n",
      "Total params: 6,079,114\n",
      "Trainable params: 6,079,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## only works if some inference is done first\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bahdanau_attention\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  513       \n",
      "=================================================================\n",
      "Total params: 394,753\n",
      "Trainable params: 394,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## only works if some inference is done first\n",
    "decoder.attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_Weights_In_Run4/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDIR_WEIGHTS_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATTEND_Run4_Decoder_wts_ep_0.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_12.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_15.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_18.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_3.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_4.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_5.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_6.h5',\n",
       " 'ATTEND_Run4_Decoder_wts_ep_9.h5']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(IPDIR_WEIGHTS_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_file_to_reload = r'ATTEND_Run4_Decoder_wts_ep_18.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Code/ModelsRuns/ImgCapAttention/SavedData/Thesis_ImgCapATTENTION_Weights_In_Run4/ATTEND_Run4_Decoder_wts_ep_18.h5'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_FILE_RELOAD = IPDIR_WEIGHTS_IN + wts_file_to_reload\n",
    "WEIGHTS_FILE_RELOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(WEIGHTS_FILE_RELOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success reloading weights\n"
     ]
    }
   ],
   "source": [
    "decoder.load_weights(WEIGHTS_FILE_RELOAD)\n",
    "print(f\"Success reloading weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption\n",
    "\n",
    "### The evaluate function is similar to the training loop, except you don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "### Stop predicting when the model predicts the end token.\n",
    "### And store the attention weights for every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        #attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    #attention_plot = attention_plot[:len(result), :]\n",
    "    attention_plot = None\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_my_greedy(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        #attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "        #predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_id = tf.math.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    #attention_plot = attention_plot[:len(result), :]\n",
    "    attention_plot = None\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = r'/media/rohit/DATA/EverythingD/01SRHBDBA_Acads/Thesis/StoryGenerator/Data/COCO_test2017_41k/test2017/000000000016.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['stall', 'oranges', 'peeled', 'policeman', 'peers', 'designated', 'raining', 'mosaic', 'tee', 'crisp', 'arriving', 'ranch', 'music', 'western', 'follows', 'string', 'steer', 'entry', 'damaged', 'guitar', 'welcome', '3d', 'ditch', 'oxen', 'wrought', 'motorbikes', 'banana', 'yarn', 'gone', 'signage', 'artificial', 'catch', 'handed', 'vintage', 'newly', 'treats', 'brick', 'engine', 'bored', 'teens', 'family', 'exit', 'pitching', 'yummy', 'ford', 'jogging', 'stands', 'cow', 'urban', 'any', 'needed', 'ten']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - no weights reloading\n",
    "## evaluate() gives PATHETIC reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['offers', 'takes', 'bedroom', 'painted', 'unopened', 'fancy', 'court', '9', 'pickles', 'phone', 'same', 'canada', 'completely', 'shoppers', 'use', 'signage', 'extra', 'kite', 'zones', 'lodge', 'skin', 'sailboats', 'paddling', 'barriers', 'nails', 'base', 'cracked', 'book', 'isle', 'computers', 'easy', 'boarding', 'rock', 'servings', 'folded', 'camouflage', 'shaded', 'snowsuit', 'eyes', 'use', 'bubble', 'oven', 'operating', 'teammate', 'cherries', 'will', 'shrubbery', 'puddle', 'scattered', 'soldiers', 'be', 'pelicans']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - no weights reloading\n",
    "## evaluate() gives PATHETIC reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - no weights reloading\n",
    "## evaluate_my_greedy() gives PATHETIC reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds', 'birds']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - no weights reloading\n",
    "## evaluate_my_greedy() gives PATHETIC reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['the', 'tv', 'on', 'a', 'piece', 'of', 'a', 'little', 'girl', 'next', 'to', 'go', 'down', 'behind', 'a', 'flower', 'toy', 'in', 'a', 'frisbee', 'in', 'the', 'in', 'the', 'edge', 'that', 'is', 'next', 'to', 'a', 'kit', 'in', 'a', 'small', 'brown', 'and', 'other', 'a', 'delicious', 'looking', 'at', 'a', 'small', 'appliances', 'on', 'the', 'edge', 'of', '<unk>', 'in', 'an', 'elephant']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - even with weights reloading\n",
    "## evaluate() gives PATHETIC reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'bus', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - even with weights reloading\n",
    "## evaluate() gives PATHETIC reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'small', '<unk>', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - even with weights reloading\n",
    "## evaluate_my_greedy() gives PATHETIC reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'small', '<unk>', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in', 'front', 'of', 'a', 'small', 'white', 'teddy', 'bear', 'in']\n"
     ]
    }
   ],
   "source": [
    "## before the ckpt.restore - even with weights reloading\n",
    "## evaluate_my_greedy() gives PATHETIC reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'man', 'in', 'a', 'blue', 'jersey', 'swinging', 'a', 'bat', 'and', 'catcher', 'and', 'umpire', 'and', 'the', 'other', 'looking', 'backwards', 'baseball', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## AFTER the ckpt.restore - no weights reloading\n",
    "## evaluate() gives DECENT reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'man', 'is', 'getting', 'ready', 'to', 'bat', 'at', 'home', 'plate', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## after the ckpt.restore - no weights reloading\n",
    "## evaluate() gives DECENT reults - but changes in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'baseball', 'player', 'is', 'swinging', 'a', 'bat', 'at', 'a', 'ball', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## after the ckpt.restore - no weights reloading\n",
    "## evaluate_my_greedy() gives DECENT reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'baseball', 'player', 'is', 'swinging', 'a', 'bat', 'at', 'a', 'ball', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## after the ckpt.restore - no weights reloading\n",
    "## evaluate_my_greedy() gives DECENT reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted caption =\n",
      "['a', 'baseball', 'player', 'is', 'swinging', 'a', 'bat', 'at', 'a', 'ball', '<end>']\n"
     ]
    }
   ],
   "source": [
    "## after the ckpt.restore - even if reloaded weights - no effect\n",
    "## evaluate_my_greedy() gives DECENT reults - same output now in each run\n",
    "image = new_image\n",
    "#real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate_my_greedy(image)\n",
    "print(f\"Model predicted caption =\\n{result}\")\n",
    "#PIL.Image.open(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_test_imgs = os.listdir(TEST_IMGS_DIR)[:20]\n",
    "few_test_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, new_img in enumerate(few_test_imgs):\n",
    "    img_to_pred = TEST_IMGS_DIR + new_img\n",
    "    \n",
    "    ipy_display(PIL.Image.open(img_to_pred))\n",
    "    #PIL.Image.open(img_to_pred)\n",
    "    result, attention_plot = evaluate_my_greedy(img_to_pred)\n",
    "    if result[-1] == '<end>':\n",
    "        result = result[:-1]\n",
    "    result = ' '.join(result[:])\n",
    "    print(f\"Input image = {img_to_pred}\\n\")\n",
    "    print(f\"Predicted caption =\\n{result}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'-'*20}\\n{'-'*20}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0D3b7izSIqUb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
